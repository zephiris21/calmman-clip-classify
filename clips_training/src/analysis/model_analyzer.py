#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import sys
import json
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.metrics import (
    confusion_matrix, classification_report, roc_curve, auc,
    precision_recall_curve, average_precision_score
)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
current_file = Path(__file__)
project_root = current_file.parent.parent.parent.parent
sys.path.insert(0, str(project_root))

# ê¸°ì¡´ ìœ í‹¸ë¦¬í‹° ì¬ì‚¬ìš©
from clips_training.src.utils.training_utils import TrainingUtils


class ModelAnalyzer:
    """
    ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„ ë° ì‹œê°í™” ë„êµ¬
    - ìë™ ì°¨íŠ¸ ìƒì„± ë° ì €ì¥
    - ë…¸íŠ¸ë¶ ì¹œí™”ì  ì¸í„°í˜ì´ìŠ¤
    - ì‹¤í–‰ ì¦‰ì‹œ ê²°ê³¼ í™•ì¸
    """
    
    def __init__(self, config_path: str = "clips_training/configs/training_config.yaml"):
        """
        ëª¨ë¸ ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        Args:
            config_path (str): ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        self.config = TrainingUtils.load_training_config(config_path)
        self.output_dirs = TrainingUtils.setup_training_directories(self.config)
        
        # ì‹œê°í™” ì„¤ì •
        self.setup_visualization_style()
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥ìš©
        self.analysis_results = {}
        
        print("âœ… ëª¨ë¸ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def setup_visualization_style(self):
        """ì‹œê°í™” ìŠ¤íƒ€ì¼ ì„¤ì •"""
        # í•œê¸€ í°íŠ¸ ì„¤ì • (Windows í™˜ê²½ ëŒ€ì‘)
        plt.rcParams['font.family'] = ['Malgun Gothic', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # ê¸°ë³¸ ìŠ¤íƒ€ì¼
        plt.style.use('seaborn-v0_8-whitegrid')
        
        # ìƒ‰ìƒ íŒ”ë ˆíŠ¸ (configì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        viz_config = self.config.get('visualization', {})
        self.colors = viz_config.get('colors', {
            'funny': '#FF6B6B',
            'boring': '#4ECDC4', 
            'highlight': '#FFD93D',
            'neutral': '#95A5A6'
        })
        
        # ê¸°ë³¸ figure í¬ê¸°
        self.figsize = viz_config.get('figure_size', [12, 8])
        self.dpi = viz_config.get('dpi', 150)
        
        print(f"ğŸ“Š ì‹œê°í™” ìŠ¤íƒ€ì¼ ì„¤ì • ì™„ë£Œ (í¬ê¸°: {self.figsize}, DPI: {self.dpi})")
    
    def load_latest_results(self) -> Tuple[Any, Any, Dict, List[str]]:
        """
        ê°€ì¥ ìµœê·¼ í•™ìŠµ ê²°ê³¼ ë¡œë“œ
        
        Returns:
            Tuple: (model, scaler, metrics, feature_names)
        """
        print("ğŸ“‚ ìµœì‹  í•™ìŠµ ê²°ê³¼ ë¡œë“œ ì¤‘...")
        
        target_config = self.config['data']['target_config']
        models_dir = self.output_dirs['target_models']
        results_dir = self.output_dirs['results']
        
        # ëª¨ë¸ íŒŒì¼ ì°¾ê¸° (ê°€ì¥ ìµœê·¼)
        model_files = list(Path(models_dir).glob("xgboost_*.pkl"))
        if not model_files:
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {models_dir}")
        
        latest_model_file = max(model_files, key=os.path.getctime)
        
        # ëª¨ë¸ ë¡œë“œ
        with open(latest_model_file, 'rb') as f:
            model = pickle.load(f)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
        scaler_files = list(Path(models_dir).glob("scaler_*.pkl"))
        if scaler_files:
            latest_scaler_file = max(scaler_files, key=os.path.getctime)
            with open(latest_scaler_file, 'rb') as f:
                scaler = pickle.load(f)
        else:
            scaler = None
        
        # ì„±ëŠ¥ ì§€í‘œ ë¡œë“œ
        metrics_files = list(Path(results_dir).glob("metrics_*.json"))
        if metrics_files:
            latest_metrics_file = max(metrics_files, key=os.path.getctime)
            with open(latest_metrics_file, 'r', encoding='utf-8') as f:
                metrics = json.load(f)
        else:
            metrics = {}
        
        # íŠ¹ì§•ëª… ë¡œë“œ
        feature_files = list(Path(models_dir).glob("feature_names_*.json"))
        if feature_files:
            latest_feature_file = max(feature_files, key=os.path.getctime)
            with open(latest_feature_file, 'r', encoding='utf-8') as f:
                feature_names = json.load(f)
        else:
            feature_names = TrainingUtils.get_feature_names(target_config)
        
        print(f"âœ… ê²°ê³¼ ë¡œë“œ ì™„ë£Œ:")
        print(f"   ëª¨ë¸: {latest_model_file.name}")
        print(f"   ì„±ëŠ¥ ì§€í‘œ: {latest_metrics_file.name if metrics_files else 'ê¸°ë³¸ê°’'}")
        print(f"   íŠ¹ì§• ìˆ˜: {len(feature_names)}")
        
        return model, scaler, metrics, feature_names
    
    def create_results_directory(self) -> str:
        """ë¶„ì„ ê²°ê³¼ ì €ì¥ìš© ë””ë ‰í† ë¦¬ ìƒì„±"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        target_config = self.config['data']['target_config']
        
        analysis_dir = os.path.join(
            self.output_dirs['results'], 
            f"analysis_config{target_config}_{timestamp}"
        )
        os.makedirs(analysis_dir, exist_ok=True)
        
        print(f"ğŸ“ ë¶„ì„ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬: {analysis_dir}")
        return analysis_dir
    
    def plot_confusion_matrix(self, y_true: np.ndarray, y_pred: np.ndarray, 
                            save_path: str) -> plt.Figure:
        """
        í˜¼ë™í–‰ë ¬ íˆíŠ¸ë§µ ìƒì„±
        
        Args:
            y_true: ì‹¤ì œ ë¼ë²¨
            y_pred: ì˜ˆì¸¡ ë¼ë²¨
            save_path: ì €ì¥ ê²½ë¡œ
            
        Returns:
            matplotlib.Figure: ìƒì„±ëœ figure
        """
        fig, ax = plt.subplots(figsize=(8, 6), dpi=self.dpi)
        
        # í˜¼ë™í–‰ë ¬ ê³„ì‚°
        cm = confusion_matrix(y_true, y_pred)
        
        # íˆíŠ¸ë§µ ìƒì„±
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=['Boring', 'Funny'],
                    yticklabels=['Boring', 'Funny'],
                    ax=ax, cbar_kws={'shrink': 0.8})
        
        ax.set_title('Confusion Matrix', fontsize=16, fontweight='bold', pad=20)
        ax.set_xlabel('Predicted Label', fontsize=12)
        ax.set_ylabel('True Label', fontsize=12)
        
        # ì •í™•ë„ í‘œì‹œ
        accuracy = (cm[0,0] + cm[1,1]) / cm.sum()
        ax.text(0.5, -0.15, f'Accuracy: {accuracy:.3f}', 
                ha='center', va='center', transform=ax.transAxes, 
                fontsize=12, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=self.dpi, bbox_inches='tight')
        print(f"ğŸ’¾ í˜¼ë™í–‰ë ¬ ì €ì¥: {save_path}")
        
        return fig
    
    def plot_roc_curve(self, y_true: np.ndarray, y_scores: np.ndarray, 
                      save_path: str) -> plt.Figure:
        """
        ROC ê³¡ì„  ìƒì„±
        
        Args:
            y_true: ì‹¤ì œ ë¼ë²¨
            y_scores: ì˜ˆì¸¡ í™•ë¥ 
            save_path: ì €ì¥ ê²½ë¡œ
            
        Returns:
            matplotlib.Figure: ìƒì„±ëœ figure
        """
        fig, ax = plt.subplots(figsize=(8, 8), dpi=self.dpi)
        
        # ROC ê³¡ì„  ê³„ì‚°
        fpr, tpr, _ = roc_curve(y_true, y_scores)
        roc_auc = auc(fpr, tpr)
        
        # ROC ê³¡ì„  ê·¸ë¦¬ê¸°
        ax.plot(fpr, tpr, color=self.colors['funny'], lw=3, 
                label=f'ROC Curve (AUC = {roc_auc:.3f})')
        ax.plot([0, 1], [0, 1], color=self.colors['neutral'], 
                lw=2, linestyle='--', alpha=0.7, label='Random Classifier')
        
        ax.set_xlim([0.0, 1.0])
        ax.set_ylim([0.0, 1.05])
        ax.set_xlabel('False Positive Rate', fontsize=12)
        ax.set_ylabel('True Positive Rate', fontsize=12)
        ax.set_title('ROC Curve', fontsize=16, fontweight='bold', pad=20)
        ax.legend(loc="lower right", fontsize=11)
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=self.dpi, bbox_inches='tight')
        print(f"ğŸ’¾ ROC ê³¡ì„  ì €ì¥: {save_path}")
        
        return fig
    
    def plot_feature_importance(self, model: Any, feature_names: List[str], 
                              save_path: str, top_n: int = 20) -> plt.Figure:
        """
        í”¼ì²˜ ì¤‘ìš”ë„ ì°¨íŠ¸ ìƒì„±
        
        Args:
            model: í•™ìŠµëœ ëª¨ë¸
            feature_names: íŠ¹ì§•ëª… ë¦¬ìŠ¤íŠ¸
            save_path: ì €ì¥ ê²½ë¡œ
            top_n: í‘œì‹œí•  ìƒìœ„ íŠ¹ì§• ìˆ˜
            
        Returns:
            matplotlib.Figure: ìƒì„±ëœ figure
        """
        fig, ax = plt.subplots(figsize=(12, 10), dpi=self.dpi)
        
        # í”¼ì²˜ ì¤‘ìš”ë„ ì¶”ì¶œ ë° ì •ë ¬
        importance_scores = model.feature_importances_
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importance_scores
        }).sort_values('importance', ascending=True).tail(top_n)
        
        # ìˆ˜í‰ ë§‰ëŒ€ ê·¸ë˜í”„
        bars = ax.barh(range(len(importance_df)), importance_df['importance'], 
                      color=self.colors['highlight'], alpha=0.8, edgecolor='black', linewidth=0.5)
        
        # íŠ¹ì§•ëª… ì„¤ì • (ê°„ì†Œí™”)
        simplified_names = []
        for name in importance_df['feature']:
            # ê¸´ íŠ¹ì§•ëª… ê°„ì†Œí™”
            if len(name) > 25:
                parts = name.split('_')
                simplified = f"{parts[0]}_{parts[-2]}_{parts[-1]}"
                simplified_names.append(simplified)
            else:
                simplified_names.append(name)
        
        ax.set_yticks(range(len(importance_df)))
        ax.set_yticklabels(simplified_names, fontsize=10)
        ax.set_xlabel('Feature Importance', fontsize=12)
        ax.set_title(f'Top {top_n} Feature Importance (XGBoost)', 
                    fontsize=16, fontweight='bold', pad=20)
        
        # ê°’ í‘œì‹œ
        for i, (bar, value) in enumerate(zip(bars, importance_df['importance'])):
            ax.text(value + 0.001, i, f'{value:.3f}', 
                   va='center', ha='left', fontsize=9)
        
        ax.grid(True, axis='x', alpha=0.3)
        plt.tight_layout()
        plt.savefig(save_path, dpi=self.dpi, bbox_inches='tight')
        print(f"ğŸ’¾ í”¼ì²˜ ì¤‘ìš”ë„ ì €ì¥: {save_path}")
        
        return fig
    
    def plot_performance_dashboard(self, metrics: Dict, y_true: np.ndarray, y_pred: np.ndarray, save_path: str) -> plt.Figure:
        """
        ì„±ëŠ¥ ì§€í‘œ ëŒ€ì‹œë³´ë“œ ìƒì„± (ê°œì„ ëœ ë²„ì „)
        
        Args:
            metrics: ì„±ëŠ¥ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
            y_true: ì‹¤ì œ ë¼ë²¨
            y_pred: ì˜ˆì¸¡ ë¼ë²¨
            save_path: ì €ì¥ ê²½ë¡œ
            
        Returns:
            matplotlib.Figure: ìƒì„±ëœ figure
        """
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10), dpi=self.dpi)
        
        # CV vs Test ì„±ëŠ¥ ë¹„êµ (Accuracyë§Œ ì˜ë¯¸ìˆëŠ” ë¹„êµ)
        test_metrics = metrics.get('test_performance', {})
        cv_metrics = metrics.get('cross_validation', {})
        
        # 1. CV vs Test Accuracy ë¹„êµ (ê°„ë‹¨í•˜ê³  ëª…í™•í•˜ê²Œ)
        cv_acc = cv_metrics.get('cv_mean', 0)
        test_acc = test_metrics.get('test_accuracy', 0)
        
        comparison_data = ['CV Accuracy', 'Test Accuracy']
        comparison_scores = [cv_acc, test_acc]
        colors_comp = [self.colors['boring'], self.colors['funny']]
        
        bars1 = ax1.bar(comparison_data, comparison_scores, 
                       color=colors_comp, alpha=0.8, edgecolor='black', linewidth=1)
        
        # ê°’ í‘œì‹œ
        for bar, score in zip(bars1, comparison_scores):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{score:.3f}', ha='center', va='bottom', fontweight='bold')
        
        ax1.set_ylabel('Accuracy Score')
        ax1.set_title('CV vs Test Accuracy Comparison')
        ax1.set_ylim(0, 1.0)
        ax1.grid(True, alpha=0.3)
        
        # ê³¼ì í•© ì—¬ë¶€ í‘œì‹œ
        if test_acc > cv_acc:
            ax1.text(0.5, 0.1, 'âœ… No Overfitting\n(Test > CV)', 
                    ha='center', va='center', transform=ax1.transAxes,
                    bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7),
                    fontsize=10, fontweight='bold')
        else:
            diff = cv_acc - test_acc
            if diff > 0.05:
                ax1.text(0.5, 0.1, 'âš ï¸ Possible Overfitting', 
                        ha='center', va='center', transform=ax1.transAxes,
                        bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.7),
                        fontsize=10, fontweight='bold')
        
        # 2. í´ë˜ìŠ¤ë³„ ì •ë°€í•œ ì„±ëŠ¥ ë¶„ì„ (í˜¼ë™í–‰ë ¬ ê¸°ë°˜)
        from sklearn.metrics import classification_report
        
        # ë¶„ë¥˜ ë³´ê³ ì„œì—ì„œ í´ë˜ìŠ¤ë³„ ì§€í‘œ ì¶”ì¶œ
        report = classification_report(y_true, y_pred, target_names=['Boring', 'Funny'], output_dict=True)
        
        classes = ['Boring', 'Funny']
        precision_scores = [report['Boring']['precision'], report['Funny']['precision']]
        recall_scores = [report['Boring']['recall'], report['Funny']['recall']]
        f1_scores = [report['Boring']['f1-score'], report['Funny']['f1-score']]
        
        x_classes = np.arange(len(classes))
        width = 0.25
        
        bars2_1 = ax2.bar(x_classes - width, precision_scores, width, 
                         label='Precision', color=self.colors['highlight'], alpha=0.8)
        bars2_2 = ax2.bar(x_classes, recall_scores, width, 
                         label='Recall', color=self.colors['neutral'], alpha=0.8)
        bars2_3 = ax2.bar(x_classes + width, f1_scores, width, 
                         label='F1-Score', color=self.colors['funny'], alpha=0.8)
        
        # ê°’ í‘œì‹œ
        for bars in [bars2_1, bars2_2, bars2_3]:
            for bar in bars:
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2, height + 0.01,
                        f'{height:.2f}', ha='center', va='bottom', fontsize=9)
        
        ax2.set_xlabel('Classes')
        ax2.set_ylabel('Score')
        ax2.set_title('Class-wise Performance Metrics')
        ax2.set_xticks(x_classes)
        ax2.set_xticklabels(classes)
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        ax2.set_ylim(0, 1.1)
        
        # 3. í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì „ì²´ ì„±ëŠ¥ ì§€í‘œ ì‹œê°í™”
        all_metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']
        all_scores = [
            test_metrics.get('test_accuracy', 0),
            test_metrics.get('test_precision', 0), 
            test_metrics.get('test_recall', 0),
            test_metrics.get('test_f1', 0),
            test_metrics.get('test_roc_auc', 0)
        ]
        
        bars3 = ax3.bar(all_metrics, all_scores, 
                       color=[self.colors['funny'], self.colors['highlight'], 
                             self.colors['neutral'], self.colors['boring'], 
                             self.colors['funny']], alpha=0.8)
        
        # ê°’ í‘œì‹œ
        for bar, score in zip(bars3, all_scores):
            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{score:.3f}', ha='center', va='bottom', fontweight='bold')
        
        ax3.set_ylabel('Score')
        ax3.set_title('Test Set Performance Metrics')
        ax3.set_xticklabels(all_metrics, rotation=45)
        ax3.grid(True, alpha=0.3)
        ax3.set_ylim(0, 1.0)
        
        # 4. ëª¨ë¸ ì •ë³´ ë° í˜¼ë™í–‰ë ¬ ìš”ì•½
        ax4.axis('off')
        
        # í˜¼ë™í–‰ë ¬ ê³„ì‚°
        cm = confusion_matrix(y_true, y_pred)
        tn, fp, fn, tp = cm.ravel()
        
        info_text = f"""
Model Configuration Summary

Algorithm: XGBoost
Top Features: {len(metrics.get('test_performance', {}).get('top_features', {}).get('names', []))}
Best Feature: {metrics.get('test_performance', {}).get('top_features', {}).get('names', ['N/A'])[0] if metrics.get('test_performance', {}).get('top_features', {}).get('names') else 'N/A'}

Performance Highlights:
âœ“ Test Accuracy: {test_metrics.get('test_accuracy', 0):.3f}
âœ“ F1-Score: {test_metrics.get('test_f1', 0):.3f}
âœ“ ROC-AUC: {test_metrics.get('test_roc_auc', 0):.3f}

Confusion Matrix:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TN: {tn:2d}     â”‚ FP: {fp:2d}     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ FN: {fn:2d}     â”‚ TP: {tp:2d}     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Training Info:
â€¢ CV Accuracy: {cv_metrics.get('cv_mean', 0):.3f}
â€¢ CV Folds: {metrics.get('model_config', {}).get('cv_folds', 5)}
â€¢ Total Time: {cv_metrics.get('cv_time', 0) + cv_metrics.get('final_training_time', 0):.1f}s

Interpretation:
â€¢ Precision (Funny): {report['Funny']['precision']:.3f}
â€¢ Recall (Funny): {report['Funny']['recall']:.3f}
â€¢ No False Positives: {fp == 0}
        """
        
        ax4.text(0.05, 0.95, info_text, transform=ax4.transAxes, fontsize=10,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        
        plt.suptitle('Model Performance Dashboard (Enhanced)', fontsize=18, fontweight='bold', y=0.98)
        plt.tight_layout()
        plt.savefig(save_path, dpi=self.dpi, bbox_inches='tight')
        print(f"ğŸ’¾ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ ì €ì¥: {save_path}")
        
        return fig
    
    def analyze_and_save_all(self, auto_close: bool = True) -> Dict[str, str]:
        """
        ì „ì²´ ë¶„ì„ ì‹¤í–‰ ë° ëª¨ë“  ì°¨íŠ¸ ì €ì¥
        
        Args:
            auto_close: ì°¨íŠ¸ ìë™ ë‹«ê¸° ì—¬ë¶€
            
        Returns:
            Dict: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œë“¤
        """
        print("\nğŸ¨ ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„ ë° ì‹œê°í™” ì‹œì‘")
        print("=" * 50)
        
        try:
            # 1. ê²°ê³¼ ë¡œë“œ
            model, scaler, metrics, feature_names = self.load_latest_results()
            
            # 2. ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
            analysis_dir = self.create_results_directory()
            
            # 3. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë‹¤ì‹œ ë¡œë“œ (ì˜ˆì¸¡ê°’ ê³„ì‚°ìš©)
            print("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
            dataset_path = self.config['data']['dataset_path']
            target_config = self.config['data']['target_config']
            
            X, y, _, _ = TrainingUtils.load_dataset_hdf5(dataset_path, target_config)
            
            # ë°ì´í„° ë¶„í•  (í•™ìŠµí•  ë•Œì™€ ë™ì¼í•œ ì„¤ì •)
            from sklearn.model_selection import train_test_split
            _, X_test, _, y_test = train_test_split(
                X, y,
                test_size=self.config['data']['test_size'],
                random_state=self.config['data']['random_state'],
                stratify=y if self.config['data']['stratify'] else None
            )
            
            # ìŠ¤ì¼€ì¼ë§ (í•„ìš”í•œ ê²½ìš°)
            if scaler is not None:
                X_test = scaler.transform(X_test)
            
            # ì˜ˆì¸¡ ìˆ˜í–‰
            y_pred = model.predict(X_test)
            y_scores = model.predict_proba(X_test)[:, 1]
            
            saved_files = {}
            
            # 4. í˜¼ë™í–‰ë ¬
            print("ğŸ“ˆ í˜¼ë™í–‰ë ¬ ìƒì„± ì¤‘...")
            cm_path = os.path.join(analysis_dir, "confusion_matrix.png")
            fig1 = self.plot_confusion_matrix(y_test, y_pred, cm_path)
            saved_files['confusion_matrix'] = cm_path
            if auto_close: plt.close(fig1)
            
            # 5. ROC ê³¡ì„ 
            print("ğŸ“ˆ ROC ê³¡ì„  ìƒì„± ì¤‘...")
            roc_path = os.path.join(analysis_dir, "roc_curve.png")
            fig2 = self.plot_roc_curve(y_test, y_scores, roc_path)
            saved_files['roc_curve'] = roc_path
            if auto_close: plt.close(fig2)
            
            # 6. í”¼ì²˜ ì¤‘ìš”ë„
            print("ğŸ“ˆ í”¼ì²˜ ì¤‘ìš”ë„ ì°¨íŠ¸ ìƒì„± ì¤‘...")
            feature_path = os.path.join(analysis_dir, "feature_importance.png")
            top_n = self.config.get('evaluation', {}).get('feature_importance', {}).get('top_n', 20)
            fig3 = self.plot_feature_importance(model, feature_names, feature_path, top_n)
            saved_files['feature_importance'] = feature_path
            if auto_close: plt.close(fig3)
            
            # 7. ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ (ê°œì„ ëœ ë²„ì „ - y_true, y_pred ì „ë‹¬)
            print("ğŸ“ˆ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ ìƒì„± ì¤‘...")
            dashboard_path = os.path.join(analysis_dir, "performance_dashboard.png")
            fig4 = self.plot_performance_dashboard(metrics, y_test, y_pred, dashboard_path)
            saved_files['performance_dashboard'] = dashboard_path
            if auto_close: plt.close(fig4)
            
            # 8. ë¶„ì„ ìš”ì•½ ì €ì¥
            summary_path = os.path.join(analysis_dir, "analysis_summary.json")
            summary = {
                'analysis_timestamp': datetime.now().isoformat(),
                'model_type': 'XGBoost',
                'test_accuracy': float(metrics.get('test_performance', {}).get('test_accuracy', 0)),
                'test_f1': float(metrics.get('test_performance', {}).get('test_f1', 0)),
                'test_roc_auc': float(metrics.get('test_performance', {}).get('test_roc_auc', 0)),
                'top_5_features': metrics.get('test_performance', {}).get('top_features', {}).get('names', [])[:5],
                'saved_files': saved_files,
                'config_used': self.config['data']['target_config']
            }
            
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            saved_files['summary'] = summary_path
            
            print("\nâœ… ë¶„ì„ ì™„ë£Œ!")
            print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {analysis_dir}")
            print(f"ğŸ“Š ìƒì„±ëœ ì°¨íŠ¸: {len(saved_files)-1}ê°œ")
            print(f"ğŸ† ìµœê³  ì„±ëŠ¥: Test Accuracy {summary['test_accuracy']:.3f}")
            
            return saved_files
            
        except Exception as e:
            print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return {}


def quick_analysis():
    """ë…¸íŠ¸ë¶ìš© ë¹ ë¥¸ ë¶„ì„ í•¨ìˆ˜"""
    analyzer = ModelAnalyzer()
    return analyzer.analyze_and_save_all(auto_close=False)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¨ ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„ê¸° ì‹¤í–‰")
    
    analyzer = ModelAnalyzer()
    saved_files = analyzer.analyze_and_save_all(auto_close=True)
    
    if saved_files:
        print(f"\nğŸ‰ ë¶„ì„ ì™„ë£Œ! {len(saved_files)}ê°œ íŒŒì¼ ìƒì„±")
        for file_type, file_path in saved_files.items():
            print(f"   {file_type}: {file_path}")
    else:
        print("âŒ ë¶„ì„ ì‹¤íŒ¨")


if __name__ == "__main__":
    main()