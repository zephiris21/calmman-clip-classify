---
title: 직전레이어 활용 계획에서 변경한 이유
tags:
  - 프로젝트
created: 2025-06-06 12:24:20
related_project:
  - "[[감정인식 분류모델]]"
category: sub-project
link:
---

> [!NOTE]
> 표준편차만 사용하자. 그게 제일 중요해보이고. 다른걸 늘리면 차원수가 너무 늘어나.
> 
> e-러닝 참여학생의 몰입도 예측에 비슷한 방법을 사용했는데
> 
> 1. 몰입도 예측 (Engagement Prediction) ◦ 시간 축 데이터(프레임별 얼굴 특징)에 대해 표준 편차를 계산하여 최종 비디오 디스크립터로 사용했습니다 . 연구자들은 평균, 최대값, 최소값 등 다른 통계적 특징도 시도했지만, 유효성 검사 세트(validation set)에서 평균 제곱 오차(MSE) 개선에 기여하지 않아 표준 편차만을 사용했습니다 . ◦ 이렇게 얻은 표준 편차 기반 비디오 디스크립터는 릿지 회귀 (ridge regression) 모델의 입력으로 사용되었습니다 . 특히 EngageWild 데이터셋의 공식 분할(official split)에서 릿지 회귀 모델이 단일 모델 중 가장 낮은 MSE를 기록했습니다 . ◦ 또한, 논문의 실험(ablation study)에서는 랜덤 포레스트 (Random Forest), 서포트 벡터 회귀 (SVR) (LinearSVR, SVR RBF), GRU (gated recurrent unit), 어텐션(Attention) 기반 모델 등 다양한 회귀 모델들도 함께 테스트했습니다 . 새로운 데이터 분할(new split)에서는 어텐션 기반 모델 등이 릿지 회귀보다 더 낮은 MSE를 보였습니다. classify_engagement 함수 내부에서는 표준 편차 계산 후 슬라이딩 윈도우 방식으로 어텐션 기반의 Keras 모델(_single_attention_model)을 사용한 것으로 보입니다. 따라서 몰입도 예측에는 주로 표준 편차로 집계된 특징과 릿지 회귀 모델이 사용되었지만, 다양한 모델들이 비교 검토되었습니다

완벽한 근거네요! 실제 연구에서 검증된 방법이군요.
## 표준편차만 사용하는 최종 설계

### **핵심 아이디어**

- **EfficientNet 1280차원** 시간축 표준편차 → **1280차원 최종 특징**
- **오디오 2차원** (RMS + 변화율) 시간축 표준편차 → **2차원 최종 특징**
- **총 1282차원** → Ridge Regression

### **왜 표준편차가 최적인가?**

1. **변화량 포착**: 재밌는 순간 = 감정 변화가 큰 순간
2. **길이 무관**: 5초든 30초든 변화의 강도로 정규화
3. **실증적 우수성**: EngageWild 데이터셋에서 검증됨
4. **차원 효율성**: 1280→1280 (증가 없음)

### **데이터 파이프라인**

```python
# 영상 → 유효 얼굴 프레임들 → [N, 1280] → std → [1280]
# 오디오 → 시간축 특징들 → [N, 2] → std → [2]  
# 결합 → [1282] → Ridge Regression → 재밌음/재미없음
```

### **장점들**

- **결측 프레임 자연스럽게 제외**
- **영상 길이 완전 자유**
- **검증된 방법론**
- **구현 단순함**


# 문제 : 차원의 저주
## 데이터셋 규모 추정

### **최소 요구치**

- **Ridge Regression**: 특징 차원(1282) 대비 **10배 이상** → **최소 12,000개**



> [!NOTE] 실제 몰입도 예측에 사용된 데이터셋은 265개 비디오
> 
> 1280차원 특징이 몰입도 예측(Engagement Prediction) 작업에 사용된 시간 축 데이터(프레임별 얼굴 특징)의 **표준 편차(standard deviation)**를 집계하여 얻은 비디오 디스크립터임을 확인했습니다 [Source 41, 이전 대화]. 이 특징은 EfficientNet-B0 및 EfficientNet-B2 모델에서 추출되었습니다 [Source 41]. 이 1280차원 특징 벡터를 최종 모델의 입력으로 사용하는 작업은 주로 EngageWild 데이터셋을 기반으로 수행되었습니다 [Source 41]. 소스에 따르면, EngageWild 데이터셋은 총 265개의 비디오로 구성되어 있으며, 각 비디오는 평균 5분 길이이고 4단계의 몰입도 레벨로 레이블링되어 있습니다 [Source 41]. 1280차원의 표준 편차 특징 벡터는 비디오 클립당 하나씩 계산되어 최종 회귀 모델(주로 릿지 회귀)의 입력으로 사용됩니다 [Source 41].

정말 중요한 통찰들이네요! 265개로 1280차원을 성공시킨 핵심 요인들이 명확해졌어요.

## 핵심 성공 요인 분석

### **1. Transfer Learning의 힘**

- **EfficientNet**: VGGFace2 + AffectNet 대규모 사전학습
- **1280차원 특징**: 이미 수백만 이미지로 학습된 robust한 표현
- **우리도 동일한 혜택** 받을 수 있음!

### **2. 표준편차 집계의 효과**

- **시간축 압축**: [T, 1280] → [1280]
- **노이즈 제거**: 개별 프레임 노이즈가 평균화됨
- **패턴 강화**: 유의미한 변화만 남음

### **3. Ridge 회귀의 정규화**

- **L2 정규화**로 과적합 방지
- **고차원에서 안정적**: XGBoost보다 적은 데이터에 유리
- **해석 가능**: 어떤 특징이 중요한지 분석 가능

## 우리 프로젝트 적용

### **100~200개 데이터로도 충분히 가능!**

```python
# 몰입도 연구: 265개 → 성공
# 우리: 100~200개 → 절반 수준이지만 시도해볼 만함

# 동일한 파이프라인
EfficientNet(사전학습) → 1280차원 → std(시간축) → Ridge회귀
```

### **추가 안전장치**

- **더 강한 정규화** (Ridge alpha 증가)
- **Cross-validation** 철저히
- **Early stopping**
- 필요시 **PCA 200~300차원**으로 경미한 축소

**결론**: 원래 1280차원 계획이 현실적으로 가능해 보여요!





