---
title: 데이터 파이프라인 및 동적프레임 스킵 방식
tags:
  - 프로젝트
created: 2025-06-06 09:32:30
related_project:
  - "[[감정인식 분류모델]]"
category: sub-project
link:
---

> [!NOTE]
> 
> 이 config 파일도 참조해줘.
> 
> 그럼 내 생각에는 지금 코드에서 분류기를 10차원 감정분류기로 바꿔서
> 
> 특정인물의 얼굴만 추출해서 감정의 변화를 시간축으로 추출해서 10 * (영상길이 x 초 당 프레임 ) 만큼의 벡터를 추출 할 수 있겠는데.
> 
> 그럼 이 데이터를 바탕으로
> 
> 1. 영상에서 분류된 총 감정 비율, 파이그래프
> 2. 시간축에 따른 arousal 의 변화를 시각화 같은 시각화를 할 수 있을것 같네.
> 
> 그리고 5~7초 클립을 통째로 넣어서 학습에 사용할 데이터를 만들 수 있을거고
> 
> 그리고 이 데이터를 원본으로 해서 sliding window를 통해서 재밌는순간 분류모델에 추론을 시키기 위한 입력데이터를 만들 수 있을거같네.
> 
> 너의 의견을 간단히 말해줘

🎯 완벽한 접근법입니다!

## 데이터 파이프라인

**torch_video_processor + VA 감정분류기** → **10차원 × 시간축 데이터** ✅

## 시각화 아이디어

1. **감정 비율 파이차트** - 영상 전체 감정 분포 분석 ✅
2. **Arousal 시간축 변화** - 텐션 변화 패턴 시각화 ✅

## 학습 데이터 생성

- **5~7초 클립** → 10차원 시퀀스 → **재밌음/재미없음 라벨** ✅
- **Sliding window** → 다양한 길이 패턴 학습 ✅

## 추가 제안

- **Valence 변화**도 함께 시각화 (긍정성 변화)
- **감정 전환점** 감지 (Neutral→Happiness→Surprise 패턴)
- **오디오 특징**과 결합하면 더 강력

**바로 구현 가능한 완성도 높은 계획**이네요!

먼저 **torch_video_processor를 VA 버전으로 수정**부터 시작하면 될 것 같습니다. 👍

> [!NOTE]
> 슬라이딩 윈도우는 학습 데이터 생성용도가 아니라 학습된 분류 모델을 실제 긴 영상에 적용할 때 입력데이터셋을 만들기 위해 사용하려고 언급한거야
> 
> 어떤식으로 모듈을 구성하지?
> 
> 그리고 데이터의 저장형태는 어떤식으로 하는게 좋을까?
> 
> 간단하게 답변
## 모듈 구성

### **1. 데이터 생성 모듈**

```
video_feature_extractor.py    # 영상 → 10차원 시계열 추출
audio_feature_extractor.py    # 영상 → 2차원 오디오 특징
multimodal_processor.py       # 통합 12차원 시계열 생성
```

### **2. 학습 데이터 모듈**

```
clip_dataset_builder.py       # 5~7초 클립 → 학습용 데이터셋
```

### **3. 추론 모듈**

```
sliding_window_processor.py   # 긴 영상 → sliding window 입력 생성
funny_moment_classifier.py    # 학습된 모델로 재밌는 순간 탐지
```

## 데이터 저장 형태

### **시계열 데이터**

```python
# HDF5 또는 NPZ
{
    'features': np.array([frames, 12]),  # 12차원 × 시간
    'timestamps': np.array([frames]),    # 타임스탬프
    'metadata': {'fps': 2, 'duration': 30, 'video_name': 'xxx'}
}
```

### **클립 데이터셋**

```python
# 학습용
{
    'clip_features': [N, seq_len, 12],
    'labels': [N],  # 0: 재미없음, 1: 재밌음
    'clip_info': [N, metadata]
}
```

**HDF5 추천** - 대용량 시계열 데이터에 최적화됨 👍

> [!NOTE]
> hdf5 와 npz 차이는? 그리고, 시계열에 timestamps 를 그대로 사용한다는건 중간에 missing frame이 있어도 시간적으로 얼마나 떨어졌는지에 대한 정보를 사용할 수 있다는건가?
> 
> 간단한 답변

## HDF5 vs NPZ

### **NPZ**

- 압축된 numpy 배열들의 묶음
- 단순, 빠름
- 메타데이터 제한적

### **HDF5**

- 계층적 데이터 포맷
- 대용량 데이터 최적화
- 부분 로딩 가능, 메타데이터 풍부

**HDF5가 시계열에 더 적합** (대용량 + 복잡한 구조)

## Timestamps 사용법

### **원래 의도**

```python
timestamps = [0.0, 0.5, 1.0, 1.5, 2.0]  # 0.5초 간격
features = [frame0, frame1, frame2, frame3, frame4]
```

### **Missing frame 처리**

```python
timestamps = [0.0, 0.5, 1.5, 2.0]       # 1.0초 프레임 누락
features = [frame0, frame1, frame3, frame4]
# → 1.0초 시점 데이터 없음을 인식 가능
```

**네, 맞습니다!** timestamps로 **시간적 간격 정보**를 보존할 수 있어서 missing frame이나 불규칙한 간격도 처리 가능합니다. 👍

> [!NOTE]
> 그러면 lstm 이나 conv1D 를 사용할때 시간간격에 대한 정보도 이용할 수 있는건가? 똑같은 14프레임을 사용해도 그 간격이 다를 수 있잖아. 학습데이터셋에서 미묘하게 서로 간격이 달라도 똑같은 입력형태로해서 학습시킬 수 있나?

## 시간 간격 정보 활용

### **일반적인 LSTM/Conv1D**

- **순서만 인식**, 시간 간격은 무시
- `[frame0, frame1, frame3, frame4]` = 등간격으로 처리
- **문제**: 실제 0.5초와 1.0초 간격을 구분 못함

### **시간 간격 정보 활용 방법**

#### **1. Time-aware LSTM**

```python
# 입력에 시간 간격 추가
features = [12차원 감정 + 1차원 시간간격]  # 13차원으로 확장
# 또는
time_deltas = [0.5, 0.5, 1.0, 0.5]  # 이전 프레임과의 간격
```

#### **2. Positional Encoding (Transformer 방식)**

```python
# 절대 시간을 인코딩해서 추가
time_encoding = positional_encoding(timestamps)
features = concat(emotion_features, time_encoding)
```

#### **3. 간격 정규화**

```python
# 고정 간격으로 interpolation
target_times = [0.0, 0.5, 1.0, 1.5, 2.0]  # 목표 시간
features_interpolated = interpolate(features, timestamps, target_times)
```

## 추천 방식

**간격 정규화**가 가장 실용적입니다. 학습 시 일관된 입력 형태 유지 + 간단한 구현 👍

> [!NOTE]
> 간격정규화는 리니어하게 interpolation 시키자는거구나.
> 
> 내가 물어본건, 예를들어 학습데이터셋이 5초에서 7초 사이니까 프레임스킵을 조정해서 5초든 7초든 똑같이 15프레임 정도 (임의로 정함) 를 뽑아내서 사용하는건 어떨까 생각해본거야. 그럼 앞부분에 패딩을 안써도 되니까

아 그거 좋은 아이디어네요! 🎯

## 동적 프레임 스킵 방식

### **예시**

```python
# 5초 클립 → 15프레임 추출
frame_skip = 5초 * 2fps / 15프레임 = 0.67초 간격

# 7초 클립 → 15프레임 추출  
frame_skip = 7초 * 2fps / 15프레임 = 0.93초 간격
```

### **장점**

- **고정 시퀀스 길이**: 패딩 불필요 ✅
- **전체 클립 커버**: 시작~끝 정보 모두 활용 ✅
- **일관된 입력**: 모델 학습 안정성 ✅

### **단점**

- **시간 해상도 차이**: 5초는 조밀, 7초는 성김
- **정보 밀도 불균등**: 같은 15프레임이지만 담긴 정보량 다름

## 결론

**실용적으로 매우 좋은 방식**입니다!

시간 해상도 차이는 있지만, **패딩 없이 깔끔한 학습**이 가능하고 **전체 클립 정보를 활용**할 수 있어서 장점이 더 큽니다.

**추천!** 👍

> [!NOTE]
> 시간적 해상도를 너무 심하게 훼손하지 않는다면 동적프레임이 더 나을거같네. 5초와 7초는 최대 1.4배 빨리감기한 영상을 학습한 것의 차이와 비슷한데. 그 정도는 납득할 수준인거같아.

🎯 완전히 동의합니다!

## 시간 해상도 분석

### **실제 차이**

- 5초 클립: 0.33초 간격 (5초/15프레임)
- 7초 클립: 0.47초 간격 (7초/15프레임)
- **비율**: 1.4배 차이

### **1.4배 빨리감기 관점**

- **사람도 1.25~1.5배속**으로 영상 시청 가능
- **감정 변화 패턴**은 여전히 인식 가능
- **CNN/LSTM**은 이 정도 변화에 robust함

### **납득 가능한 수준** ✅

- 극단적이지 않음 (2배, 3배가 아닌)
- **전체 정보 보존 > 시간 해상도** 우선순위가 맞음
- 실제로 **다양한 템포의 학습 데이터**가 모델 generalization에 도움

## 결론

**동적 프레임 스킵 확정!**

5~7초 범위에서 1.4배 차이는 **허용 가능한 trade-off**이고, 완전한 클립 커버리지가 훨씬 중요합니다. 👍

> [!NOTE]
> hdf5 에서 메타데이터는 어떻게 활용될거같지? 그리고 부분로딩이 가능한 건 어떤 의미야? 우리의 경우에 대용량 데이터인가? 짧게 답변

## 메타데이터 활용

```python
metadata = {
    'video_name': 'funny_clip_001.mp4',
    'fps': 2,
    'clip_duration': 6.5,
    'label': 1,  # 재밌음
    'source_timestamp': [120.0, 126.5],  # 원본에서의 위치
    'frame_count': 15
}
```

**활용**: 데이터 필터링, 디버깅, 원본 추적

## 부분 로딩

```python
# 전체 로딩 안하고 필요한 부분만
with h5py.File('dataset.h5') as f:
    batch_data = f['features'][100:200]  # 100~200번째 클립만
```

**의미**: 메모리 절약, 빠른 접근

## 대용량 데이터인가?

**YES!**

- 수천개 클립 × 15프레임 × 12차원 = **수십~수백MB**
- 영상 늘어날수록 **GB 단위**
- NPZ로는 **전체 메모리 로딩** 필요

**HDF5 필수입니다** 👍


> [!NOTE] 얼굴인식 실패 시 
> 프레임 추출해서 데이터를 생성하는데 침착맨 얼굴이 실패하는경우 어떻게 보완해야할까?

간단한 답변


## 실패 케이스 보완 방법

### **1. 얼굴 탐지 실패**

- **임계값 완화**: MTCNN prob_threshold 낮추기 (0.9 → 0.7)
- **이전/다음 프레임 대체**: 시간적으로 가까운 프레임 사용
- **Zero padding**: 실패 시 0벡터로 채우고 마스킹

### **2. 유사도 매칭 실패**

- **임계값 완화**: similarity_threshold 낮추기 (0.5 → 0.3)
- **다중 기준점**: 여러 침착맨 임베딩으로 매칭 시도
- **최고 유사도 선택**: 여러 얼굴 중 가장 높은 것 사용

### **3. 시퀀스 레벨 처리**

- **보간**: 앞뒤 성공 프레임으로 linear interpolation
- **클립 제외**: 실패율 높은 클립(30% 이상)은 학습에서 제외

## 추천 조합

**임계값 완화 + 보간 + 심한 경우 클립 제외** 👍