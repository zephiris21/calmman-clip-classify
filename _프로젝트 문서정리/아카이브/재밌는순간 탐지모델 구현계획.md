---
title: 구현계획
tags:
  - 프로젝트
created: 2025-06-04 15:33:48
related_project:
  - "[[감정인식 분류모델]]"
category: sub-project
link:
  - https://claude.ai/share/0b74d4a1-da2e-4b61-9a09-3419f3b396e7
---
https://claude.ai/share/0b74d4a1-da2e-4b61-9a09-3419f3b396e7

https://notebooklm.google.com/notebook/809e3b14-128d-4864-bae8-ec45828a52ad

> [!NOTE]
> 코드 구현 계획을 다듬어나가보자.
> 
> 모델 학습코드를 우선 만들고
> 
> 그 다음 동영상처리해서 재밌는 순간의 타임스탬프와 시간별 텐션예측 (재밌는 순간이 텐션변화와 비례한다고 가정해서) 을 하도록 코드를 만들거야.
> 
> ## 학습코드
> 
> 우선 pt 파일을 로드해서 분류헤드를 제거한 가중치를 사용해야해. 그리고 사용할 직전레이어의 차원수를 알아야해. 그리고 모델의 구조를 정확하게 테스트해서 알아야 모델을 문제없이 로드할수있어.
> 
> 학습시킬 영상 클립들을 수집해야해. 영상의 길이는 몇초가 좋을지 모르겠네. 그리고 재밌다고 판단할 기준도 어렵고. 수집방법도 생각해봐야겠어.
> 
> 클립을 수집했다면 클립의 이미지들의 전처리가 필요해. 영상을 프레임단위로 나누고 224 사이즈로 전처리해야하고, 필요하다면 정렬도 해야해. 5초짜리 클립이면 10~ 15 프레임을 전처리하고, enet-b0-va 모델 추론시켜서 특징벡터를 추출해서 평균과 표준편차를 계산해서 학습의 입력 데이터셋으로 준비해야해.
> 
> 결과 레이블은 어떤방식으로 할지 정해야해. 단순히 재미있다 없다로 할지. 다른 방법이 있을지.
> 
> 학습은 머신러닝 xgboost 모델을 사용할 계획이야. 다른 방법도 생각은 해보고 비교해봐도 좋을 거 같아.

좋은 계획이네요! 단계별로 구체화해보겠습니다.

## 1. 모델 구조 파악 및 로딩

### 우선 해야 할 것들:

```python
# 1. 모델 구조 확인
model = torch.load('enet_b0_8_va_mtl.pt', map_location='cpu')
print(model.keys())  # 저장된 키 확인
print(model.children())  # 레이어 구조 확인

# 2. 특징 벡터 차원 확인
# penultimate layer의 출력 차원 (논문에서는 D차원이라고 했는데 실제 값 확인 필요)
```

### 예상되는 이슈:

- EfficientNet-B0의 일반적인 특징 차원은 1280차원
- VA-MTL 버전이라면 구조가 변경되었을 수 있음

## 2. 데이터 수집 전략

### 클립 길이: **5-7초 추천**

- 하나의 "재밌는 순간"을 담기에 적절
- 너무 길면 여러 감정이 섞임

### 수집 방법 아이디어:

1. **유튜브 예능/코미디 클립**
    
    - 웃음소리가 많은 구간 vs 조용한 구간
    - 댓글 반응이 좋은 타임스탬프 활용
2. **스트리밍 하이라이트**
    
    - 트위치 클립 중 조회수/반응이 좋은 것들
    - 시청자 채팅 반응이 폭발하는 구간
3. **영화/드라마 장면**
    
    - 코미디 vs 일반 대화 장면
    - IMDb 평점이나 리뷰 기반

### 레이블링 방식 제안:

1. **이진 분류**: 재밌음(1) vs 보통(0)
2. **3단계**: 재밌음(2) vs 보통(1) vs 지루함(0)
3. **회귀**: 0-10점 척도 (크라우드소싱 활용)

## 3. 전처리 파이프라인

### 프레임 추출:

- 5초 × 2fps = 10프레임 (15프레임스킵 사용)
- 얼굴 감지 → 최소 3-4장 이상 필요

### 이미지 전처리:

- 224×224 리사이즈
- 얼굴 정렬 (논문에서 언급된 눈 기반 정렬)
- 정규화 (ImageNet 기준값 사용)

## 4. 학습 모델 비교

### XGBoost vs 대안들:

1. **XGBoost**: 좋은 선택, 특히 STAT 특징에 효과적
2. **Random Forest**: 해석 가능성 좋음
3. **Neural Network**: 특징 간 복잡한 관계 학습 가능
4. **SVM**: 논문에서 사용한 방식과 유사

### 특징 벡터 설계:

- 평균 (1280차원) + 표준편차 (1280차원) = 2560차원
- 추가로 최대/최소값도 고려해볼 수 있음

어떤 부분부터 구현해볼까요? 모델 구조 파악부터 시작하는 게 좋을 것 같은데요.


# analyze_model_structure.py 모델 구조 분석 스크립트
완벽하게 분석되었네요! 모델 구조가 명확해졌습니다:

## 핵심 정보

- **모델**: `timm.models.efficientnet.EfficientNet` (EfficientNet-B0)
- **특징 벡터 차원**: **1280차원**
- **분류기**: 10개 클래스 (8개 감정 + 2개 추가?)
- **총 파라미터**: 4,062,423개

## 중요한 발견

1. **`classifier.weight` 크기**: `[10, 1280]` → 입력이 1280차원
2. **`conv_head.weight`**: `[1280, 320, 1, 1]` → 320→1280 변환
3. **특징 추출기 생성**: ✅ 성공 (timm 사용)
4. **테스트 결과**: ✅ 1280차원 특징 벡터 출력 확인

## 다음 단계 계획

이제 모델 구조를 완전히 파악했으니:

1. **데이터 수집** - 유튜브 영상 클립들 + 재미도 레이블
2. **전처리 파이프라인** - 영상→프레임→얼굴감지→1280차원 특징추출
3. **STAT 특징 계산** - 평균(1280) + 표준편차(1280) = 2560차원
4. **XGBoost 학습** - 2560차원 → 재미도 점수

어떤 단계부터 진행할까요? 데이터 수집 방법부터 구체화해볼까요?

# 계획 변경
1280 > 2560 차원은 너무 크고, 파인튜닝이 어렵고 PCA 로 차원축소도 어려움. 데이터셋이 많이 필요한 상황

VA 모델의 분류헤드를 그대로 사용해서 
이미지당 10차원의 출력벡터(8종감정분류 + VA)를 얻고 이것을 클립길이 약 10~20프레임의 시간축 데이터를 구해서 

LSTM 또는 
`Conv1D` → 시간축 패턴 학습

으로 간단한 방식으로 학습을 하는게 효과적이라고 판단함.


## 계획 수행 단계
- VA 모델 테스트
- 영상 > VA 모델 배치처리 (비디오 프로세서)
		시간별 변화, 통계 데이터 수집 

- 재밌는 클립 수집 (5~10초 길이)
		영상길이가 다를때 어떻게 처리할지 고민
- 


> [!NOTE]
> 침착맨 웃긴 영상 클립을 5~7초 범위로 수집하려고해.
> 
> 그리고 그걸 재밌다 재미없다 이진분류로 나눌거고.
> 
> 감정분류 10차원 데이터에 영상길이 따라 시간축이 곱해진 데이터를 입력으로 해서 학습시킬건데 영상길이에 따라서 입력차원이 달라지는건 어떻게 일반화시킬지 고민이네. 그냥 영상길이를 동일하게 자를까? 아니면 앞부분에 0값을 채우는 방식?

# 영상 시간축 처리 방법법


## 가변 길이 시퀀스 처리 방법

### 1. **패딩 (Padding)** - 추천 ⭐

```python
# 최대 길이로 통일 (예: 7초 = 14프레임)
max_frames = 14
# 짧은 영상: 뒤에 0 패딩
# [감정데이터] + [0, 0, ..., 0]
```

**장점**: 구현 간단, 정보 손실 없음 **단점**: 계산 비용 증가

### 2. **고정 길이 자르기**

```python
# 모든 클립을 5초로 통일
fixed_length = 5  # 초
# 긴 영상: 앞/뒤 자르기
# 짧은 영상: 반복 또는 패딩
```

**장점**: 일관된 입력 크기 **단점**: 정보 손실 가능

### 3. **통계 특징으로 압축** - 가장 실용적 ⭐⭐

```python
# 길이와 무관하게 고정 차원으로 변환
features = [
    mean_10d,      # 평균 (10차원)
    std_10d,       # 표준편차 (10차원)  
    max_10d,       # 최대값 (10차원)
    min_10d        # 최소값 (10차원)
]
# 총 40차원 고정
```

## 추천 방식

**5-7초 클립이라면 통계 특징 압축**이 가장 좋을 것 같습니다:

- 길이 무관하게 **40차원 고정**
- 시간적 패턴도 표준편차로 포착
- **XGBoost/LSTM** 모두 사용 가능


## 패딩 사용의 장단점

### 패딩의 장점 ✅

- **시간적 순서 보존** - 감정 변화 패턴 그대로 유지
- **정보 손실 없음** - 원본 데이터 완전 보존
- **LSTM/GRU 최적화** - 시퀀스 모델에 이상적
- **마스킹 지원** - 0값 무시하고 학습 가능

### 패딩의 단점 ❌

- **계산 비용 증가** - 불필요한 0값 처리
- **메모리 사용량 증가** - 긴 시퀀스에 맞춰 할당
- **학습 불안정** - 0값이 많으면 gradient 문제

## 실제 구현 예시

```python
# 5초 = 10프레임, 7초 = 14프레임
max_length = 14  # 최대 길이로 통일

# 짧은 클립 (5초, 10프레임)
input_data = [frame1, frame2, ..., frame10]
padded_data = input_data + [zeros] * 4  # 뒤에 4개 패딩

# LSTM에서 마스킹 사용
model = LSTM(mask_zero=True)  # 0값 무시
```

## 패딩 vs 통계 특징

|측면|패딩|통계 특징|
|---|---|---|
|시간 패턴|**완전 보존**|일부 손실|
|계산 비용|높음|**낮음**|
|구현 복잡도|보통|**간단**|

**"놀람→웃음" 같은 감정 전환 패턴**이 중요하다면 **패딩이 맞습니다**.