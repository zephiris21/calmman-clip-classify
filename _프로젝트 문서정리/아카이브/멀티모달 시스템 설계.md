---
title: 멀티모달 시스템 설계
tags:
  - 프로젝트
created: 2025-06-06 10:02:00
related_project:
  - "[[감정인식 분류모델]]"
category: sub-project
link:
---
# 프로젝트 총 정리: 재밌는 영상 클립 분류 시스템

## 1. 프로젝트 개요

### **목표**

- 5~7초 클립에서 "재밌음/재미없음" 자동 분류
- 멀티모달 접근: 시각(감정) + 청각(음향) 특징 결합

### **기술 스택**

- **감정 분석**: VA-MTL EfficientNet (10차원 출력)
- **얼굴 탐지**: MTCNN + 얼굴 정렬
- **얼굴 인식**: FaceNet (특정인물 필터링)
- **시퀀스 모델**: Conv1D (LSTM 대신)
- **데이터**: HDF5 저장

## 2. 재활용 코드 결정

### **재활용할 핵심 코드** ✅

#### **A. VA 감정 모델 (va_emotion_core.py)**

```python
class VAEmotionCore:
    - extract_emotion_features()         # 단일 얼굴 → 10차원
    - extract_emotion_features_batch()   # 배치 처리
    - get_emotion_interpretation()       # 해석 기능
```

- **제거된 기능**: 디렉토리 분석, 복잡한 로드 방식
- **핵심 기능만**: 10차원 감정 벡터 추출

#### **B. MTCNN 얼굴 처리 (mtcnn_wrapper.py)**

```python
class FaceDetector:
    - process_image_batch()             # 배치 얼굴 탐지
    - align_face_by_eyes()              # 얼굴 정렬
```

#### **C. 비디오 프로세서 파이프라인 (torch_video_processor.py)**

```python
- _frame_reader_worker()              # 프레임 추출
- _face_detection_worker()            # MTCNN 배치 처리
- _filter_faces_by_recognition()      # 침착맨 얼굴 필터링 ⭐
- threading + queue 기반 파이프라인
- YAML config 시스템
```

## 3. 모듈 구성 결정

### **새로운 모듈 구조**

```
src/
├── models/
│   ├── va_emotion_core.py           # VA 감정 모델 코어
│   ├── face_recognition_core.py     # FaceNet 얼굴 인식 (기존 재활용)
│   └── funny_moment_classifier.py   # Conv1D 분류 모델
├── processors/
│   ├── video_processor.py           # 비디오 → 프레임 추출 + 얼굴 탐지
│   ├── emotion_extractor.py         # 얼굴 → 10차원 감정 특징
│   ├── audio_extractor.py           # 오디오 → 2차원 음향 특징  
│   ├── multimodal_processor.py      # 통합 12차원 시계열 생성
│   └── sliding_window_processor.py  # 긴 영상 추론용
├── dataset/
│   ├── clip_dataset_builder.py      # 학습용 데이터셋 생성
│   └── hdf5_loader.py               # HDF5 데이터 로더
└── utils/
    ├── config.py                    # 설정 관리
    └── visualization.py             # 감정 분포, Arousal 변화 시각화
```

## 4. 데이터 파이프라인 설계

### **A. 특징 추출 파이프라인**

```
영상 클립 (5~7초)
  ↓
프레임 추출 (동적 스킵) → 15프레임 고정
  ↓
MTCNN 얼굴 탐지 + 정렬
  ↓
FaceNet 유사도 필터링 (침착맨만)
  ↓
VA 모델 → 10차원 감정 벡터
  ↓
오디오 분석 → 2차원 음향 벡터
  ↓
결합 → 12차원 × 15프레임 시계열
```

### **B. 데이터 저장 형태**

```python
# HDF5 구조 (대용량 + 부분로딩 + 메타데이터)
{
    'features': [N, 15, 12],           # N개 클립의 시계열
    'labels': [N],                     # 0: 재미없음, 1: 재밌음  
    'metadata': {
        'video_names': [N],
        'clip_durations': [N],
        'source_timestamps': [N, 2],   # [시작, 끝]
        'frame_counts': [N]
    }
}
```

## 5. 핵심 기술 결정사항

### **A. 감정 레이블 순서 수정** ✅

```python
# 올바른 순서로 수정됨
emotion_labels = [
    'Anger', 'Contempt', 'Disgust', 'Fear',    # 0-3
    'Happiness', 'Neutral', 'Sadness', 'Surprise'  # 4-7
]
```

### **B. 시퀀스 길이 처리: 동적 프레임 스킵** ✅

```python
# 5초 클립: 5초/15프레임 = 0.33초 간격
# 7초 클립: 7초/15프레임 = 0.47초 간격
# 1.4배 차이 = 허용 가능한 "빨리감기" 수준
```

**장점**: 전체 클립 커버, 패딩 불필요 **단점**: 시간 해상도 차이 (하지만 허용 가능)

### **C. 모델 선택: Conv1D over LSTM** ✅

**이유**:

- 짧은 시퀀스 (15프레임)에 적합
- Local pattern 감지 효과적
- 빠른 학습, Gradient 안정성

### **D. 얼굴 인식 필터링 전략** ✅

- **FaceNet 유사도 매칭**으로 침착맨만 추출
- **실패 케이스 보완**:
    - 임계값 완화 (MTCNN 0.9→0.7, FaceNet 0.5→0.3)
    - 시간적 보간 (앞뒤 프레임으로)
    - 심한 경우 클립 제외 (실패율 30% 이상)

## 6. 데이터 활용 계획

### **A. 시각화 기능**

1. **감정 분포 파이차트**: 영상 전체 감정 비율
2. **Arousal 시간축 변화**: 텐션 변화 패턴
3. **Valence 변화**: 긍정성 변화 추이

### **B. 학습 데이터 생성**

- **5~7초 클립** → **15프레임 × 12차원** → **재밌음/재미없음 라벨**

### **C. 추론 시스템**

- **긴 영상** → **Sliding Window** → **재밌는 순간 탐지**

## 7. 기술적 최적화

### **A. 성능 최적화**

- **Threading + Queue** 기반 파이프라인
- **배치 처리**: MTCNN, FaceNet, VA 모델 모두
- **GPU 메모리 관리**: timm 0.9.x 다운그레이드로 호환성 확보

### **B. 데이터 관리**

- **HDF5**: 대용량 시계열 데이터 최적화
- **부분 로딩**: 메모리 효율성
- **메타데이터**: 원본 추적, 디버깅 지원

## 8. 다음 구현 단계

1. **오디오 특징 추출기** 구현 (2차원: RMS + 변화율)
2. **torch_video_processor → VA 버전** 수정
3. **멀티모달 통합 프로세서** 구현
4. **Conv1D 분류 모델** 구현
5. **HDF5 데이터셋 빌더** 구현

## 결론

**완성도 높은 멀티모달 시스템 설계**가 완료되었습니다. 기존 코드의 핵심 부분을 효과적으로 재활용하면서, 새로운 요구사항에 맞는 최적화된 파이프라인을 구성했습니다. 특히 **동적 프레임 스킵**과 **얼굴 인식 필터링** 전략이 핵심 차별화 요소입니다! 🎯