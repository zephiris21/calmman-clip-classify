#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import cv2
import yaml
import time
import json
import logging
import threading
import queue
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Optional

import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image
from facenet_pytorch import InceptionResnetV1

from mtcnn_wrapper import FaceDetector
from pytorch_classifier import TorchFacialClassifier


class TorchVideoProcessor:
    """
    PyTorch ê¸°ë°˜ ì¹¨ì°©ë§¨ í‚¹ë°›ëŠ” ìˆœê°„ íƒì§€ë¥¼ ìœ„í•œ ë¹„ë””ì˜¤ í”„ë¡œì„¸ì„œ
    ì–¼êµ´ ì¸ì‹ ê¸°ëŠ¥ í†µí•©
    """
    
    def __init__(self, config_path: str = "config/config_torch.yaml"):
        """
        ë¹„ë””ì˜¤ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        
        Args:
            config_path (str): ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        # PyTorch ì„¤ì • ìµœì í™” - CUDA ì»¨í…ìŠ¤íŠ¸ ì˜í–¥ ìµœì†Œí™”
        if torch.cuda.is_available():
            # CUDA í• ë‹¹ì ìµœì í™”
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
            
            # PyTorch ë²„ì „ì— ë”°ë¥¸ ì•ˆì „í•œ CUDA ì„¤ì •
            try:
                # JIT fusion ë¹„í™œì„±í™” ì‹œë„ (ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
                if hasattr(torch, '_C'):
                    if hasattr(torch._C, '_jit_set_nvfuser_enabled'):
                        torch._C._jit_set_nvfuser_enabled(False)
                
                # CUDA ê·¸ë˜í”„ ë¹„í™œì„±í™” (ë¶ˆí•„ìš”í•œ ìµœì í™” ë°©ì§€)
                if hasattr(torch.cuda, 'graph'):
                    torch.cuda.graph.disable_compute_capability_caching()
                
                # ë©”ëª¨ë¦¬ ìºì‹± ì •ì±… ì„¤ì •
                if hasattr(torch.cuda, 'set_per_process_memory_fraction'):
                    torch.cuda.set_per_process_memory_fraction(0.7)  # GPU ë©”ëª¨ë¦¬ì˜ 70%ë§Œ ì‚¬ìš©
            except Exception as e:
                print(f"CUDA ìµœì í™” ì„¤ì • ì¤‘ ê²½ê³  (ë¬´ì‹œë¨): {e}")
        
        # ì„¤ì • ë¡œë“œ
        self.config = self._load_config(config_path)
        
        # ë¡œê¹… ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self._setup_logging()
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë³€ìˆ˜
        self.stats = {
            'frames_processed': 0,
            'faces_detected': 0,
            'faces_recognized': 0,      # ìƒˆë¡œ ì¶”ê°€: ì¸ì‹ëœ ì–¼êµ´ ìˆ˜
            'faces_filtered': 0,        # ìƒˆë¡œ ì¶”ê°€: í•„í„°ë§ëœ ì–¼êµ´ ìˆ˜
            'angry_moments': 0,
            'processing_start_time': None,
            'last_stats_time': time.time(),
            'batch_count': 0,
            'total_inference_time': 0,
            'total_recognition_time': 0  # ìƒˆë¡œ ì¶”ê°€: ì–¼êµ´ ì¸ì‹ ì‹œê°„
        }
        
        # ì¢…ë£Œ í”Œë˜ê·¸ ì¶”ê°€
        self.stop_flag = False
        self.face_detection_done = False
        self.classification_done = False
        
        # í ìƒì„±
        self.frame_queue = queue.Queue(maxsize=self.config['performance']['max_queue_size'])
        self.face_queue = queue.Queue(maxsize=self.config['performance']['max_queue_size'])
        self.result_queue = queue.Queue()
        
        # ì–¼êµ´ íƒì§€ê¸° ì´ˆê¸°í™”
        self._init_face_detector()
        
        # ì–¼êµ´ ì¸ì‹ ëª¨ë¸ ì´ˆê¸°í™” (ì˜µì…˜)
        self._init_face_recognition()
        
        # PyTorch ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ
        self._load_classifier()
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self._create_output_dirs()
        
        self.logger.info("âœ… TorchVideoProcessor ì´ˆê¸°í™” ì™„ë£Œ")
        self._print_config_summary()
    
    def _load_config(self, config_path: str) -> Dict:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            print(f"âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ: {config_path}")
            return config
        except Exception as e:
            print(f"âŒ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def _setup_logging(self):
        """ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •"""
        # ë¡œê±° ìƒì„±
        self.logger = logging.getLogger('TorchVideoProcessor')
        self.logger.setLevel(getattr(logging, self.config['logging']['level']))
        
        # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)
        
        # í¬ë§¤í„° ì„¤ì •
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
        # ì½˜ì†” í•¸ë“¤ëŸ¬
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        self.logger.addHandler(console_handler)
        
        # íŒŒì¼ í•¸ë“¤ëŸ¬ (ì˜µì…˜)
        if self.config['logging']['save_logs']:
            log_dir = "results/video_processing/logs"
            os.makedirs(log_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_file = os.path.join(log_dir, f"torch_processing_{timestamp}.log")
            
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setFormatter(formatter)
            self.logger.addHandler(file_handler)
            
            self.logger.info(f"ë¡œê·¸ íŒŒì¼: {log_file}")
    
    def _init_face_detector(self):
        """MTCNN ì–¼êµ´ íƒì§€ê¸° ì´ˆê¸°í™”"""
        mtcnn_config = self.config['mtcnn']
        
        self.face_detector = FaceDetector(
            image_size=mtcnn_config['image_size'],
            margin=mtcnn_config['margin'],
            prob_threshold=mtcnn_config['prob_threshold'],
            align_faces=mtcnn_config['align_faces']
        )
        
        self.logger.info(f"âœ… MTCNN ì´ˆê¸°í™” ì™„ë£Œ (ë°°ì¹˜ í¬ê¸°: {mtcnn_config['batch_size']})")
    
    def _init_face_recognition(self):
        """FaceNet ì–¼êµ´ ì¸ì‹ ëª¨ë¸ ì´ˆê¸°í™”"""
        face_recog_config = self.config.get('face_recognition', {})
        
        # ì–¼êµ´ ì¸ì‹ì´ ë¹„í™œì„±í™”ë˜ì—ˆê±°ë‚˜ í…ŒìŠ¤íŠ¸ ëª¨ë“œì¸ ê²½ìš° ëª¨ë¸ì„ ë¡œë“œí•˜ì§€ ì•ŠìŒ
        if not face_recog_config.get('enabled', False) or face_recog_config.get('test_mode', False):
            mode_str = "ë¹„í™œì„±í™”ë¨" if not face_recog_config.get('enabled', False) else "í…ŒìŠ¤íŠ¸ ëª¨ë“œ"
            self.logger.info(f"âš ï¸ ì–¼êµ´ ì¸ì‹ {mode_str} - ëª¨ë¸ ë¡œë“œ ì•ˆí•¨")
            self.facenet_model = None
            self.target_embedding = None
            return
        
        try:
            # ëª…ì‹œì ìœ¼ë¡œ CPU ë¬¸ìì—´ ëŒ€ì‹  torch.device ê°ì²´ ì‚¬ìš©
            cpu_device = torch.device('cpu')
            
            # FaceNetì„ ëª…ì‹œì ìœ¼ë¡œ CPUì— ë¡œë“œ
            with torch.no_grad():
                # ë¨¼ì € ê¸°ë³¸ ë””ë°”ì´ìŠ¤ì— ë¡œë“œí•œ í›„ CPUë¡œ ì´ë™
                self.facenet_model = InceptionResnetV1(pretrained='vggface2')
                self.facenet_model = self.facenet_model.to(cpu_device).eval()
            
            # íƒ€ê²Ÿ ì„ë² ë”©ë„ ëª…ì‹œì ìœ¼ë¡œ CPUë¡œ
            embedding_path = face_recog_config['embedding_path']
            if os.path.exists(embedding_path):
                embedding_data = np.load(embedding_path, allow_pickle=True).item()
                # numpy ë°°ì—´ì—ì„œ í…ì„œë¡œ ë³€í™˜ í›„ CPUë¡œ ì´ë™
                self.target_embedding = torch.tensor(embedding_data['embedding']).to(cpu_device)
                
                self.logger.info(f"âœ… ì–¼êµ´ ì¸ì‹ ì´ˆê¸°í™” ì™„ë£Œ (ëª…ì‹œì  CPU ë””ë°”ì´ìŠ¤)")
                self.logger.info(f"   ì„ë² ë”© íŒŒì¼: {embedding_path}")
                self.logger.info(f"   ì‚¬ìš©ëœ ì´ë¯¸ì§€: {embedding_data['num_images']}ê°œ")
                self.logger.info(f"   ìœ ì‚¬ë„ ì„ê³„ê°’: {face_recog_config['similarity_threshold']}")
            else:
                self.logger.error(f"âŒ ì„ë² ë”© íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {embedding_path}")
                self.facenet_model = None
                self.target_embedding = None
            
        except Exception as e:
            self.logger.error(f"âŒ ì–¼êµ´ ì¸ì‹ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            import traceback
            self.logger.error(traceback.format_exc())  # ìƒì„¸ ì˜¤ë¥˜ ìŠ¤íƒ ì¶œë ¥
            self.facenet_model = None
            self.target_embedding = None
    
    def _load_classifier(self):
        """PyTorch ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ"""
        try:
            self.classifier = TorchFacialClassifier(self.config)
            self.logger.info("âœ… PyTorch ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"âŒ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def _create_output_dirs(self):
        """ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
        base_dir = self.config['output']['base_dir']
        os.makedirs(base_dir, exist_ok=True)
        
        if self.config['logging']['save_logs']:
            os.makedirs("logs", exist_ok=True)
    
    def _create_video_output_dir(self, video_path: str) -> str:
        """ì˜ìƒë³„ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
        video_name = Path(video_path).stem
        timestamp = datetime.now().strftime("%Y%m%d")
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        safe_name = "".join(c for c in video_name if c.isalnum() or c in (' ', '-', '_')).rstrip()
        
        video_dir = os.path.join(
            self.config['output']['base_dir'],
            f"{timestamp}_{safe_name}"
        )
        
        # í•˜ìœ„ ë””ë ‰í† ë¦¬ ìƒì„±
        if self.config['output']['save_highlights']:
            os.makedirs(os.path.join(video_dir, "highlights"), exist_ok=True)
        
        if self.config['output']['save_timestamps']:
            os.makedirs(os.path.join(video_dir, "timestamps"), exist_ok=True)
        
        if self.config['output']['save_processing_log']:
            os.makedirs(os.path.join(video_dir, "logs"), exist_ok=True)
        
        return video_dir
    
    def _print_config_summary(self):
        """ì„¤ì • ìš”ì•½ ì¶œë ¥"""
        self.logger.info("ğŸ“‹ ì„¤ì • ìš”ì•½:")
        self.logger.info(f"   í”„ë ˆì„ ìŠ¤í‚µ: {self.config['video']['frame_skip']}í”„ë ˆì„ë§ˆë‹¤")
        self.logger.info(f"   MTCNN ë°°ì¹˜: {self.config['mtcnn']['batch_size']}")
        
        # ì–¼êµ´ ì¸ì‹ ì„¤ì • ì¶œë ¥
        if self.config.get('face_recognition', {}).get('enabled', False):
            face_recog_config = self.config['face_recognition']
            self.logger.info(f"   ì–¼êµ´ ì¸ì‹: í™œì„±í™” (ì„ê³„ê°’: {face_recog_config['similarity_threshold']})")
        else:
            self.logger.info(f"   ì–¼êµ´ ì¸ì‹: ë¹„í™œì„±í™”")
            
        self.logger.info(f"   ë¶„ë¥˜ ë°°ì¹˜: {self.config['classifier']['batch_size']}")
        self.logger.info(f"   ë°°ì¹˜ íƒ€ì„ì•„ì›ƒ: {self.config['classifier']['batch_timeout']}ì´ˆ")
        self.logger.info(f"   í í¬ê¸°: {self.config['performance']['max_queue_size']}")
        self.logger.info(f"   ë””ë°”ì´ìŠ¤: {self.config['classifier']['device']}")
    
    def _get_face_embeddings_batch(self, face_images: List[Image.Image]) -> torch.Tensor:
        """
        ì–¼êµ´ ì´ë¯¸ì§€ ë°°ì¹˜ì—ì„œ ì„ë² ë”© ì¶”ì¶œ
        
        Args:
            face_images (List[PIL.Image]): 224x224 ì–¼êµ´ ì´ë¯¸ì§€ë“¤
            
        Returns:
            torch.Tensor: ì •ê·œí™”ëœ ì„ë² ë”© ë²¡í„°ë“¤ [batch_size, 512]
        """
        if self.facenet_model is None:
            return None
        
        try:
            # 224x224 â†’ 160x160 ë¦¬ì‚¬ì´ì§• (FaceNetìš©)
            resized_images = []
            for face_img in face_images:
                resized_img = face_img.resize((160, 160), Image.BILINEAR)
                img_array = np.array(resized_img)
                img_tensor = torch.from_numpy(img_array).float().permute(2, 0, 1)
                img_tensor = (img_tensor - 127.5) / 128.0  # ì •ê·œí™” [-1, 1]
                resized_images.append(img_tensor)
            
            # ë°°ì¹˜ í…ì„œ ìƒì„± (facenet_modelê³¼ ë™ì¼í•œ ë””ë°”ì´ìŠ¤ ì‚¬ìš©)
            batch_tensor = torch.stack(resized_images).to(self.facenet_model.device)
            
            # ì„ë² ë”© ì¶”ì¶œ
            with torch.no_grad():
                embeddings = self.facenet_model(batch_tensor)
                embeddings = F.normalize(embeddings, p=2, dim=1)  # L2 ì •ê·œí™”
            
            return embeddings
            
        except Exception as e:
            self.logger.error(f"ë°°ì¹˜ ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            import traceback
            self.logger.error(traceback.format_exc())  # ìƒì„¸ ì˜¤ë¥˜ ìŠ¤íƒ ì¶œë ¥
            return None
    
    def _calculate_similarities_batch(self, embeddings: torch.Tensor) -> torch.Tensor:
        """
        ë°°ì¹˜ ì„ë² ë”©ê³¼ íƒ€ê²Ÿ ì„ë² ë”© ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        
        Args:
            embeddings (torch.Tensor): ë°°ì¹˜ ì„ë² ë”© [batch_size, 512]
            
        Returns:
            torch.Tensor: ìœ ì‚¬ë„ ê°’ë“¤ [batch_size]
        """
        if embeddings is None or self.target_embedding is None:
            return None
        
        try:
            # ê³„ì‚°ì„ ìœ„í•´ ì„ë² ë”©ê³¼ íƒ€ê²Ÿì´ ê°™ì€ ë””ë°”ì´ìŠ¤ì— ìˆì–´ì•¼ í•¨
            device = embeddings.device
            target_embedding = self.target_embedding.to(device)
            
            # íƒ€ê²Ÿ ì„ë² ë”©ì„ ë°°ì¹˜ í¬ê¸°ë¡œ í™•ì¥
            target_expanded = target_embedding.unsqueeze(0).repeat(embeddings.size(0), 1)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = F.cosine_similarity(embeddings, target_expanded)
            
            return similarities
        except Exception as e:
            self.logger.error(f"ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            import traceback
            self.logger.error(traceback.format_exc())  # ìƒì„¸ ì˜¤ë¥˜ ìŠ¤íƒ ì¶œë ¥
            return None
    
    def process_video(self, video_path: str) -> Dict:
        """
        ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬
        
        Args:
            video_path (str): ì²˜ë¦¬í•  ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            Dict: ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
        """
        if not os.path.exists(video_path):
            raise FileNotFoundError(f"ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        
        self.logger.info(f"ğŸ¬ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘: {os.path.basename(video_path)}")
        
        # ì˜ìƒë³„ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.video_output_dir = self._create_video_output_dir(video_path)
        self.logger.info(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.video_output_dir}")
        
        # ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        video_info = self._get_video_info(video_path)
        self.logger.info(f"   ê¸¸ì´: {video_info['duration']:.1f}ì´ˆ, FPS: {video_info['fps']:.1f}")
        
        # ê²°ê³¼ ì €ì¥ìš© ë° í”Œë˜ê·¸ ì´ˆê¸°í™”
        self.angry_moments = []
        self.stats['processing_start_time'] = time.time()
        self.face_detection_done = False
        self.classification_done = False
        self.stop_flag = False
        
        # ìŠ¤ë ˆë“œ ì‹œì‘
        threads = []
        
        # 1. í”„ë ˆì„ ì½ê¸° ìŠ¤ë ˆë“œ
        frame_thread = threading.Thread(
            target=self._frame_reader_worker, 
            args=(video_path,)
        )
        threads.append(frame_thread)
        
        # 2. ì–¼êµ´ íƒì§€ ìŠ¤ë ˆë“œ
        face_thread = threading.Thread(target=self._face_detection_worker)
        threads.append(face_thread)
        
        # 3. ë°°ì¹˜ ë¶„ë¥˜ ìŠ¤ë ˆë“œ
        classify_thread = threading.Thread(target=self._batch_classification_worker)
        threads.append(classify_thread)
        
        # 4. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ (ë°ëª¬ìœ¼ë¡œ ì„¤ì •)
        monitor_thread = threading.Thread(target=self._performance_monitor)
        monitor_thread.daemon = True
        threads.append(monitor_thread)
        
        # ëª¨ë“  ìŠ¤ë ˆë“œ ì‹œì‘
        for thread in threads:
            thread.start()
        
        # í”„ë ˆì„ ì½ê¸° ì™„ë£Œ ëŒ€ê¸°
        frame_thread.join()
        self.logger.info("âœ… í”„ë ˆì„ ì½ê¸° ì™„ë£Œ")
        
        # ë‘ ì‘ì—… ëª¨ë‘ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        self.logger.info("â³ ì–¼êµ´ íƒì§€ ë° ë¶„ë¥˜ ì™„ë£Œ ëŒ€ê¸° ì¤‘...")
        while not (self.face_detection_done and self.classification_done):
            time.sleep(0.1)
        
        self.logger.info("âœ… ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ!")
        
        # ì¢…ë£Œ í”Œë˜ê·¸ ì„¤ì •
        self.stop_flag = True
        
        # ì›Œì»¤ ìŠ¤ë ˆë“œë“¤ ì •ë¦¬
        face_thread.join(timeout=2)
        classify_thread.join(timeout=2)
        
        # ê²°ê³¼ ì €ì¥
        results = self._save_results(video_path, video_info)
        
        # ìµœì¢… í†µê³„ ì¶œë ¥
        self._print_final_stats()
        
        return results
    
    def _get_video_info(self, video_path: str) -> Dict:
        """ë¹„ë””ì˜¤ ì •ë³´ ì¶”ì¶œ"""
        cap = cv2.VideoCapture(video_path)
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = frame_count / fps if fps > 0 else 0
        
        cap.release()
        
        return {
            'fps': fps,
            'frame_count': frame_count,
            'duration': duration
        }
    
    def _frame_reader_worker(self, video_path: str):
        """í”„ë ˆì„ ì½ê¸° ì›Œì»¤"""
        cap = cv2.VideoCapture(video_path)
        frame_skip = self.config['video']['frame_skip']
        frame_count = 0
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # í”„ë ˆì„ ìŠ¤í‚µ
                if frame_count % frame_skip != 0:
                    frame_count += 1
                    continue
                
                # BGR to RGB ë³€í™˜
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # íƒ€ì„ìŠ¤íƒ¬í”„ ê³„ì‚°
                timestamp = frame_count / cap.get(cv2.CAP_PROP_FPS)
                
                # íì— ì¶”ê°€
                frame_data = {
                    'frame': frame_rgb,
                    'frame_number': frame_count,
                    'timestamp': timestamp
                }
                
                self.frame_queue.put(frame_data)
                frame_count += 1
                
        except Exception as e:
            self.logger.error(f"âŒ í”„ë ˆì„ ì½ê¸° ì˜¤ë¥˜: {e}")
        finally:
            cap.release()
            # ì¢…ë£Œ ì‹ í˜¸
            self.frame_queue.put(None)
            self.logger.info("ğŸ“¹ í”„ë ˆì„ ì½ê¸° ì›Œì»¤ ì¢…ë£Œ")
    
    def _face_detection_worker(self):
        """ì–¼êµ´ íƒì§€ ì›Œì»¤"""
        batch_size = self.config['mtcnn']['batch_size']
        frame_batch = []
        
        try:
            while True:
                frame_data = self.frame_queue.get()
                
                if frame_data is None:  # ì¢…ë£Œ ì‹ í˜¸
                    # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
                    if frame_batch:
                        self._process_face_batch(frame_batch)
                    self.face_queue.put(None)  # ë‹¤ìŒ ì›Œì»¤ì— ì¢…ë£Œ ì‹ í˜¸
                    break
                
                frame_batch.append(frame_data)
                
                # ë°°ì¹˜ê°€ ì°¼ìœ¼ë©´ ì²˜ë¦¬
                if len(frame_batch) >= batch_size:
                    self._process_face_batch(frame_batch)
                    frame_batch = []
                
                self.frame_queue.task_done()
                
        except Exception as e:
            self.logger.error(f"âŒ ì–¼êµ´ íƒì§€ ì˜¤ë¥˜: {e}")
        finally:
            self.face_detection_done = True
            self.logger.info("âœ… ì–¼êµ´ íƒì§€ ì™„ë£Œ")
    
    def _process_face_batch(self, frame_batch: List[Dict]):
        """í”„ë ˆì„ ë°°ì¹˜ì—ì„œ ì–¼êµ´ íƒì§€ ë° ì¸ì‹ (MTCNN + FaceNet ë°°ì¹˜ ì²˜ë¦¬)"""
        batch_start_time = time.time()
        
        try:
            # PIL ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            pil_images = []
            frame_metadata_list = []
            
            for frame_data in frame_batch:
                try:
                    pil_image = Image.fromarray(frame_data['frame'])
                    pil_images.append(pil_image)
                    frame_metadata_list.append({
                        'frame_number': frame_data['frame_number'],
                        'timestamp': frame_data['timestamp']
                    })
                except Exception as e:
                    self.logger.warning(f"âš ï¸ í”„ë ˆì„ {frame_data['frame_number']} PIL ë³€í™˜ ì‹¤íŒ¨: {e}")
            
            if not pil_images:
                self.logger.warning("âš ï¸ ë³€í™˜ëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # MTCNN ë°°ì¹˜ ì²˜ë¦¬ í˜¸ì¶œ (224x224 ì–¼êµ´ ì´ë¯¸ì§€ ì¶”ì¶œ)
            face_results = self.face_detector.process_image_batch(
                pil_images, frame_metadata_list
            )
            
            # ì›ë³¸ íƒì§€ ê²°ê³¼ì˜ ì–¼êµ´ ìˆ˜ ì €ì¥ (ë¡œê¹…ìš©)
            total_detected = len([r for r in face_results if 'face_image' in r])
            
            # ì–¼êµ´ ì¸ì‹ í™œì„±í™” + ëª¨ë¸ ë¡œë“œëœ ê²½ìš°ì—ë§Œ í•„í„°ë§ ìˆ˜í–‰
            # FaceNet ëª¨ë¸ì´ Noneì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì–¼êµ´ ì¸ì‹ ìˆ˜í–‰ (í…ŒìŠ¤íŠ¸ ëª¨ë“œëŠ” ì´ë¯¸ Noneìœ¼ë¡œ ì„¤ì •ë¨)
            if self.facenet_model is not None:
                face_results = self._filter_faces_by_recognition(face_results)
            
            # ê²°ê³¼ë¥¼ face_queueì— ì¶”ê°€
            faces_in_batch = 0
            for face_data in face_results:
                try:
                    # face_queueì— ì¶”ê°€í•  ë°ì´í„° êµ¬ì„±
                    queue_data = {
                        'face_image': face_data['face_image'],
                        'frame_number': face_data['frame_number'],
                        'timestamp': face_data['timestamp']
                    }
                    self.face_queue.put(queue_data)
                    faces_in_batch += 1
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ì–¼êµ´ ë°ì´í„° í ì¶”ê°€ ì‹¤íŒ¨: {e}")
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.stats['frames_processed'] += len(frame_batch)
            self.stats['faces_detected'] += total_detected
            
            batch_time = time.time() - batch_start_time
            
            # ë°°ì¹˜ ë‹¨ìœ„ ë¡œê¹…
            if self.config['logging']['batch_summary']:
                recognition_info = ""
                # ì–¼êµ´ ì¸ì‹ ëª¨ë¸ì´ ë¡œë“œëœ ê²½ìš°ì—ë§Œ í•„í„°ë§ ì •ë³´ ê³„ì‚°
                if self.facenet_model is not None:
                    filtered = total_detected - faces_in_batch
                    recognition_info = f" (ì¸ì‹ í›„: {faces_in_batch}ê°œ, í•„í„°ë§: {filtered}ê°œ)"
                
                self.logger.info(
                    f"ì–¼êµ´ íƒì§€ ë°°ì¹˜: {len(frame_batch)}í”„ë ˆì„ â†’ {faces_in_batch}ê°œ ì–¼êµ´{recognition_info} "
                    f"({batch_time:.2f}ì´ˆ)"
                )
        
        except Exception as e:
            self.logger.error(f"âŒ ì–¼êµ´ íƒì§€ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œì—ë„ í†µê³„ëŠ” ì—…ë°ì´íŠ¸
            self.stats['frames_processed'] += len(frame_batch)
    
    def _filter_faces_by_recognition(self, face_results: List[Dict]) -> List[Dict]:
        """
        ì–¼êµ´ ì¸ì‹ìœ¼ë¡œ ì¹¨ì°©ë§¨ ì–¼êµ´ë§Œ í•„í„°ë§
        
        Args:
            face_results (List[Dict]): MTCNN íƒì§€ ê²°ê³¼
            
        Returns:
            List[Dict]: í•„í„°ë§ëœ ì–¼êµ´ ê²°ê³¼
        """
        # ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ê±°ë‚˜ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ í•„í„°ë§ ì—†ì´ ë°˜í™˜
        if not face_results or self.facenet_model is None:
            return face_results
        
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™•ì¸ (ì„¤ì • íŒŒì¼ì—ì„œ test_modeê°€ trueë©´ í•„í„°ë§ ê±´ë„ˆëœ€)
        if self.config.get('face_recognition', {}).get('test_mode', False):
            self.logger.info("ğŸ§ª ì–¼êµ´ ì¸ì‹ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: í•„í„°ë§ ê±´ë„ˆëœ€")
            return face_results
        
        try:
            recognition_start_time = time.time()
            
            # ì–¼êµ´ ì´ë¯¸ì§€ë“¤ ì¶”ì¶œ
            face_images = [result['face_image'] for result in face_results]
            
            # ë°°ì¹˜ ì„ë² ë”© ì¶”ì¶œ
            embeddings = self._get_face_embeddings_batch(face_images)
            
            if embeddings is None:
                return face_results
            
            # ë°°ì¹˜ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = self._calculate_similarities_batch(embeddings)
            
            if similarities is None:
                return face_results
            
            # ì„ê³„ê°’ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§
            threshold = self.config['face_recognition']['similarity_threshold']
            matches = similarities > threshold
            
            # í•„í„°ë§ëœ ê²°ê³¼ ìƒì„±
            filtered_results = []
            recognized_count = 0
            
            for i, (result, similarity, is_match) in enumerate(zip(face_results, similarities, matches)):
                if is_match:
                    # ìœ ì‚¬ë„ ì •ë³´ ì¶”ê°€
                    result['similarity'] = float(similarity)
                    filtered_results.append(result)
                    recognized_count += 1
                else:
                    # í•„í„°ë§ëœ ì–¼êµ´ ì €ì¥ (ì˜µì…˜)
                    if self.config['face_recognition']['logging']['save_filtered_faces']:
                        self._save_filtered_face(result, float(similarity))
            
            recognition_time = time.time() - recognition_start_time
            self.stats['total_recognition_time'] += recognition_time
            self.stats['faces_recognized'] += recognized_count
            self.stats['faces_filtered'] += (len(face_results) - recognized_count)
            
            # ì¸ì‹ í†µê³„ ë¡œê¹…
            if self.config['face_recognition']['logging']['log_filtered_count']:
                self.logger.debug(
                    f"ì–¼êµ´ ì¸ì‹: {len(face_results)}ê°œ â†’ {recognized_count}ê°œ "
                    f"(í•„í„°ë§: {len(face_results) - recognized_count}ê°œ, {recognition_time:.3f}ì´ˆ)"
                )
            
            return filtered_results
            
        except Exception as e:
            self.logger.error(f"âŒ ì–¼êµ´ ì¸ì‹ í•„í„°ë§ ì‹¤íŒ¨: {e}")
            import traceback
            self.logger.error(traceback.format_exc())  # ìƒì„¸ ì˜¤ë¥˜ ìŠ¤íƒ ì¶œë ¥
            return face_results  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
    
    def _save_filtered_face(self, face_data: Dict, similarity: float):
        """í•„í„°ë§ëœ ì–¼êµ´ ì´ë¯¸ì§€ ì €ì¥ (ë””ë²„ê¹…ìš©)"""
        try:
            timestamp_str = f"{int(face_data['timestamp']):05d}"
            filename = f"filtered_{timestamp_str}_{similarity:.3f}.jpg"
            
            filtered_dir = os.path.join(self.video_output_dir, "filtered_faces")
            os.makedirs(filtered_dir, exist_ok=True)
            
            save_path = os.path.join(filtered_dir, filename)
            face_data['face_image'].save(save_path)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í•„í„°ë§ëœ ì–¼êµ´ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _batch_classification_worker(self):
        """ë°°ì¹˜ ë¶„ë¥˜ ì›Œì»¤"""
        batch_size = self.config['classifier']['batch_size']
        timeout = self.config['classifier']['batch_timeout']
        
        face_batch = []
        last_batch_time = time.time()
        
        try:
            while True:
                try:
                    # íƒ€ì„ì•„ì›ƒ ì„¤ì •ìœ¼ë¡œ ì–¼êµ´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                    remaining_timeout = max(0.1, timeout - (time.time() - last_batch_time))
                    face_data = self.face_queue.get(timeout=remaining_timeout)
                    
                    if face_data is None:  # ì¢…ë£Œ ì‹ í˜¸
                        # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
                        if face_batch:
                            self._process_classification_batch(face_batch)
                        break
                    
                    face_batch.append(face_data)
                    
                    # ë°°ì¹˜ ì²˜ë¦¬ ì¡°ê±´ í™•ì¸
                    should_process = (
                        len(face_batch) >= batch_size or  # ë°°ì¹˜ê°€ ê°€ë“ ì°¸
                        (self.face_detection_done and len(face_batch) > 0) or  # íƒì§€ ì™„ë£Œ + ë‚¨ì€ ë°°ì¹˜
                        (time.time() - last_batch_time) >= timeout  # íƒ€ì„ì•„ì›ƒ
                    )
                    
                    if should_process:
                        self._process_classification_batch(face_batch)
                        face_batch = []
                        last_batch_time = time.time()
                    
                    self.face_queue.task_done()
                    
                except queue.Empty:
                    # íƒ€ì„ì•„ì›ƒ ë°œìƒ - í˜„ì¬ ë°°ì¹˜ ì²˜ë¦¬
                    if face_batch:
                        if self.config['logging']['batch_summary']:
                            self.logger.info(f"íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ë°°ì¹˜ ì²˜ë¦¬: {len(face_batch)}ê°œ")
                        self._process_classification_batch(face_batch)
                        face_batch = []
                        last_batch_time = time.time()
                
        except Exception as e:
            self.logger.error(f"âŒ ë¶„ë¥˜ ì›Œì»¤ ì˜¤ë¥˜: {e}")
        finally:
            self.classification_done = True
            self.logger.info("âœ… ë¶„ë¥˜ ì²˜ë¦¬ ì™„ë£Œ")
    
    def _process_classification_batch(self, face_batch: List[Dict]):
        """ë¶„ë¥˜ ë°°ì¹˜ ì²˜ë¦¬"""
        if not face_batch:
            return
        
        try:
            # ì–¼êµ´ ì´ë¯¸ì§€ë“¤ ì¶”ì¶œ
            face_images = [face_data['face_image'] for face_data in face_batch]
            
            # ë°°ì¹˜ ì˜ˆì¸¡
            batch_start_time = time.time()
            predictions = self.classifier.predict_batch(face_images)
            batch_time = time.time() - batch_start_time
            
            self.stats['batch_count'] += 1
            self.stats['total_inference_time'] += batch_time
            
            # ê²°ê³¼ ì²˜ë¦¬
            angry_count = 0
            for face_data, prediction in zip(face_batch, predictions):
                if prediction['is_angry']:
                    angry_moment = {
                        'timestamp': face_data['timestamp'],
                        'frame_number': face_data['frame_number'],
                        'confidence': prediction['confidence']
                    }
                    
                    # ì–¼êµ´ ì¸ì‹ ìœ ì‚¬ë„ ì •ë³´ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
                    if 'similarity' in face_data:
                        angry_moment['similarity'] = face_data['similarity']
                    
                    self.angry_moments.append(angry_moment)
                    self.stats['angry_moments'] += 1
                    angry_count += 1
                    
                    # í‚¹ë°›ëŠ” í”„ë ˆì„ ì €ì¥ (ì˜µì…˜)
                    if self.config['output']['save_highlights']:
                        self._save_highlight_image(face_data, prediction['confidence'])
                    
                    if self.config['debug']['timing_detailed']:
                        timestamp_str = str(timedelta(seconds=int(face_data['timestamp'])))
                        similarity_info = f", ìœ ì‚¬ë„: {face_data.get('similarity', 'N/A'):.3f}" if 'similarity' in face_data else ""
                        self.logger.info(f"ğŸ˜¡ í‚¹ë°›ëŠ” ìˆœê°„! {timestamp_str} (ì‹ ë¢°ë„: {prediction['confidence']:.3f}{similarity_info})")
            
        except Exception as e:
            self.logger.error(f"âš ï¸ ë°°ì¹˜ ë¶„ë¥˜ ì˜¤ë¥˜: {e}")
    
    def _save_highlight_image(self, face_data: Dict, confidence: float):
        """í‚¹ë°›ëŠ” ìˆœê°„ ì´ë¯¸ì§€ ì €ì¥"""
        try:
            timestamp_str = f"{int(face_data['timestamp']):05d}"
            similarity_str = f"_{face_data['similarity']:.3f}" if 'similarity' in face_data else ""
            filename = f"angry_{timestamp_str}_{confidence:.3f}{similarity_str}.jpg"
            
            save_path = os.path.join(
                self.video_output_dir,
                "highlights",
                filename
            )
            
            face_data['face_image'].save(save_path)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í•˜ì´ë¼ì´íŠ¸ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _performance_monitor(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
        interval = self.config['performance']['monitoring_interval']
        
        while not self.stop_flag:
            time.sleep(interval)
            
            if self.stats['processing_start_time'] is None:
                continue
            
            # í˜„ì¬ í†µê³„
            elapsed = time.time() - self.stats['processing_start_time']
            fps = self.stats['frames_processed'] / elapsed if elapsed > 0 else 0
            
            # í ìƒíƒœ
            frame_queue_size = self.frame_queue.qsize()
            face_queue_size = self.face_queue.qsize()
            
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            memory_info = self.classifier.get_memory_usage()
            
            if self.config['logging']['performance_tracking']:
                avg_batch_time = (self.stats['total_inference_time'] / self.stats['batch_count'] 
                                 if self.stats['batch_count'] > 0 else 0)
                
                # ì–¼êµ´ ì¸ì‹ í†µê³„ ì¶”ê°€ (ëª¨ë¸ì´ ë¡œë“œëœ ê²½ìš°ì—ë§Œ)
                recognition_info = ""
                if self.facenet_model is not None:
                    recognition_rate = (self.stats['faces_recognized'] / max(1, self.stats['faces_detected'])) * 100
                    avg_recognition_time = (self.stats['total_recognition_time'] / max(1, self.stats['batch_count']))
                    recognition_info = f", ì¸ì‹ë¥ : {recognition_rate:.1f}%, ì¸ì‹ì‹œê°„: {avg_recognition_time:.3f}ì´ˆ"
                
                self.logger.info(
                    f"ğŸ“Š [{elapsed:.1f}s] "
                    f"í”„ë ˆì„: {self.stats['frames_processed']} ({fps:.1f} FPS), "
                    f"ì–¼êµ´: {self.stats['faces_detected']}, "
                    f"í‚¹ë°›ìŒ: {self.stats['angry_moments']}, "
                    f"í: {frame_queue_size}/{face_queue_size}, "
                    f"GPU: {memory_info['allocated']:.1f}GB, "
                    f"ë°°ì¹˜ í‰ê· : {avg_batch_time:.3f}ì´ˆ{recognition_info}"
                )
        
        self.logger.info("ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ")
    
    def _save_results(self, video_path: str, video_info: Dict) -> Dict:
        """ê²°ê³¼ ì €ì¥"""
        results = {
            'video_path': video_path,
            'video_info': video_info,
            'processing_stats': self.stats.copy(),
            'angry_moments': self.angry_moments,
            'total_angry_moments': len(self.angry_moments),
            'config': self.config
        }
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ JSON ì €ì¥
        if self.config['output']['save_timestamps']:
            timestamp_file = os.path.join(
                self.video_output_dir,
                "timestamps",
                "angry_moments.json"
            )
            
            with open(timestamp_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {timestamp_file}")
        
        return results
    
    def _print_final_stats(self):
        """ìµœì¢… í†µê³„ ì¶œë ¥"""
        elapsed = time.time() - self.stats['processing_start_time']
        fps = self.stats['frames_processed'] / elapsed if elapsed > 0 else 0
        avg_batch_time = (self.stats['total_inference_time'] / self.stats['batch_count'] 
                         if self.stats['batch_count'] > 0 else 0)
        
        self.logger.info("ğŸ¯ ì²˜ë¦¬ ì™„ë£Œ!")
        self.logger.info(f"   ì´ ì²˜ë¦¬ ì‹œê°„: {elapsed:.1f}ì´ˆ")
        self.logger.info(f"   ì²˜ë¦¬ëœ í”„ë ˆì„: {self.stats['frames_processed']}ê°œ ({fps:.1f} FPS)")
        self.logger.info(f"   íƒì§€ëœ ì–¼êµ´: {self.stats['faces_detected']}ê°œ")
        
        # ì–¼êµ´ ì¸ì‹ í†µê³„ ì¶œë ¥ (ëª¨ë¸ì´ ë¡œë“œëœ ê²½ìš°ì—ë§Œ)
        if self.facenet_model is not None:
            recognition_rate = (self.stats['faces_recognized'] / max(1, self.stats['faces_detected'])) * 100
            avg_recognition_time = (self.stats['total_recognition_time'] / max(1, self.stats['batch_count']))
            self.logger.info(f"   ì¸ì‹ëœ ì–¼êµ´: {self.stats['faces_recognized']}ê°œ ({recognition_rate:.1f}%)")
            self.logger.info(f"   í•„í„°ë§ëœ ì–¼êµ´: {self.stats['faces_filtered']}ê°œ")
            self.logger.info(f"   í‰ê·  ì¸ì‹ ì‹œê°„: {avg_recognition_time:.3f}ì´ˆ/ë°°ì¹˜")
        
        self.logger.info(f"   í‚¹ë°›ëŠ” ìˆœê°„: {self.stats['angry_moments']}ê°œ")
        self.logger.info(f"   ë¶„ë¥˜ ë°°ì¹˜: {self.stats['batch_count']}íšŒ (í‰ê·  {avg_batch_time:.3f}ì´ˆ)")
        
        # GPU ë©”ëª¨ë¦¬ ìµœì¢… ì‚¬ìš©ëŸ‰
        memory_info = self.classifier.get_memory_usage()
        self.logger.info(f"   ìµœëŒ€ GPU ë©”ëª¨ë¦¬: {memory_info['max_allocated']:.1f}GB")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import sys
    import argparse
    
    # ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description='ì¹¨ì°©ë§¨ í‚¹ë°›ëŠ” ìˆœê°„ íƒì§€ (PyTorch + ì–¼êµ´ ì¸ì‹)')
    parser.add_argument('filename', nargs='?', help='ì²˜ë¦¬í•  ë¹„ë””ì˜¤ íŒŒì¼ëª… (í™•ì¥ì í¬í•¨)')
    parser.add_argument('--dir', '--directory', help='ë¹„ë””ì˜¤ íŒŒì¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    parser.add_argument('--config', default='config/config_torch.yaml', help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    try:
        # í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        processor = TorchVideoProcessor(args.config)
        
        # ë¹„ë””ì˜¤ ê²½ë¡œ ê²°ì •
        if args.filename:
            # ëª…ë ¹ì¤„ì—ì„œ íŒŒì¼ëª… ì œê³µ
            video_dir = args.dir if args.dir else processor.config['video']['default_directory']
            video_path = os.path.join(video_dir, args.filename)
        else:
            # configì—ì„œ ê¸°ë³¸ê°’ ì‚¬ìš©
            video_dir = processor.config['video']['default_directory']
            video_filename = processor.config['video']['default_filename']
            video_path = os.path.join(video_dir, video_filename)
        
        processor.logger.info(f"ğŸ¬ ì²˜ë¦¬í•  ì˜ìƒ: {video_path}")
        
        # ë¹„ë””ì˜¤ ì²˜ë¦¬
        results = processor.process_video(video_path)
        
        processor.logger.info("âœ… ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # ê°•ì œ ì¢…ë£Œ
        print("ğŸ”š í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        sys.exit(0)


if __name__ == "__main__":
    main()