#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import sys
import json
import yaml
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
from sklearn.cluster import DBSCAN
import logging

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
current_file = Path(__file__)
project_root = current_file.parent.parent.parent  # pipeline/modules/highlight_clusterer.py
sys.path.insert(0, str(project_root))

# íŒŒì´í”„ë¼ì¸ ìœ í‹¸ë¦¬í‹° import
sys.path.append('pipeline')
from utils.pipeline_utils import PipelineUtils


class HighlightClusterer:
    """
    í…ì…˜ í•˜ì´ë¼ì´íŠ¸ í´ëŸ¬ìŠ¤í„°ë§ ì‹œìŠ¤í…œ
    - DBSCANìœ¼ë¡œ ë°€ì§‘ êµ¬ê°„ íƒì§€
    - ë‹¨ì¼ í¬ì¸íŠ¸ í´ëŸ¬ìŠ¤í„° í™•ì¥ (Â±3ì´ˆ)
    - ìœˆë„ìš° ìƒì„±ì„ ìœ„í•œ í´ëŸ¬ìŠ¤í„° ë²”ìœ„ ê³„ì‚°
    """
    
    def __init__(self, config_path: str = None):
        """
        í•˜ì´ë¼ì´íŠ¸ í´ëŸ¬ìŠ¤í„°ë§ ì´ˆê¸°í™”
        
        Args:
            config_path (str): config íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: funclip_extraction_config.yaml)
        """
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì‘ì—… ë””ë ‰í† ë¦¬ ë³€ê²½
        os.chdir(project_root)
        
        self.logger = logging.getLogger(__name__)
        
        # Config ë¡œë“œ
        if config_path is None:
            config_path = "pipeline/configs/funclip_extraction_config.yaml"
        
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # í´ëŸ¬ìŠ¤í„°ë§ íŒŒë¼ë¯¸í„° ì„¤ì •
        clustering_config = config['clustering']
        self.eps = clustering_config['eps']  # 9ì´ˆ
        self.min_samples = clustering_config['min_samples']  # 1
        self.expansion_buffer = clustering_config['expansion_buffer']  # 3ì´ˆ
        
        self.logger.info(f"âœ… í•˜ì´ë¼ì´íŠ¸ í´ëŸ¬ìŠ¤í„°ë§ ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"   DBSCAN íŒŒë¼ë¯¸í„°: eps={self.eps}ì´ˆ, min_samples={self.min_samples}")
        self.logger.info(f"   ë‹¨ì¼ í´ëŸ¬ìŠ¤í„° í™•ì¥: Â±{self.expansion_buffer}ì´ˆ")
    
    def load_tension_highlights(self, tension_json_path: str) -> List[Dict]:
        """
        í…ì…˜ ë¶„ì„ JSONì—ì„œ í•˜ì´ë¼ì´íŠ¸ ë¡œë“œ
        
        Args:
            tension_json_path (str): í…ì…˜ JSON íŒŒì¼ ê²½ë¡œ
            
        Returns:
            List[Dict]: í•˜ì´ë¼ì´íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        try:
            with open(tension_json_path, 'r', encoding='utf-8') as f:
                tension_data = json.load(f)
            
            highlights = tension_data['edit_suggestions']['highlights']
            
            self.logger.info(f"ğŸ“Š í•˜ì´ë¼ì´íŠ¸ ë¡œë“œ ì™„ë£Œ: {len(highlights)}ê°œ")
            self.logger.info(f"   ì†ŒìŠ¤: {os.path.basename(tension_json_path)}")
            
            # ì‹œê°„ ë²”ìœ„ ì •ë³´
            if highlights:
                timestamps = [h['timestamp'] for h in highlights]
                self.logger.info(f"   ì‹œê°„ ë²”ìœ„: {min(timestamps):.1f}ì´ˆ ~ {max(timestamps):.1f}ì´ˆ")
            
            return highlights
            
        except Exception as e:
            self.logger.error(f"âŒ í•˜ì´ë¼ì´íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def cluster_with_dbscan(self, highlights: List[Dict]) -> List[Dict]:
        """
        DBSCANìœ¼ë¡œ í•˜ì´ë¼ì´íŠ¸ í´ëŸ¬ìŠ¤í„°ë§ (ì›ë³¸ í¬ì¸íŠ¸ë§Œ ì‚¬ìš©)
        
        Args:
            highlights (List[Dict]): í•˜ì´ë¼ì´íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[Dict]: í´ëŸ¬ìŠ¤í„° ë¦¬ìŠ¤íŠ¸
        """
        if not highlights:
            self.logger.warning("âš ï¸ í•˜ì´ë¼ì´íŠ¸ê°€ ì—†ì–´ ë¹ˆ í´ëŸ¬ìŠ¤í„° ë°˜í™˜")
            return []
        
        self.logger.info("ğŸ” DBSCAN í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘...")
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ (1ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜)
        timestamps = np.array([[h['timestamp']] for h in highlights])
        
        # DBSCAN í´ëŸ¬ìŠ¤í„°ë§
        dbscan = DBSCAN(eps=self.eps, min_samples=self.min_samples)
        cluster_labels = dbscan.fit_predict(timestamps)
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ê·¸ë£¹í™”
        clusters = {}
        for i, label in enumerate(cluster_labels):
            if label not in clusters:
                clusters[label] = []
            clusters[label].append({
                'index': i,
                'highlight': highlights[i]
            })
        
        # í´ëŸ¬ìŠ¤í„° ë¦¬ìŠ¤íŠ¸ ìƒì„±
        cluster_list = []
        cluster_id = 0
        
        for label, points in clusters.items():
            # ë…¸ì´ì¦ˆ í¬ì¸íŠ¸(-1)ë„ ê°œë³„ í´ëŸ¬ìŠ¤í„°ë¡œ ì²˜ë¦¬
            cluster_points = [p['highlight'] for p in points]
            
            cluster_list.append({
                'cluster_id': cluster_id,
                'original_label': int(label),
                'points': cluster_points,
                'is_expanded': False  # ì•„ì§ í™•ì¥ ì•ˆë¨
            })
            cluster_id += 1
        
        self.logger.info(f"âœ… DBSCAN ì™„ë£Œ: {len(highlights)}ê°œ â†’ {len(cluster_list)}ê°œ í´ëŸ¬ìŠ¤í„°")
        
        # í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¶œë ¥
        for cluster in cluster_list:
            point_count = len(cluster['points'])
            timestamps = [p['timestamp'] for p in cluster['points']]
            time_range = f"{min(timestamps):.1f}~{max(timestamps):.1f}ì´ˆ" if point_count > 1 else f"{timestamps[0]:.1f}ì´ˆ"
            self.logger.info(f"   í´ëŸ¬ìŠ¤í„° {cluster['cluster_id']}: {point_count}ê°œ í¬ì¸íŠ¸ ({time_range})")
        
        return cluster_list
    
    def expand_single_clusters(self, clusters: List[Dict]) -> List[Dict]:
        """
        ë‹¨ì¼ í¬ì¸íŠ¸ í´ëŸ¬ìŠ¤í„°ë¥¼ Â±bufferì´ˆë¡œ í™•ì¥
        
        Args:
            clusters (List[Dict]): ì›ë³¸ í´ëŸ¬ìŠ¤í„° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[Dict]: í™•ì¥ëœ í´ëŸ¬ìŠ¤í„° ë¦¬ìŠ¤íŠ¸
        """
        self.logger.info("ğŸ”§ ë‹¨ì¼ í¬ì¸íŠ¸ í´ëŸ¬ìŠ¤í„° í™•ì¥ ì¤‘...")
        
        expanded_clusters = []
        single_count = 0
        
        for cluster in clusters:
            if len(cluster['points']) == 1:
                # ë‹¨ì¼ í¬ì¸íŠ¸ í´ëŸ¬ìŠ¤í„° â†’ í™•ì¥
                expanded_cluster = self._expand_single_cluster(cluster)
                expanded_clusters.append(expanded_cluster)
                single_count += 1
                
                original_time = cluster['points'][0]['timestamp']
                self.logger.info(f"   í´ëŸ¬ìŠ¤í„° {cluster['cluster_id']}: {original_time:.1f}ì´ˆ â†’ "
                               f"{original_time - self.expansion_buffer:.1f}~{original_time + self.expansion_buffer:.1f}ì´ˆ")
            else:
                # ë©€í‹° í¬ì¸íŠ¸ í´ëŸ¬ìŠ¤í„° â†’ ê·¸ëŒ€ë¡œ ìœ ì§€
                expanded_clusters.append(cluster)
        
        self.logger.info(f"âœ… ë‹¨ì¼ í´ëŸ¬ìŠ¤í„° í™•ì¥ ì™„ë£Œ: {single_count}ê°œ í™•ì¥ë¨")
        return expanded_clusters
    
    def _expand_single_cluster(self, cluster: Dict) -> Dict:
        """
        ë‹¨ì¼ í¬ì¸íŠ¸ í´ëŸ¬ìŠ¤í„°ë¥¼ Â±bufferì´ˆë¡œ í™•ì¥
        
        Args:
            cluster (Dict): ë‹¨ì¼ í¬ì¸íŠ¸ í´ëŸ¬ìŠ¤í„°
            
        Returns:
            Dict: í™•ì¥ëœ í´ëŸ¬ìŠ¤í„°
        """
        original_point = cluster['points'][0]
        original_time = original_point['timestamp']
        
        # ê°€ìƒ í¬ì¸íŠ¸ ìƒì„±
        virtual_start = {
            'timestamp': original_time - self.expansion_buffer,
            'tension': 0.0,  # ê°€ìƒ í¬ì¸íŠ¸ëŠ” í…ì…˜ 0
            'type': 'virtual_start'
        }
        
        virtual_end = {
            'timestamp': original_time + self.expansion_buffer,
            'tension': 0.0,
            'type': 'virtual_end'
        }
        
        # ì›ë³¸ í¬ì¸íŠ¸ì— íƒ€ì… ì¶”ê°€
        enhanced_original = original_point.copy()
        enhanced_original['type'] = 'original'
        
        # í™•ì¥ëœ í´ëŸ¬ìŠ¤í„° ìƒì„±
        expanded_cluster = cluster.copy()
        expanded_cluster['points'] = [virtual_start, enhanced_original, virtual_end]
        expanded_cluster['is_expanded'] = True
        
        return expanded_cluster
    
    def get_cluster_spans(self, clusters: List[Dict]) -> List[Dict]:
        """
        ê° í´ëŸ¬ìŠ¤í„°ì˜ ì‹œê°„ ë²”ìœ„ ê³„ì‚°
        
        Args:
            clusters (List[Dict]): í´ëŸ¬ìŠ¤í„° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[Dict]: ì‹œê°„ ë²”ìœ„ê°€ ì¶”ê°€ëœ í´ëŸ¬ìŠ¤í„° ë¦¬ìŠ¤íŠ¸
        """
        self.logger.info("â±ï¸ í´ëŸ¬ìŠ¤í„° ì‹œê°„ ë²”ìœ„ ê³„ì‚° ì¤‘...")
        
        clusters_with_spans = []
        
        for cluster in clusters:
            timestamps = [p['timestamp'] for p in cluster['points']]
            
            span = {
                'start': min(timestamps),
                'end': max(timestamps),
                'duration': max(timestamps) - min(timestamps)
            }
            
            # í´ëŸ¬ìŠ¤í„°ì— span ì •ë³´ ì¶”ê°€
            cluster_with_span = cluster.copy()
            cluster_with_span['span'] = span
            
            clusters_with_spans.append(cluster_with_span)
            
            self.logger.info(f"   í´ëŸ¬ìŠ¤í„° {cluster['cluster_id']}: "
                           f"{span['start']:.1f}~{span['end']:.1f}ì´ˆ (ì§€ì†ì‹œê°„: {span['duration']:.1f}ì´ˆ)")
        
        self.logger.info("âœ… í´ëŸ¬ìŠ¤í„° ì‹œê°„ ë²”ìœ„ ê³„ì‚° ì™„ë£Œ")
        return clusters_with_spans
    
    def process_highlights(self, tension_json_path: str) -> Dict:
        """
        ì „ì²´ í•˜ì´ë¼ì´íŠ¸ í´ëŸ¬ìŠ¤í„°ë§ í”„ë¡œì„¸ìŠ¤
        
        Args:
            tension_json_path (str): í…ì…˜ JSON íŒŒì¼ ê²½ë¡œ
            
        Returns:
            Dict: í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ (ë©”íƒ€ë°ì´í„° í¬í•¨)
        """
        self.logger.info("ğŸª í•˜ì´ë¼ì´íŠ¸ í´ëŸ¬ìŠ¤í„°ë§ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
        
        # 1. í•˜ì´ë¼ì´íŠ¸ ë¡œë“œ
        highlights = self.load_tension_highlights(tension_json_path)
        
        # 2. DBSCAN í´ëŸ¬ìŠ¤í„°ë§
        clusters = self.cluster_with_dbscan(highlights)
        
        # 3. ë‹¨ì¼ í´ëŸ¬ìŠ¤í„° í™•ì¥
        expanded_clusters = self.expand_single_clusters(clusters)
        
        # 4. ì‹œê°„ ë²”ìœ„ ê³„ì‚°
        final_clusters = self.get_cluster_spans(expanded_clusters)
        
        # 5. ê²°ê³¼ êµ¬ì„±
        result = {
            'metadata': {
                'video_name': self._extract_video_name(tension_json_path),
                'source_file': os.path.basename(tension_json_path),
                'total_highlights': len(highlights),
                'total_clusters': len(final_clusters),
                'single_expanded_count': sum(1 for c in final_clusters if c.get('is_expanded', False)),
                'clustered_at': datetime.now().isoformat(),
                'config': {
                    'eps': self.eps,
                    'min_samples': self.min_samples,
                    'expansion_buffer': self.expansion_buffer
                }
            },
            'clusters': final_clusters
        }
        
        self.logger.info("ğŸª í•˜ì´ë¼ì´íŠ¸ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ!")
        self.logger.info(f"   ìµœì¢… ê²°ê³¼: {len(highlights)}ê°œ í•˜ì´ë¼ì´íŠ¸ â†’ {len(final_clusters)}ê°œ í´ëŸ¬ìŠ¤í„°")
        
        return result
    
    def _extract_video_name(self, tension_json_path: str) -> str:
        """í…ì…˜ JSON íŒŒì¼ëª…ì—ì„œ ë¹„ë””ì˜¤ ì´ë¦„ ì¶”ì¶œ"""
        filename = os.path.basename(tension_json_path)
        # tension_f_001_ë°•ì •ë¯¼_ìœ íŠœë¸Œ_ì‚´ë¦¬ê¸°_11.0_24.0_20250612_135829.json
        # â†’ f_001_ë°•ì •ë¯¼_ìœ íŠœë¸Œ_ì‚´ë¦¬ê¸°_11.0_24.0
        if filename.startswith('tension_'):
            name_part = filename[8:]  # 'tension_' ì œê±°
            # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤íƒ¬í”„ ë¶€ë¶„ ì œê±° (_20250612_135829.json)
            parts = name_part.split('_')
            if len(parts) >= 3:
                # ë§ˆì§€ë§‰ 2ê°œ ë¶€ë¶„ì´ ë‚ ì§œ+ì‹œê°„ í˜•ì‹ì´ë©´ ì œê±°
                if parts[-1].endswith('.json'):
                    parts[-1] = parts[-1].replace('.json', '')
                if len(parts) > 2 and len(parts[-1]) == 6 and parts[-1].isdigit():  # ì‹œê°„ ë¶€ë¶„
                    parts = parts[:-1]
                if len(parts) > 2 and len(parts[-1]) == 8 and parts[-1].isdigit():  # ë‚ ì§œ ë¶€ë¶„
                    parts = parts[:-1]
                return '_'.join(parts)
        
        # ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ í™•ì¥ìë§Œ ì œê±°
        return os.path.splitext(filename)[0]
    
    def save_clusters(self, clusters_result: Dict, output_path: str) -> None:
        """
        í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            clusters_result (Dict): í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼
            output_path (str): ì €ì¥í•  íŒŒì¼ ê²½ë¡œ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€)
        """
        try:
            # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
            if not os.path.isabs(output_path):
                output_path = os.path.join(project_root, output_path)
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # JSON ì €ì¥
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(clusters_result, f, indent=2, ensure_ascii=False)
            
            # ìƒëŒ€ ê²½ë¡œë¡œ ë¡œê·¸ ì¶œë ¥
            relative_path = os.path.relpath(output_path, project_root)
            self.logger.info(f"ğŸ’¾ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {relative_path}")
            
            # ìš”ì•½ ì •ë³´ ì¶œë ¥
            metadata = clusters_result['metadata']
            self.logger.info(f"   ì´ í´ëŸ¬ìŠ¤í„°: {metadata['total_clusters']}ê°œ")
            self.logger.info(f"   í™•ì¥ëœ í´ëŸ¬ìŠ¤í„°: {metadata['single_expanded_count']}ê°œ")
            
        except Exception as e:
            self.logger.error(f"âŒ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise


def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    import argparse
    
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì‘ì—… ë””ë ‰í† ë¦¬ ë³€ê²½
    os.chdir(project_root)
    
    parser = argparse.ArgumentParser(description='í•˜ì´ë¼ì´íŠ¸ í´ëŸ¬ìŠ¤í„°ë§')
    parser.add_argument('tension_json', help='í…ì…˜ JSON íŒŒì¼ ê²½ë¡œ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€)')
    parser.add_argument('--output', help='ì¶œë ¥ JSON ê²½ë¡œ (ê¸°ë³¸: outputs/clip_analysis/clusters.json)')
    parser.add_argument('--config', help='Config íŒŒì¼ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    try:
        # í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰
        clusterer = HighlightClusterer(config_path=args.config)
        result = clusterer.process_highlights(args.tension_json)
        
        # ê²°ê³¼ ì €ì¥ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ ê²½ë¡œ)
        if args.output:
            output_path = args.output
        else:
            # ê¸°ë³¸ ì¶œë ¥ ê²½ë¡œ ìƒì„±
            video_name = result['metadata']['video_name']
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_path = f"outputs/clip_analysis/{video_name}/clusters_{video_name}_{timestamp}.json"
        
        clusterer.save_clusters(result, output_path)
        
        print(f"\nâœ… í•˜ì´ë¼ì´íŠ¸ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ!")
        print(f"ğŸ“Š {result['metadata']['total_highlights']}ê°œ í•˜ì´ë¼ì´íŠ¸ â†’ {result['metadata']['total_clusters']}ê°œ í´ëŸ¬ìŠ¤í„°")
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {os.path.relpath(output_path if os.path.isabs(output_path) else os.path.join(project_root, output_path), project_root)}")
        
    except Exception as e:
        print(f"âŒ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()