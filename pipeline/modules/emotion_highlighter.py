#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import sys
import glob
import numpy as np
from PIL import Image
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import argparse

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì´ë™
current_dir = os.path.dirname(os.path.abspath(__file__))
pipeline_root = os.path.dirname(current_dir)  # pipeline/
project_root = os.path.dirname(pipeline_root)  # project_root/
os.chdir(project_root)

# íŒŒì´í”„ë¼ì¸ ìœ í‹¸ë¦¬í‹° import
sys.path.append('pipeline')
from utils.pipeline_utils import PipelineUtils


class EmotionHighlighter:
    """
    ê°ì • í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œê¸°
    HDF5 ë¹„ë””ì˜¤ ë°ì´í„°ì—ì„œ ê° ê°ì •ë³„ ìƒìœ„ Nê°œ í”„ë ˆì„ì˜ ì–¼êµ´ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œ
    """
    
    def __init__(self, config: Dict):
        """
        ê°ì • í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œê¸° ì´ˆê¸°í™”
        
        Args:
            config (Dict): ì„¤ì • ì •ë³´
        """
        self.config = config
        self.highlight_config = config['emotion_highlights']
        
        # ê°ì • ë ˆì´ë¸” (VA ëª¨ë¸ ìˆœì„œ)
        self.emotion_labels = [
            'Anger',       # 0
            'Contempt',    # 1
            'Disgust',     # 2
            'Fear',        # 3
            'Happiness',   # 4
            'Neutral',     # 5
            'Sadness',     # 6
            'Surprise'     # 7
        ]
        
        print(f"âœ… ê°ì • í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   ì¶”ì¶œí•  ê°ì •: {self.highlight_config['include_emotions']}")
        print(f"   ê° ê°ì •ë³„ ìƒìœ„: {self.highlight_config['top_n_per_emotion']}ê°œ")
    
    def extract_highlights(self, video_hdf5_path: str, output_dir: str) -> Dict:
        """
        ë¹„ë””ì˜¤ HDF5ì—ì„œ ê°ì • í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ
        
        Args:
            video_hdf5_path (str): ë¹„ë””ì˜¤ HDF5 íŒŒì¼ ê²½ë¡œ
            output_dir (str): í•˜ì´ë¼ì´íŠ¸ ì´ë¯¸ì§€ ì €ì¥ ë””ë ‰í† ë¦¬
            
        Returns:
            Dict: ì¶”ì¶œ ê²°ê³¼ ì •ë³´
        """
        print(f"\nğŸ­ ê°ì • í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ ì‹œì‘")
        print(f"   ì…ë ¥: {video_hdf5_path}")
        print(f"   ì¶œë ¥: {output_dir}")
        
        # ë¹„ë””ì˜¤ ë°ì´í„° ë¡œë“œ
        video_data = PipelineUtils.load_video_hdf5(video_hdf5_path)
        if video_data is None:
            raise ValueError(f"ë¹„ë””ì˜¤ HDF5 ë¡œë“œ ì‹¤íŒ¨: {video_hdf5_path}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        # ì–¼êµ´ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì°¾ê¸°
        chimchakman_faces_dir = PipelineUtils.get_chimchakman_faces_directory(video_data)
        if not chimchakman_faces_dir:
            raise ValueError("ì¹¨ì°©ë§¨ ì–¼êµ´ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        print(f"   ì–¼êµ´ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬: {chimchakman_faces_dir}")
        
        # ê°ì • ë°ì´í„° ì¶”ì¶œ
        emotions = video_data['sequences']['emotions']  # [N, 10]
        timestamps = video_data['sequences']['timestamps']
        face_detected = video_data['sequences']['face_detected']
        
        print(f"   ì´ í”„ë ˆì„: {len(emotions)}ê°œ")
        print(f"   ì–¼êµ´ íƒì§€ëœ í”„ë ˆì„: {np.sum(face_detected)}ê°œ")
        
        # ì–¼êµ´ì´ íƒì§€ëœ í”„ë ˆì„ë§Œ í•„í„°ë§
        valid_indices = np.where(face_detected)[0]
        if len(valid_indices) == 0:
            print("âš ï¸ ì–¼êµ´ì´ íƒì§€ëœ í”„ë ˆì„ì´ ì—†ìŠµë‹ˆë‹¤")
            return {'extracted_count': 0, 'highlights': {}}
        
        valid_emotions = emotions[valid_indices]
        valid_timestamps = timestamps[valid_indices]
        
        print(f"   ìœ íš¨í•œ í”„ë ˆì„: {len(valid_indices)}ê°œ")
        
        # ê° ê°ì •ë³„ í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ
        extraction_results = {}
        total_extracted = 0
        
        for emotion in self.highlight_config['include_emotions']:
            try:
                highlights = self._extract_emotion_highlights(
                    emotion, valid_emotions, valid_timestamps, valid_indices,
                    chimchakman_faces_dir, output_dir
                )
                extraction_results[emotion] = highlights
                total_extracted += len(highlights)
                
                print(f"   {emotion}: {len(highlights)}ê°œ ì¶”ì¶œ")
                
            except Exception as e:
                print(f"   âš ï¸ {emotion} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                extraction_results[emotion] = []
        
        print(f"\nâœ… ê°ì • í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ ì™„ë£Œ: ì´ {total_extracted}ê°œ")
        
        return {
            'extracted_count': total_extracted,
            'highlights': extraction_results,
            'video_name': video_data['metadata']['video_name'],
            'output_dir': output_dir
        }
    
    def _extract_emotion_highlights(self, emotion: str, emotions: np.ndarray, 
                                  timestamps: np.ndarray, frame_indices: np.ndarray,
                                  faces_dir: str, output_dir: str) -> List[Dict]:
        """
        íŠ¹ì • ê°ì •ì˜ í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ
        
        Args:
            emotion (str): ê°ì • ì´ë¦„
            emotions (np.ndarray): ê°ì • ë°ì´í„° [N, 10]
            timestamps (np.ndarray): íƒ€ì„ìŠ¤íƒ¬í”„ ë°°ì—´
            frame_indices (np.ndarray): í”„ë ˆì„ ì¸ë±ìŠ¤ ë°°ì—´
            faces_dir (str): ì–¼êµ´ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬
            output_dir (str): ì¶œë ¥ ë””ë ‰í† ë¦¬
            
        Returns:
            List[Dict]: ì¶”ì¶œëœ í•˜ì´ë¼ì´íŠ¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        top_n = self.highlight_config['top_n_per_emotion']
        min_threshold = self.highlight_config['min_emotion_threshold']
        
        # ê°ì • ê°’ ì¶”ì¶œ
        if emotion == 'Valence':
            # Valence: ì ˆëŒ“ê°’ì´ í° ìˆœì„œ (ê·¹ê°’)
            emotion_values = np.abs(emotions[:, 8])
        elif emotion == 'Arousal':
            # Arousal: ë†’ì€ ê°’ ìˆœì„œ
            emotion_values = emotions[:, 9]
        else:
            # 8ê°œ ê¸°ë³¸ ê°ì • ì¤‘ í•˜ë‚˜
            if emotion not in self.emotion_labels:
                raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ê°ì •: {emotion}")
            
            emotion_idx = self.emotion_labels.index(emotion)
            emotion_values = emotions[:, emotion_idx]
        
        # ì„ê³„ê°’ ì´ìƒì¸ ê°’ë“¤ë§Œ í•„í„°ë§
        valid_mask = emotion_values >= min_threshold
        if not np.any(valid_mask):
            return []
        
        valid_values = emotion_values[valid_mask]
        valid_timestamps = timestamps[valid_mask]
        valid_frame_indices = frame_indices[valid_mask]
        
        # ìƒìœ„ Nê°œ ì„ íƒ
        top_indices = np.argsort(valid_values)[-top_n:][::-1]  # ë‚´ë¦¼ì°¨ìˆœ
        
        highlights = []
        
        for rank, idx in enumerate(top_indices):
            timestamp = valid_timestamps[idx]
            frame_idx = valid_frame_indices[idx]
            value = valid_values[idx]
            
            # í•´ë‹¹ í”„ë ˆì„ì˜ ì–¼êµ´ ì´ë¯¸ì§€ ì°¾ê¸°
            face_image_path = self._find_face_image(faces_dir, timestamp)
            
            if face_image_path and os.path.exists(face_image_path):
                # í•˜ì´ë¼ì´íŠ¸ ì´ë¯¸ì§€ ì €ì¥
                highlight_info = self._save_highlight_image(
                    face_image_path, emotion, rank + 1, timestamp, value, output_dir
                )
                
                if highlight_info:
                    highlights.append(highlight_info)
        
        return highlights
    
    def _find_face_image(self, faces_dir: str, timestamp: float) -> Optional[str]:
        """
        íƒ€ì„ìŠ¤íƒ¬í”„ì— í•´ë‹¹í•˜ëŠ” ì–¼êµ´ ì´ë¯¸ì§€ ì°¾ê¸°
        
        Args:
            faces_dir (str): ì–¼êµ´ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬
            timestamp (float): íƒ€ì„ìŠ¤íƒ¬í”„ (ì´ˆ)
            
        Returns:
            str: ì°¾ì€ ì–¼êµ´ ì´ë¯¸ì§€ ê²½ë¡œ (ì—†ìœ¼ë©´ None)
        """
        # íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ë°€ë¦¬ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜
        timestamp_ms = int(timestamp * 1000)
        
        # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ í•´ë‹¹ íƒ€ì„ìŠ¤íƒ¬í”„ì˜ ì´ë¯¸ì§€ ì°¾ê¸°
        # íŒŒì¼ëª… í˜•ì‹: timestamp_{timestamp:06d}_face{face_idx}_{type}_sim{similarity:.3f}.jpg
        pattern = f"timestamp_{timestamp_ms:06d}_*.jpg"
        search_pattern = os.path.join(faces_dir, pattern)
        
        matching_files = glob.glob(search_pattern)
        
        if matching_files:
            # ì—¬ëŸ¬ ê°œ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ ì„ íƒ (ë³´í†µ face0)
            return matching_files[0]
        
        # íŒ¨í„´ì´ ì•ˆ ë§ìœ¼ë©´ ê°€ì¥ ê°€ê¹Œìš´ íƒ€ì„ìŠ¤íƒ¬í”„ ì°¾ê¸°
        all_images = glob.glob(os.path.join(faces_dir, "timestamp_*.jpg"))
        
        if not all_images:
            return None
        
        # íŒŒì¼ëª…ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œí•˜ì—¬ ê°€ì¥ ê°€ê¹Œìš´ ê²ƒ ì°¾ê¸°
        closest_file = None
        min_diff = float('inf')
        
        for image_path in all_images:
            filename = os.path.basename(image_path)
            try:
                # timestamp_015750_face0_chimchakman_sim0.856.jpg í˜•íƒœì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
                parts = filename.split('_')
                if len(parts) >= 2 and parts[0] == 'timestamp':
                    file_timestamp_ms = int(parts[1])
                    diff = abs(file_timestamp_ms - timestamp_ms)
                    
                    if diff < min_diff:
                        min_diff = diff
                        closest_file = image_path
                        
                        # 100ms ì´ë‚´ë©´ ì •í™•í•œ ë§¤ì¹˜ë¡œ ê°„ì£¼
                        if diff <= 100:
                            break
                            
            except (ValueError, IndexError):
                continue
        
        return closest_file
    
    def _save_highlight_image(self, source_path: str, emotion: str, rank: int, 
                            timestamp: float, value: float, output_dir: str) -> Optional[Dict]:
        """
        í•˜ì´ë¼ì´íŠ¸ ì´ë¯¸ì§€ ì €ì¥
        
        Args:
            source_path (str): ì›ë³¸ ì–¼êµ´ ì´ë¯¸ì§€ ê²½ë¡œ
            emotion (str): ê°ì • ì´ë¦„
            rank (int): ìˆœìœ„
            timestamp (float): íƒ€ì„ìŠ¤íƒ¬í”„
            value (float): ê°ì • ê°’
            output_dir (str): ì¶œë ¥ ë””ë ‰í† ë¦¬
            
        Returns:
            Dict: ì €ì¥ëœ í•˜ì´ë¼ì´íŠ¸ ì •ë³´
        """
        try:
            # íŒŒì¼ëª… ìƒì„±
            filename_format = self.highlight_config['filename_format']
            filename = filename_format.format(
                emotion=emotion,
                rank=rank,
                timestamp=timestamp,
                value=value
            )
            
            output_path = os.path.join(output_dir, filename)
            
            # ì´ë¯¸ì§€ ë³µì‚¬ ë° ì €ì¥
            with Image.open(source_path) as img:
                # RGB ë³€í™˜ (í•„ìš”í•œ ê²½ìš°)
                if img.mode != 'RGB':
                    img = img.convert('RGB')
                
                # ì €ì¥
                img.save(output_path, 'JPEG', quality=95)
            
            return {
                'emotion': emotion,
                'rank': rank,
                'timestamp': timestamp,
                'value': value,
                'filename': filename,
                'output_path': output_path,
                'source_path': source_path
            }
            
        except Exception as e:
            print(f"   âš ï¸ í•˜ì´ë¼ì´íŠ¸ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None
    
    def print_extraction_summary(self, results: Dict):
        """
        ì¶”ì¶œ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        
        Args:
            results (Dict): ì¶”ì¶œ ê²°ê³¼ ì •ë³´
        """
        print(f"\nğŸ“Š ê°ì • í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ ìš”ì•½")
        print(f"{'='*50}")
        print(f"ë¹„ë””ì˜¤: {results['video_name']}")
        print(f"ì´ ì¶”ì¶œ ê°œìˆ˜: {results['extracted_count']}ê°œ")
        print(f"ì €ì¥ ìœ„ì¹˜: {results['output_dir']}")
        print(f"{'='*50}")
        
        for emotion, highlights in results['highlights'].items():
            if highlights:
                print(f"{emotion:>10}: {len(highlights):2d}ê°œ")
                for highlight in highlights[:3]:  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                    print(f"          â””â”€ #{highlight['rank']} {highlight['timestamp']:.1f}s (ê°’: {highlight['value']:.3f})")
            else:
                print(f"{emotion:>10}:  0ê°œ")


def main():
    """ë…ë¦½ ì‹¤í–‰ìš© ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ê°ì • í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œê¸°')
    parser.add_argument('video_hdf5', help='ë¹„ë””ì˜¤ HDF5 íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--output', '-o', default='outputs/highlights', 
                       help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: outputs/highlights)')
    parser.add_argument('--config', '-c', default='pipeline/configs/integrated_config.yaml',
                       help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    try:
        # ì„¤ì • ë¡œë“œ
        print("ğŸ“‹ ì„¤ì • ë¡œë“œ ì¤‘...")
        config = PipelineUtils.load_config(args.config)
        
        # ì¶”ì¶œê¸° ì´ˆê¸°í™”
        highlighter = EmotionHighlighter(config)
        
        # í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ
        results = highlighter.extract_highlights(args.video_hdf5, args.output)
        
        # ê²°ê³¼ ì¶œë ¥
        highlighter.print_extraction_summary(results)
        
        print(f"\nâœ… ê°ì • í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()