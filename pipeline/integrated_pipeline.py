#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import sys
import glob
import time
import argparse
from pathlib import Path
from typing import Optional, Dict, List

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
current_file = Path(__file__)
project_root = current_file.parent.parent
sys.path.insert(0, str(project_root))

# íŒŒì´í”„ë¼ì¸ ëª¨ë“ˆë“¤ import
sys.path.append('pipeline')
from utils.pipeline_utils import PipelineUtils
from modules.emotion_highlighter import EmotionHighlighter
from modules.thumbnail_classifier import ThumbnailClassifier

# ê¸°ì¡´ ì „ì²˜ë¦¬ ëª¨ë“ˆë“¤ import
from video_analyzer.inference_prep.video_preprocessor import LongVideoProcessor
from video_analyzer.inference_prep.audio_preprocessor import LongVideoAudioPreprocessor
from tension_analyzer.tension_calculator import MultiEmotionTensionCalculator
from tension_analyzer.tension_visualizer import TensionVisualizer


class IntegratedPipeline:
    """
    ì¹¨ì°©ë§¨ AI ì¬ë¯¸ë„ ë¶„ì„ í†µí•© íŒŒì´í”„ë¼ì¸
    ë¹„ë””ì˜¤ ì „ì²˜ë¦¬ë¶€í„° ì‹œê°í™”ê¹Œì§€ ì „ì²´ ê³¼ì •ì„ ê´€ë¦¬
    """
    
    def __init__(self, config_path: str = "pipeline/configs/integrated_config.yaml"):
        """
        í†µí•© íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        
        Args:
            config_path (str): í†µí•© ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        print(f"\n{'='*60}")
        print(f"ğŸ¬ ì¹¨ì°©ë§¨ AI ì¬ë¯¸ë„ ë¶„ì„ ì‹œìŠ¤í…œ v2.0")
        print(f"{'='*60}")
        
        # ì„¤ì • ë¡œë“œ
        self.config = PipelineUtils.load_config(config_path)
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dirs = PipelineUtils.setup_output_directories(self.config)
        
        # ë¡œê¹… ì„¤ì •
        self.logger = PipelineUtils.setup_logging(self.config, self.output_dirs)
        
        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ëª¨ë“œ
        self.auto_mode = self.config['pipeline']['execution']['auto_mode'] == 0
        
        # ê²°ê³¼ ì¶”ì 
        self.results = {
            'video_path': None,
            'video_hdf5': None,
            'audio_hdf5': None,
            'tension_json': None,
            'highlights_dir': None,
            'classification_dir': None,
            'visualization_dir': None,
            'processing_times': {},
            'total_start_time': None
        }
        
        self.logger.info("âœ… í†µí•© íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def run_pipeline(self, video_path: str) -> Dict:
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            video_path (str): ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            Dict: ì „ì²´ ì‹¤í–‰ ê²°ê³¼
        """
        self.results['video_path'] = video_path
        self.results['total_start_time'] = time.time()
        
        try:
            # ì‹¤í–‰ ëª¨ë“œ í™•ì¸
            if self.auto_mode:
                print(f"\nğŸ”„ ìë™ ëª¨ë“œë¡œ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤...")
            else:
                print(f"\nğŸ® ë‹¨ê³„ë³„ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤. ê° ë‹¨ê³„ë§ˆë‹¤ Enterë¥¼ ëˆŒëŸ¬ ì§„í–‰í•˜ì„¸ìš”.")
            
            print(f"ğŸ“ ì²˜ë¦¬í•  ì˜ìƒ: {video_path}")
            
            # 1ë‹¨ê³„: ë¹„ë””ì˜¤ ì „ì²˜ë¦¬
            if not self._run_step_1_video_preprocessing():
                return self.results
            
            # 2ë‹¨ê³„: ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
            if not self._run_step_2_audio_preprocessing():
                return self.results
            
            # 3ë‹¨ê³„: í…ì…˜ ê³„ì‚°
            if not self._run_step_3_tension_calculation():
                return self.results
            
            # 4ë‹¨ê³„: ê°ì • í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ
            if not self._run_step_4_emotion_highlights():
                return self.results
            
            # 5ë‹¨ê³„: ì¸ë„¤ì¼ ë¶„ë¥˜
            if not self._run_step_5_thumbnail_classification():
                return self.results
            
            # ì‹œê°í™” ë‹¨ê³„
            if not self._run_step_viz_visualization():
                return self.results
            
            # ìµœì¢… ê²°ê³¼ ì¶œë ¥
            self._print_final_results()
            
            return self.results
            
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return self.results
    
    def _run_step_1_video_preprocessing(self) -> bool:
        """1ë‹¨ê³„: ë¹„ë””ì˜¤ ì „ì²˜ë¦¬"""
        PipelineUtils.print_step_banner(1, "ë¹„ë””ì˜¤ ì „ì²˜ë¦¬", "ì–¼êµ´ íƒì§€ + ê°ì • ë¶„ì„í•˜ì—¬ HDF5 ì €ì¥")
        
        try:
            step_start = time.time()
            
            # ë¹„ë””ì˜¤ ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™” (ê¸°ì¡´ ì„¤ì • í™œìš©)
            video_config_path = "pipeline/configs/integrated_config.yaml"
            processor = LongVideoProcessor(video_config_path)
            
            # ë¹„ë””ì˜¤ ì²˜ë¦¬
            result = processor.process_long_video(self.results['video_path'])
            
            if result and 'hdf5_path' in result:
                self.results['video_hdf5'] = result['hdf5_path']
                
                step_time = time.time() - step_start
                self.results['processing_times']['video_preprocessing'] = step_time
                
                # ê²°ê³¼ ì •ë³´ ì¶”ì¶œ
                stats = result.get('stats', {})
                total_faces = stats.get('chimchakman_faces', 0) + stats.get('other_faces_filtered', 0)
                
                PipelineUtils.print_completion_banner(
                    1, "ë¹„ë””ì˜¤ ì „ì²˜ë¦¬", 
                    f"í”„ë ˆì„: {stats.get('frames_processed', 0):,}ê°œ, "
                    f"ì¹¨ì°©ë§¨ ì–¼êµ´: {stats.get('chimchakman_faces', 0):,}ê°œ, "
                    f"ì†Œìš”ì‹œê°„: {step_time:.1f}ì´ˆ"
                )
                
                # ë‹¨ê³„ë³„ ëª¨ë“œì—ì„œ ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸°
                return PipelineUtils.wait_for_user_input(self.auto_mode, "ë¹„ë””ì˜¤ ì „ì²˜ë¦¬")
            else:
                self.logger.error("âŒ ë¹„ë””ì˜¤ ì „ì²˜ë¦¬ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ 1ë‹¨ê³„ ë¹„ë””ì˜¤ ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return False
    
    def _run_step_2_audio_preprocessing(self) -> bool:
        """2ë‹¨ê³„: ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬"""
        PipelineUtils.print_step_banner(2, "ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬", "VAD + RMS ë¶„ì„í•˜ì—¬ HDF5 ì €ì¥")
        
        try:
            step_start = time.time()
            
            # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™” (ê¸°ì¡´ ì„¤ì • í™œìš©)
            audio_config_path = "pipeline/configs/integrated_config.yaml"
            preprocessor = LongVideoAudioPreprocessor(audio_config_path)
            
            # ì˜¤ë””ì˜¤ ì²˜ë¦¬
            result = preprocessor.preprocess_long_video_audio(self.results['video_path'])
            
            if result and 'hdf5_path' in result:
                self.results['audio_hdf5'] = result['hdf5_path']
                
                step_time = time.time() - step_start
                self.results['processing_times']['audio_preprocessing'] = step_time
                
                # VAD í†µê³„ ì •ë³´
                vad_stats = result['metadata']['vad_statistics']
                
                PipelineUtils.print_completion_banner(
                    2, "ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬",
                    f"ë°œí™” ë¹„ìœ¨: {vad_stats['voice_activity_ratio']:.1%}, "
                    f"ì´ í”„ë ˆì„: {result['metadata']['total_frames']:,}ê°œ, "
                    f"ì†Œìš”ì‹œê°„: {step_time:.1f}ì´ˆ"
                )
                
                return PipelineUtils.wait_for_user_input(self.auto_mode, "ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬")
            else:
                self.logger.error("âŒ ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ 2ë‹¨ê³„ ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return False
    
    def _run_step_3_tension_calculation(self) -> bool:
        """3ë‹¨ê³„: í…ì…˜ ê³„ì‚°"""
        PipelineUtils.print_step_banner(3, "í…ì…˜ ê³„ì‚°", "ê°ì • + ì˜¤ë””ì˜¤ ë°ì´í„°ë¡œ í…ì…˜ íƒ€ì„ë¼ì¸ ìƒì„±")
        
        try:
            step_start = time.time()
            
            # í…ì…˜ ê³„ì‚°ê¸° ì´ˆê¸°í™”
            tension_config_path = "pipeline/configs/integrated_config.yaml"
            calculator = MultiEmotionTensionCalculator(tension_config_path)
            
            # íŒŒì¼ëª… íŒ¨í„´ ì¶”ì¶œ (ë¹„ë””ì˜¤ëª…ì—ì„œ)
            video_name = Path(self.results['video_path']).stem
            filename_pattern = video_name
            
            # í…ì…˜ ê³„ì‚°
            result = calculator.calculate_tension(filename_pattern)
            
            if result:
                # JSON íŒŒì¼ ê²½ë¡œ ì°¾ê¸° (ìƒì„±ëœ íŒŒì¼ì—ì„œ)
                tension_output_dir = calculator.tension_output_dir
                json_files = glob.glob(os.path.join(tension_output_dir, f"tension_{filename_pattern}*.json"))
                
                if json_files:
                    self.results['tension_json'] = json_files[-1]  # ìµœì‹  íŒŒì¼
                
                step_time = time.time() - step_start
                self.results['processing_times']['tension_calculation'] = step_time
                
                # í†µê³„ ì •ë³´
                stats = result['statistics']
                
                PipelineUtils.print_completion_banner(
                    3, "í…ì…˜ ê³„ì‚°",
                    f"í‰ê·  í…ì…˜: {stats['avg_tension']:.2f}, "
                    f"í•˜ì´ë¼ì´íŠ¸: {stats['highlight_count']}ê°œ, "
                    f"ì†Œìš”ì‹œê°„: {step_time:.1f}ì´ˆ"
                )
                
                return PipelineUtils.wait_for_user_input(self.auto_mode, "í…ì…˜ ê³„ì‚°")
            else:
                self.logger.error("âŒ í…ì…˜ ê³„ì‚° ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ 3ë‹¨ê³„ í…ì…˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return False
    
    def _run_step_4_emotion_highlights(self) -> bool:
        """4ë‹¨ê³„: ê°ì • í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ"""
        PipelineUtils.print_step_banner(4, "ê°ì • í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ", "ê° ê°ì •ë³„ ìƒìœ„ ìˆœê°„ë“¤ì˜ ì–¼êµ´ ì´ë¯¸ì§€ ì €ì¥")
        
        try:
            step_start = time.time()
            
            # ê°ì • í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œê¸° ì´ˆê¸°í™”
            highlighter = EmotionHighlighter(self.config)
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
            highlights_dir = os.path.join(self.output_dirs['highlights'])
            
            # í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ
            result = highlighter.extract_highlights(self.results['video_hdf5'], highlights_dir)
            
            if result['extracted_count'] > 0:
                self.results['highlights_dir'] = highlights_dir
                
                step_time = time.time() - step_start
                self.results['processing_times']['emotion_highlights'] = step_time
                
                PipelineUtils.print_completion_banner(
                    4, "ê°ì • í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ",
                    f"ì¶”ì¶œëœ í•˜ì´ë¼ì´íŠ¸: {result['extracted_count']}ê°œ, "
                    f"ì†Œìš”ì‹œê°„: {step_time:.1f}ì´ˆ"
                )
                
                return PipelineUtils.wait_for_user_input(self.auto_mode, "ê°ì • í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ")
            else:
                self.logger.warning("âš ï¸ ì¶”ì¶œëœ ê°ì • í•˜ì´ë¼ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
                return PipelineUtils.wait_for_user_input(self.auto_mode, "ê°ì • í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ")
                
        except Exception as e:
            self.logger.error(f"âŒ 4ë‹¨ê³„ ê°ì • í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return False
    
    def _run_step_5_thumbnail_classification(self) -> bool:
        """5ë‹¨ê³„: ì¸ë„¤ì¼ ë¶„ë¥˜"""
        PipelineUtils.print_step_banner(5, "ì¸ë„¤ì¼ ë¶„ë¥˜", "ê³¼ì¥ëœ í‘œì •ì˜ ì¸ë„¤ì¼ìš© ì–¼êµ´ ì´ë¯¸ì§€ ì„ ë³„")
        
        try:
            step_start = time.time()
            
            # ì¸ë„¤ì¼ ë¶„ë¥˜ê¸° ì´ˆê¸°í™”
            classifier = ThumbnailClassifier(self.config)
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
            classification_dir = os.path.join(self.output_dirs['classification'])
            
            # ì¸ë„¤ì¼ ë¶„ë¥˜ (HDF5ì—ì„œ ìë™ìœ¼ë¡œ ì–¼êµ´ í´ë” ì°¾ê¸°)
            result = classifier.classify_faces_from_hdf5(self.results['video_hdf5'], classification_dir)
            
            self.results['classification_dir'] = classification_dir
            
            step_time = time.time() - step_start
            self.results['processing_times']['thumbnail_classification'] = step_time
            
            PipelineUtils.print_completion_banner(
                5, "ì¸ë„¤ì¼ ë¶„ë¥˜",
                f"ì²˜ë¦¬ëœ ì–¼êµ´: {result['classified_count']}ê°œ, "
                f"ì¸ë„¤ì¼: {result['saved_count']}ê°œ, "
                f"ì†Œìš”ì‹œê°„: {step_time:.1f}ì´ˆ"
            )
            
            return PipelineUtils.wait_for_user_input(self.auto_mode, "ì¸ë„¤ì¼ ë¶„ë¥˜")
                
        except Exception as e:
            self.logger.error(f"âŒ 5ë‹¨ê³„ ì¸ë„¤ì¼ ë¶„ë¥˜ ì˜¤ë¥˜: {e}")
            return False
    
    def _run_step_viz_visualization(self) -> bool:
        """ì‹œê°í™” ë‹¨ê³„"""
        PipelineUtils.print_step_banner("viz", "ì‹œê°í™”", "í…ì…˜ ê³¡ì„  ë° ê°ì • ë¶„ì„ ê·¸ë˜í”„ ìƒì„±")
        
        try:
            step_start = time.time()
            
            if not self.results['tension_json']:
                self.logger.warning("âš ï¸ í…ì…˜ ë°ì´í„°ê°€ ì—†ì–´ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")
                return True
            
            # OpenMP ì¶©ëŒ ë°©ì§€
            import os
            os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
            
            # matplotlib ë°±ì—”ë“œ ì„¤ì •
            import matplotlib
            matplotlib.use('Agg')  # GUI ì—†ëŠ” ë°±ì—”ë“œ ì‚¬ìš©
            import matplotlib.pyplot as plt
            
            # ì‹œê°í™” ë„êµ¬ ì´ˆê¸°í™”
            visualizer = TensionVisualizer(config=self.config)
            
            # í…ì…˜ ë°ì´í„° ë¡œë“œ
            if visualizer.load_tension_data(self.results['tension_json']):
                # ë¹„ë””ì˜¤ëª… ì¶”ì¶œ ë° ì•ˆì „í•œ í´ë”ëª… ìƒì„±
                video_name = Path(self.results['video_path']).stem
                safe_video_name = PipelineUtils.safe_filename(video_name)
                
                # ë¹„ë””ì˜¤ë³„ ì‹œê°í™” ë””ë ‰í† ë¦¬ ìƒì„±
                viz_dir = os.path.join(self.output_dirs['visualization'], safe_video_name)
                os.makedirs(viz_dir, exist_ok=True)
                
                self.results['visualization_dir'] = viz_dir
                
                # ìë™ìœ¼ë¡œ ì–¼êµ´ í´ë” ê°ì§€
                visualizer.auto_find_faces_dir()
                
                # 1. í…ì…˜ ê³¡ì„  ì €ì¥
                visualizer.plot_tension_curves(figsize=(16, 10))
                tension_plot_path = os.path.join(viz_dir, "tension_curves.png")
                plt.savefig(tension_plot_path, dpi=150, bbox_inches='tight')
                plt.close()
                
                # 2. ê°ì • ë¶„í¬ ì €ì¥
                visualizer.plot_emotion_pie_chart(figsize=(12, 8))
                emotion_plot_path = os.path.join(viz_dir, "emotion_distribution.png")
                plt.savefig(emotion_plot_path, dpi=150, bbox_inches='tight')
                plt.close()
                
                # 3. ê°ì • í”¼í¬ ì–¼êµ´ ì €ì¥
                visualizer.show_emotion_peak_faces(figsize=(16, 10))
                faces_plot_path = os.path.join(viz_dir, "emotion_peak_faces.png")
                plt.savefig(faces_plot_path, dpi=150, bbox_inches='tight')
                plt.close()
                
                step_time = time.time() - step_start
                self.results['processing_times']['visualization'] = step_time
                
                PipelineUtils.print_completion_banner(
                    "viz", "ì‹œê°í™”",
                    f"ìƒì„±ëœ ê·¸ë˜í”„: 3ê°œ (í…ì…˜ê³¡ì„ , ê°ì •ë¶„í¬, í”¼í¬ì–¼êµ´), "
                    f"ì†Œìš”ì‹œê°„: {step_time:.1f}ì´ˆ"
                )
                
                return PipelineUtils.wait_for_user_input(self.auto_mode, "ì‹œê°í™”")
            else:
                self.logger.error("âŒ í…ì…˜ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ ì‹œê°í™” ë‹¨ê³„ ì˜¤ë¥˜: {e}")
            return False
    
    def _print_final_results(self):
        """ìµœì¢… ê²°ê³¼ ì¶œë ¥"""
        total_time = time.time() - self.results['total_start_time']
        
        print(f"\n{'='*60}")
        print(f"ğŸ¯ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print(f"{'='*60}")
        
        # ì²˜ë¦¬ëœ íŒŒì¼ ì •ë³´
        video_name = Path(self.results['video_path']).name
        print(f"ğŸ“ ì²˜ë¦¬ëœ ì˜ìƒ: {video_name}")
        print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time//60:.0f}ë¶„ {total_time%60:.0f}ì´ˆ")
        
        # ê° ë‹¨ê³„ë³„ ì†Œìš” ì‹œê°„
        print(f"\nğŸ“Š ë‹¨ê³„ë³„ ì†Œìš”ì‹œê°„:")
        for step, time_taken in self.results['processing_times'].items():
            step_name = {
                'video_preprocessing': 'ë¹„ë””ì˜¤ ì „ì²˜ë¦¬',
                'audio_preprocessing': 'ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬', 
                'tension_calculation': 'í…ì…˜ ê³„ì‚°',
                'emotion_highlights': 'ê°ì • í•˜ì´ë¼ì´íŠ¸',
                'thumbnail_classification': 'ì¸ë„¤ì¼ ë¶„ë¥˜',
                'visualization': 'ì‹œê°í™”'
            }.get(step, step)
            print(f"   {step_name}: {time_taken:.1f}ì´ˆ")
        
        # ê²°ê³¼ íŒŒì¼ ìœ„ì¹˜
        print(f"\nğŸ“‚ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {self.output_dirs['base']}/")
        
        if self.results['video_hdf5']:
            print(f"   ğŸ“Š ë¹„ë””ì˜¤ HDF5: {os.path.relpath(self.results['video_hdf5'])}")
        if self.results['audio_hdf5']:
            print(f"   ğŸµ ì˜¤ë””ì˜¤ HDF5: {os.path.relpath(self.results['audio_hdf5'])}")
        if self.results['tension_json']:
            print(f"   âš¡ í…ì…˜ ë¶„ì„: {os.path.relpath(self.results['tension_json'])}")
        if self.results['highlights_dir']:
            highlights_count = len(glob.glob(os.path.join(self.results['highlights_dir'], "*.jpg")))
            print(f"   ğŸ­ ê°ì • í•˜ì´ë¼ì´íŠ¸: {highlights_count}ê°œ ì´ë¯¸ì§€")
        if self.results['classification_dir']:
            thumbnails_count = len(glob.glob(os.path.join(self.results['classification_dir'], "*.jpg")))
            print(f"   ğŸ–¼ï¸ ì¸ë„¤ì¼: {thumbnails_count}ê°œ ì´ë¯¸ì§€")
        if self.results['visualization_dir']:
            viz_count = len(glob.glob(os.path.join(self.results['visualization_dir'], "*.png")))
            print(f"   ğŸ“ˆ ì‹œê°í™”: {viz_count}ê°œ ê·¸ë˜í”„")
        
        print(f"\nâœ¨ ë¶„ì„ ì™„ë£Œ: {video_name}")
        print(f"{'='*60}")


def find_video_file(input_path: str, base_dir: str = "data/videos") -> Optional[str]:
    """
    ë¹„ë””ì˜¤ íŒŒì¼ ì°¾ê¸° (íŒ¨í„´ ë§¤ì¹­ ì§€ì›)
    
    Args:
        input_path (str): ì…ë ¥ ê²½ë¡œ ë˜ëŠ” íŒŒì¼ëª…
        base_dir (str): ê¸°ë³¸ ê²€ìƒ‰ ë””ë ‰í† ë¦¬
        
    Returns:
        str: ì°¾ì€ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (ì—†ìœ¼ë©´ None)
    """
    # 1. ì ˆëŒ€ ê²½ë¡œì¸ì§€ í™•ì¸
    if os.path.isabs(input_path) and os.path.exists(input_path):
        return input_path
    
    # 2. ìƒëŒ€ ê²½ë¡œì¸ì§€ í™•ì¸
    if os.path.exists(input_path):
        return input_path
    
    # 3. í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ í™•ì¸
    if os.path.exists(os.path.basename(input_path)):
        return os.path.basename(input_path)
    
    # 4. ê¸°ë³¸ ë””ë ‰í† ë¦¬ì—ì„œ íŒ¨í„´ ë§¤ì¹­
    if os.path.exists(base_dir):
        # ì •í™•í•œ íŒŒì¼ëª… ë§¤ì¹­
        exact_path = os.path.join(base_dir, input_path)
        if os.path.exists(exact_path):
            return exact_path
        
        # íŒ¨í„´ ë§¤ì¹­ (í™•ì¥ì ì—†ì´ ì…ë ¥ëœ ê²½ìš°)
        name_without_ext = os.path.splitext(input_path)[0]
        patterns = [
            f"{input_path}",
            f"{name_without_ext}.*",
            f"*{input_path}*",
            f"*{name_without_ext}*"
        ]
        
        for pattern in patterns:
            search_pattern = os.path.join(base_dir, pattern)
            matches = glob.glob(search_pattern)
            
            # ë¹„ë””ì˜¤ íŒŒì¼ë§Œ í•„í„°ë§
            video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv']
            video_matches = [m for m in matches 
                           if any(m.lower().endswith(ext) for ext in video_extensions)]
            
            if video_matches:
                return video_matches[0]  # ì²« ë²ˆì§¸ ë§¤ì¹˜ ë°˜í™˜
    
    return None


def get_video_input() -> Optional[str]:
    """
    ëŒ€í™”í˜• ë¹„ë””ì˜¤ íŒŒì¼ ì…ë ¥
    
    Returns:
        str: ì„ íƒëœ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
    """
    print(f"\nğŸ“ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì§€ì •í•˜ì„¸ìš”:")
    print(f"   1. íŒŒì¼ëª…ë§Œ ì…ë ¥ (data/videos/ì—ì„œ ê²€ìƒ‰)")
    print(f"   2. ìƒëŒ€/ì ˆëŒ€ ê²½ë¡œ ì…ë ¥")
    print(f"   ì˜ˆì‹œ: clip.mp4, videos/clip.mp4, D:/videos/clip.mp4")
    
    while True:
        user_input = input("ì…ë ¥: ").strip()
        
        if not user_input:
            print("âŒ íŒŒì¼ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            continue
        
        # ë¹„ë””ì˜¤ íŒŒì¼ ì°¾ê¸°
        video_path = find_video_file(user_input)
        
        if video_path:
            # íŒŒì¼ ì •ë³´ í‘œì‹œ
            file_size = os.path.getsize(video_path) / (1024 * 1024)  # MB
            print(f"âœ… ë¹„ë””ì˜¤ íŒŒì¼ í™•ì¸: {video_path}")
            print(f"   í¬ê¸°: {file_size:.1f}MB")
            return video_path
        else:
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {user_input}")
            print(f"   data/videos/ ë””ë ‰í† ë¦¬ë¥¼ í™•ì¸í•˜ê±°ë‚˜ ì „ì²´ ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            
            retry = input("ë‹¤ì‹œ ì‹œë„í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
            if retry != 'y':
                return None


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ì¹¨ì°©ë§¨ AI ì¬ë¯¸ë„ ë¶„ì„ í†µí•© íŒŒì´í”„ë¼ì¸')
    parser.add_argument('video_input', nargs='?', help='ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” íŒŒì¼ëª…')
    parser.add_argument('--config', '-c', default='pipeline/configs/integrated_config.yaml',
                       help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--auto', '-a', action='store_true',
                       help='ìë™ ëª¨ë“œë¡œ ì‹¤í–‰ (ë‹¨ê³„ë³„ ëŒ€ê¸° ì—†ìŒ)')
    
    args = parser.parse_args()
    
    try:
        # 1. ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ê²°ì •
        if args.video_input:
            # ëª…ë ¹ì¤„ ì¸ìë¡œ ì œê³µë¨
            video_path = find_video_file(args.video_input)
            if not video_path:
                print(f"âŒ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.video_input}")
                return 1
        else:
            # ëŒ€í™”í˜• ì…ë ¥
            video_path = get_video_input()
            if not video_path:
                print("âŒ ë¹„ë””ì˜¤ íŒŒì¼ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return 1
        
        # 2. ì„¤ì • íŒŒì¼ì—ì„œ ì‹¤í–‰ ëª¨ë“œ ì¡°ì • (ëª…ë ¹ì¤„ ì˜µì…˜ ìš°ì„ )
        if args.auto:
            # ì„ì‹œë¡œ ì„¤ì • ìˆ˜ì •
            import yaml
            with open(args.config, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            config['pipeline']['execution']['auto_mode'] = 0
            
            # ì„ì‹œ ì„¤ì • íŒŒì¼ ìƒì„±
            temp_config = args.config.replace('.yaml', '_temp.yaml')
            with open(temp_config, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
            args.config = temp_config
        
        # 3. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        pipeline = IntegratedPipeline(args.config)
        results = pipeline.run_pipeline(video_path)
        
        # 4. ì„ì‹œ ì„¤ì • íŒŒì¼ ì •ë¦¬
        if args.auto and os.path.exists(args.config) and 'temp' in args.config:
            os.remove(args.config)
        
        return 0
        
    except KeyboardInterrupt:
        print(f"\nğŸ›‘ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return 1
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())