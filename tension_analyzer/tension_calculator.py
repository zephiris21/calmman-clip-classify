#!/usr/bin/env python
# -*- coding: utf-8 -*-

print("=== í…ì…˜ ë¶„ì„ê¸° ì‹œì‘ ===")

import os
import sys
print(f"âœ… Python ë²„ì „: {sys.version}")
print(f"âœ… í˜„ì¬ ë””ë ‰í† ë¦¬: {os.getcwd()}")

try:
    import yaml
    print("âœ… yaml ì„í¬íŠ¸ ì™„ë£Œ")
except Exception as e:
    print(f"âŒ yaml ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    sys.exit(1)

try:
    import h5py
    print("âœ… h5py ì„í¬íŠ¸ ì™„ë£Œ")
except Exception as e:
    print(f"âŒ h5py ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    sys.exit(1)

try:
    import numpy as np
    print("âœ… numpy ì„í¬íŠ¸ ì™„ë£Œ")
except Exception as e:
    print(f"âŒ numpy ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    sys.exit(1)

import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import logging
from datetime import datetime, timedelta

print("âœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ")

class MultiEmotionTensionCalculator:
    """
    ë©€í‹°ê°ì • ê¸°ë°˜ í…ì…˜ ê³„ì‚° ì‹œìŠ¤í…œ
    - ì¤‘ë¦½ ì œì™¸ 7ê°€ì§€ ê°ì • + Arousal*10 ì¡°í•©
    - VAD ê¸°ë°˜ Voice RMS ê³„ì‚°
    - ì–¼êµ´ ì—†ì„ ë•Œ ì´ì „ ê°’ ìœ ì§€ + Decay
    """
    
    def __init__(self, config_path: str = "tension_analyzer/configs/tension_config.yaml"):
        print(f"ğŸ”§ í…ì…˜ ê³„ì‚°ê¸° ì´ˆê¸°í™” ì‹œì‘")
        print(f"   ì„¤ì • íŒŒì¼ ê²½ë¡œ: {config_path}")
        
        self.config = self._load_config(config_path)
        print("âœ… ì„¤ì • ë¡œë“œ ì™„ë£Œ")
        
        self._setup_logging()
        print("âœ… ë¡œê¹… ì„¤ì • ì™„ë£Œ")
        
        self._create_output_dirs()
        print("âœ… ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± ì™„ë£Œ")
        
        # HDF5 íŒŒì¼ ê¸°ë³¸ ê²½ë¡œ
        self.audio_sequences_dir = "video_analyzer/preprocessed_data/audio_sequences"
        self.video_sequences_dir = "video_analyzer/preprocessed_data/video_sequences"
        print(f"   ì˜¤ë””ì˜¤ ë””ë ‰í† ë¦¬: {self.audio_sequences_dir}")
        print(f"   ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬: {self.video_sequences_dir}")
        
        # ê°ì • ë ˆì´ë¸” (MTCNN ìˆœì„œ)
        self.emotion_labels = [
            'Anger',       # 0
            'Contempt',    # 1 
            'Disgust',     # 2
            'Fear',        # 3
            'Happiness',   # 4
            'Neutral',     # 5 â† ì œì™¸í•  ê°ì •
            'Sadness',     # 6
            'Surprise'     # 7
        ]
        self.neutral_idx = 5  # Neutral ì¸ë±ìŠ¤
        print(f"   ì¤‘ë¦½ ê°ì • ì¸ë±ìŠ¤: {self.neutral_idx}")
        
        # í…ì…˜ ê³„ì‚° íŒŒë¼ë¯¸í„°
        self.window_duration = self.config['tension']['window_duration']  # 0.5ì´ˆ
        self.emotion_weight = self.config['tension']['emotion_weight']    # 0.7
        self.audio_weight = self.config['tension']['audio_weight']        # 0.3
        self.arousal_multiplier = self.config['tension']['arousal_multiplier']  # 10
        
        # Voice RMS ì •ê·œí™”
        self.voice_rms_max = self.config['audio']['voice_rms_max']  # 0.1
        
        # Decay íŒŒë¼ë¯¸í„°
        self.decay_rate = self.config['decay']['decay_rate']              # 0.95
        self.silence_3sec_decay = self.config['decay']['silence_3sec_decay']  # 0.85
        self.silence_threshold = self.config['decay']['silence_threshold_seconds']  # 1.0
        
        # í¸ì§‘ íƒì§€ íŒŒë¼ë¯¸í„°
        self.highlight_sensitivity = self.config['editing']['highlight_sensitivity']  # 2.0
        self.change_threshold = self.config['editing']['change_threshold']            # 0.2
        self.low_tension_threshold = self.config['editing']['low_tension_threshold']  # 3.0
        
        self.logger.info("âœ… ë©€í‹°ê°ì • í…ì…˜ ê³„ì‚°ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        self._print_config_summary()
        print("=== ì´ˆê¸°í™” ì™„ë£Œ ===")
    
    def _find_h5_files(self, filename_pattern: str) -> Tuple[str, str]:
        """HDF5 íŒŒì¼ ìŒ ì°¾ê¸° (íŒ¨í„´ í•˜ë‚˜ë¡œ ì˜¤ë””ì˜¤+ë¹„ë””ì˜¤ ëª¨ë‘ ì°¾ê¸°)"""
        print(f"ğŸ” íŒŒì¼ íŒ¨í„´ìœ¼ë¡œ ê²€ìƒ‰ ì¤‘: {filename_pattern}")
        
        audio_file = None
        video_file = None
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ ì°¾ê¸°
        if os.path.exists(self.audio_sequences_dir):
            for file in os.listdir(self.audio_sequences_dir):
                if filename_pattern in file and file.endswith('.h5'):
                    audio_file = os.path.join(self.audio_sequences_dir, file)
                    print(f"âœ… ì˜¤ë””ì˜¤ íŒŒì¼ ë°œê²¬: {file}")
                    break
        
        # ë¹„ë””ì˜¤ íŒŒì¼ ì°¾ê¸°
        if os.path.exists(self.video_sequences_dir):
            for file in os.listdir(self.video_sequences_dir):
                if filename_pattern in file and file.endswith('.h5'):
                    video_file = os.path.join(self.video_sequences_dir, file)
                    print(f"âœ… ë¹„ë””ì˜¤ íŒŒì¼ ë°œê²¬: {file}")
                    break
        
        if audio_file is None:
            print(f"âŒ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {filename_pattern}")
            if os.path.exists(self.audio_sequences_dir):
                files = os.listdir(self.audio_sequences_dir)
                print(f"   ì˜¤ë””ì˜¤ ë””ë ‰í† ë¦¬ íŒŒì¼ë“¤: {files[:3]}...")
            raise FileNotFoundError(f"ì˜¤ë””ì˜¤ HDF5 íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename_pattern}")
        
        if video_file is None:
            print(f"âŒ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {filename_pattern}")
            if os.path.exists(self.video_sequences_dir):
                files = os.listdir(self.video_sequences_dir)
                print(f"   ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬ íŒŒì¼ë“¤: {files[:3]}...")
            raise FileNotFoundError(f"ë¹„ë””ì˜¤ HDF5 íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename_pattern}")
        
        return audio_file, video_file
    
    def _load_config(self, config_path: str) -> Dict:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        print(f"ğŸ“‹ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹œì‘: {config_path}")
        
        default_config = {
            'tension': {
                'window_duration': 0.5,
                'emotion_weight': 0.7,
                'audio_weight': 0.3,
                'arousal_multiplier': 10
            },
            'audio': {
                'voice_rms_max': 0.1,
                'vad_activity_threshold': 0.2
            },
            'decay': {
                'decay_rate': 0.95,
                'silence_3sec_decay': 0.85,
                'silence_threshold_seconds': 1.0
            },
            'editing': {
                'highlight_sensitivity': 2.0,
                'change_threshold': 0.2,
                'low_tension_threshold': 3.0
            },
            'output': {
                'base_dir': 'tension_analyzer',
                'tension_analysis_dir': 'outputs/tension_data'
            },
            'logging': {
                'level': 'INFO',
                'save_detailed_log': True
            }
        }
        
        if os.path.exists(config_path):
            print(f"âœ… ì„¤ì • íŒŒì¼ ë°œê²¬, ë¡œë“œ ì¤‘...")
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            print(f"âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
            
            # ê¸°ë³¸ê°’ê³¼ ë³‘í•©
            def merge_dict(default, loaded):
                for key, value in default.items():
                    if key not in loaded:
                        loaded[key] = value
                        print(f"   ê¸°ë³¸ê°’ ì¶”ê°€: {key}")
                    elif isinstance(value, dict) and isinstance(loaded[key], dict):
                        merge_dict(value, loaded[key])
                return loaded
            config = merge_dict(default_config, config)
        else:
            print(f"âš ï¸ ì„¤ì • íŒŒì¼ ì—†ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©: {config_path}")
            config = default_config
            
            # ê¸°ë³¸ ì„¤ì • íŒŒì¼ ìƒì„±
            os.makedirs(os.path.dirname(config_path), exist_ok=True)
            with open(config_path, 'w', encoding='utf-8') as f:
                yaml.dump(default_config, f, default_flow_style=False, allow_unicode=True)
            print(f"âœ… ê¸°ë³¸ ì„¤ì • íŒŒì¼ ìƒì„±: {config_path}")
        
        return config
    
    def _setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        self.logger = logging.getLogger('MultiEmotionTensionCalculator')
        self.logger.setLevel(getattr(logging, self.config['logging']['level']))
        
        # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)
        
        # ì½˜ì†” í•¸ë“¤ëŸ¬
        console_handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        console_handler.setFormatter(formatter)
        self.logger.addHandler(console_handler)
    
    def _create_output_dirs(self):
        """ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
        base_dir = self.config['output']['base_dir']
        self.tension_output_dir = os.path.join(base_dir, self.config['output']['tension_analysis_dir'])
        os.makedirs(self.tension_output_dir, exist_ok=True)
        print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.tension_output_dir}")
    
    def _print_config_summary(self):
        """ì„¤ì • ìš”ì•½ ì¶œë ¥"""
        print("ğŸ“‹ ë©€í‹°ê°ì • í…ì…˜ ê³„ì‚° ì„¤ì •:")
        print(f"   ìœˆë„ìš°: {self.window_duration}ì´ˆ")
        print(f"   ê°€ì¤‘ì¹˜ - ê°ì •: {self.emotion_weight}, ì˜¤ë””ì˜¤: {self.audio_weight}")
        print(f"   Arousal ë°°ìˆ˜: {self.arousal_multiplier}")
        print(f"   Decay - ì¼ë°˜: {self.decay_rate}, 3ì´ˆì¹¨ë¬µ: {self.silence_3sec_decay}")
        print(f"   Voice RMS ìµœëŒ€: {self.voice_rms_max}")
    
    def calculate_tension(self, filename_pattern: str, youtube_url: str = None) -> Optional[Dict]:
        """
        ë©€í‹°ê°ì • ê¸°ë°˜ í…ì…˜ ê³„ì‚°
        
        Args:
            filename_pattern (str): íŒŒì¼ëª… íŒ¨í„´ (ì˜¤ë””ì˜¤+ë¹„ë””ì˜¤ ëª¨ë‘ ë§¤ì¹­)
            youtube_url (str): ìœ íŠœë¸Œ URL (ì˜µì…˜)
            
        Returns:
            Dict: í…ì…˜ ë¶„ì„ ê²°ê³¼ (JSON êµ¬ì¡°)
        """
        try:
            print(f"ğŸ¬ ë©€í‹°ê°ì • í…ì…˜ ë¶„ì„ ì‹œì‘")
            print(f"   íŒŒì¼ëª… íŒ¨í„´: {filename_pattern}")
            
            # HDF5 íŒŒì¼ ìŒ ì°¾ê¸°
            audio_h5_path, video_h5_path = self._find_h5_files(filename_pattern)
            print(f"âœ… íŒŒì¼ ìŒ í™•ì¸ ì™„ë£Œ")
            
            # 1. ë°ì´í„° ë¡œë“œ ë° ë™ê¸°í™”
            print("ğŸ“Š ë°ì´í„° ë¡œë“œ ë° ë™ê¸°í™” ì‹œì‘...")
            data = self._load_and_sync_data(audio_h5_path, video_h5_path)
            if data is None:
                print("âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
                return None
            print("âœ… ë°ì´í„° ë™ê¸°í™” ì™„ë£Œ")
            
            # 2. ìœˆë„ìš°ë³„ í…ì…˜ ê³„ì‚°
            print("âš¡ ìœˆë„ìš°ë³„ í…ì…˜ ê³„ì‚° ì‹œì‘...")
            tension_results = self._calculate_windowed_tension(data)
            print("âœ… í…ì…˜ ê³„ì‚° ì™„ë£Œ")
            
            # 3. í¸ì§‘ í¬ì¸íŠ¸ íƒì§€
            print("âœ‚ï¸ í¸ì§‘ í¬ì¸íŠ¸ íƒì§€ ì‹œì‘...")
            edit_suggestions = self._detect_edit_opportunities(tension_results, data)
            print("âœ… í¸ì§‘ í¬ì¸íŠ¸ íƒì§€ ì™„ë£Œ")
            
            # 4. JSON ê²°ê³¼ ìƒì„±
            print("ğŸ“ JSON ê²°ê³¼ ìƒì„± ì¤‘...")
            result = self._generate_json_result(
                data, tension_results, edit_suggestions, 
                audio_h5_path, video_h5_path, youtube_url
            )
            print("âœ… JSON ê²°ê³¼ ìƒì„± ì™„ë£Œ")
            
            # 5. ê²°ê³¼ ì €ì¥
            print("ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
            self._save_tension_analysis(result, filename_pattern)
            print("âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
            
            return result
            
        except Exception as e:
            print(f"âŒ í…ì…˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            import traceback
            print("ğŸ“ ìƒì„¸ ì˜¤ë¥˜:")
            traceback.print_exc()
            return None
    
    def _load_and_sync_data(self, audio_h5_path: str, video_h5_path: str) -> Optional[Dict]:
        """ë°ì´í„° ë¡œë“œ ë° ì‹œê°„ ë™ê¸°í™”"""
        try:
            print(f"ğŸ“‚ ì˜¤ë””ì˜¤ ë°ì´í„° ë¡œë“œ: {os.path.basename(audio_h5_path)}")
            # ì˜¤ë””ì˜¤ ë°ì´í„° ë¡œë“œ
            with h5py.File(audio_h5_path, 'r') as f:
                print(f"   ì˜¤ë””ì˜¤ HDF5 í‚¤ë“¤: {list(f.keys())}")
                audio_data = {
                    'rms_values': f['sequences/rms_values'][:],
                    'vad_labels': f['sequences/vad_labels'][:],
                    'audio_timestamps': f['sequences/timestamps'][:],
                    'audio_interval': f.attrs['analysis_interval']  # 0.05ì´ˆ
                }
                print(f"   RMS ë°ì´í„°: {len(audio_data['rms_values'])}ê°œ")
                print(f"   VAD ë°ì´í„°: {len(audio_data['vad_labels'])}ê°œ")
            
            print(f"ğŸ“‚ ë¹„ë””ì˜¤ ë°ì´í„° ë¡œë“œ: {os.path.basename(video_h5_path)}")
            # ë¹„ë””ì˜¤ ë°ì´í„° ë¡œë“œ
            with h5py.File(video_h5_path, 'r') as f:
                print(f"   ë¹„ë””ì˜¤ HDF5 í‚¤ë“¤: {list(f.keys())}")
                video_data = {
                    'emotions': f['sequences/emotions'][:],      # [N, 10] - 8ê°ì • + VA
                    'face_detected': f['sequences/face_detected'][:],
                    'video_timestamps': f['sequences/timestamps'][:],
                    'video_name': f.attrs['video_name']
                }
                print(f"   ê°ì • ë°ì´í„°: {video_data['emotions'].shape}")
                print(f"   ì–¼êµ´ íƒì§€: {len(video_data['face_detected'])}ê°œ")
            
            # ì‹œê°„ í•´ìƒë„ í™•ì¸
            print(f"â±ï¸ ì‹œê°„ í•´ìƒë„:")
            print(f"   ì˜¤ë””ì˜¤: {len(audio_data['rms_values'])}í”„ë ˆì„ ({audio_data['audio_interval']:.3f}ì´ˆ ê°„ê²©)")
            print(f"   ë¹„ë””ì˜¤: {len(video_data['emotions'])}í”„ë ˆì„")
            
            # ë°ì´í„° ë™ê¸°í™” (ì˜¤ë””ì˜¤ 0.05ì´ˆ ê¸°ì¤€)
            print("ğŸ”„ ë°ì´í„° ë™ê¸°í™” ì¤‘...")
            synced_data = self._synchronize_data(audio_data, video_data)
            print("âœ… ë°ì´í„° ë™ê¸°í™” ì™„ë£Œ")
            
            return synced_data
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _synchronize_data(self, audio_data: Dict, video_data: Dict) -> Dict:
        """ì˜¤ë””ì˜¤(0.05ì´ˆ)ì™€ ë¹„ë””ì˜¤(~0.25ì´ˆ) ë°ì´í„° ë™ê¸°í™”"""
        print("ğŸ”„ ì‹œê°„ ë™ê¸°í™” ì²˜ë¦¬ ì¤‘...")
        
        audio_timestamps = audio_data['audio_timestamps']
        video_timestamps = video_data['video_timestamps']
        
        print(f"   ì˜¤ë””ì˜¤ ì‹œê°„ ë²”ìœ„: {audio_timestamps[0]:.2f} ~ {audio_timestamps[-1]:.2f}ì´ˆ")
        print(f"   ë¹„ë””ì˜¤ ì‹œê°„ ë²”ìœ„: {video_timestamps[0]:.2f} ~ {video_timestamps[-1]:.2f}ì´ˆ")
        
        # ë¹„ë””ì˜¤ ë°ì´í„°ë¥¼ ì˜¤ë””ì˜¤ íƒ€ì„ìŠ¤íƒ¬í”„ì— ë§ì¶° ë³´ê°„
        synced_emotions = []
        synced_face_detected = []
        
        for i, audio_ts in enumerate(audio_timestamps):
            # ê°€ì¥ ê°€ê¹Œìš´ ë¹„ë””ì˜¤ í”„ë ˆì„ ì°¾ê¸°
            video_idx = np.argmin(np.abs(video_timestamps - audio_ts))
            
            # ê°ì • ë°ì´í„° (10ì°¨ì›: 8ê°ì • + Valence + Arousal)
            emotion_frame = video_data['emotions'][video_idx]
            if np.isnan(emotion_frame).any():  # NaN ì²˜ë¦¬
                emotion_frame = np.zeros(10)
            synced_emotions.append(emotion_frame)
            
            # ì–¼êµ´ íƒì§€ ë°ì´í„°
            synced_face_detected.append(video_data['face_detected'][video_idx])
            
            if i % 1000 == 0:  # ì§„í–‰ ìƒí™© ì¶œë ¥
                print(f"   ë™ê¸°í™” ì§„í–‰: {i}/{len(audio_timestamps)} ({i/len(audio_timestamps)*100:.1f}%)")
        
        result = {
            'timestamps': audio_timestamps,
            'rms_values': audio_data['rms_values'],
            'vad_labels': audio_data['vad_labels'],
            'emotions': np.array(synced_emotions),
            'face_detected': np.array(synced_face_detected),
            'interval': audio_data['audio_interval'],  # 0.05ì´ˆ
            'video_name': video_data['video_name']
        }
        
        print(f"âœ… ë™ê¸°í™” ì™„ë£Œ: {len(result['timestamps'])}ê°œ í”„ë ˆì„")
        return result
    
    def _calculate_windowed_tension(self, data: Dict) -> Dict:
        """ìœˆë„ìš°ë³„ í…ì…˜ ê³„ì‚° (ë©€í‹°ê°ì • ê¸°ë°˜)"""
        print("âš¡ ìœˆë„ìš°ë³„ í…ì…˜ ê³„ì‚° ì‹œì‘...")
        
        timestamps = data['timestamps']
        interval = data['interval']
        
        # ìœˆë„ìš° ì„¤ì •
        window_frames = int(self.window_duration / interval)  # 0.5ì´ˆ / 0.05ì´ˆ = 10í”„ë ˆì„
        step_frames = window_frames // 2  # 50% ê²¹ì¹¨ (0.25ì´ˆ ê°„ê²©)
        
        print(f"   ìœˆë„ìš° í¬ê¸°: {window_frames}í”„ë ˆì„ ({self.window_duration}ì´ˆ)")
        print(f"   ìŠ¤í… í¬ê¸°: {step_frames}í”„ë ˆì„ (50% ê²¹ì¹¨)")
        
        tension_timestamps = []
        emotion_tensions = []
        audio_tensions = []
        combined_tensions = []
        
        # ì´ì „ ê°’ ì¶”ì ìš©
        prev_emotion_tension = 0.0
        silence_count = 0
        
        total_windows = (len(timestamps) - window_frames) // step_frames + 1
        print(f"   ì´ ìœˆë„ìš° ìˆ˜: {total_windows}")
        
        # ìœˆë„ìš°ë³„ ì²˜ë¦¬
        for i in range(0, len(timestamps) - window_frames + 1, step_frames):
            window_end = min(i + window_frames, len(timestamps))
            
            # ìœˆë„ìš° ì¤‘ì•™ ì‹œê°„
            center_time = timestamps[i + window_frames // 2]
            tension_timestamps.append(center_time)
            
            # ìœˆë„ìš° ë°ì´í„° ì¶”ì¶œ
            window_rms = data['rms_values'][i:window_end]
            window_vad = data['vad_labels'][i:window_end]
            window_emotions = data['emotions'][i:window_end]
            window_face = data['face_detected'][i:window_end]
            
            # í…ì…˜ ê³„ì‚°
            emotion_tension, audio_tension, combined_tension, prev_emotion_tension, silence_count = \
                self._calculate_single_window_tension(
                    window_rms, window_vad, window_emotions, window_face,
                    prev_emotion_tension, silence_count
                )
            
            emotion_tensions.append(emotion_tension)
            audio_tensions.append(audio_tension)
            combined_tensions.append(combined_tension)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            current_window = len(tension_timestamps)
            if current_window % 100 == 0:
                print(f"   ìœˆë„ìš° ì²˜ë¦¬: {current_window}/{total_windows} ({current_window/total_windows*100:.1f}%)")
        
        result = {
            'timestamps': tension_timestamps,
            'emotion_tension': emotion_tensions,
            'audio_tension': audio_tensions,
            'combined_tension': combined_tensions
        }
        
        print(f"âœ… í…ì…˜ ê³„ì‚° ì™„ë£Œ: {len(tension_timestamps)}ê°œ ìœˆë„ìš°")
        print(f"   í‰ê·  ê²°í•© í…ì…˜: {np.mean(combined_tensions):.2f}")
        print(f"   ìµœëŒ€ ê²°í•© í…ì…˜: {np.max(combined_tensions):.2f}")
        
        return result
    
    def _calculate_single_window_tension(self, rms_values: np.ndarray, vad_labels: np.ndarray,
                                       emotions: np.ndarray, face_detected: np.ndarray,
                                       prev_emotion_tension: float, silence_count: int) -> Tuple:
        """ë‹¨ì¼ ìœˆë„ìš° í…ì…˜ ê³„ì‚° (ê°œì„ ëœ ë¡œì§)"""
        
        # 1. ì–¼êµ´ íƒì§€ ë¹„ìœ¨
        face_ratio = np.mean(face_detected)
        
        # 2. VAD ê¸°ë°˜ Voice RMS ê³„ì‚°
        voice_frames = rms_values[vad_labels == 1]
        voice_rms = np.mean(voice_frames) if len(voice_frames) > 0 else 0.0
        voice_rms_norm = min(voice_rms / self.voice_rms_max, 1.0) * 5  # 0~5 ìŠ¤ì¼€ì¼
        
        # 3. VAD í™œë™ ë¹„ìœ¨
        vad_activity = np.mean(vad_labels)
        
        # 4. ê°ì • í…ì…˜ ê³„ì‚° (ì–¼êµ´ ìˆì„ ë•Œë§Œ)
        if face_ratio >= 0.5:  # ì–¼êµ´ì´ ì¶©ë¶„íˆ ë³´ì´ëŠ” ê²½ìš°
            emotion_tension = self._calculate_multi_emotion_score(emotions)
            prev_emotion_tension = emotion_tension  # ì´ì „ ê°’ ì—…ë°ì´íŠ¸
            silence_count = 0  # ì¹¨ë¬µ ì¹´ìš´íŠ¸ ë¦¬ì…‹
        else:
            emotion_tension = 0.0  # ì–¼êµ´ ì—†ìœ¼ë©´ ê°ì • í…ì…˜ 0
        
        # 5. ì˜¤ë””ì˜¤ í…ì…˜
        audio_tension = voice_rms_norm
        
        # 6. ê²°í•© í…ì…˜ (ë¡œì§ ì²˜ë¦¬ í¬í•¨)
        if face_ratio >= 0.5:  # ì–¼êµ´ ë³´ì„
            combined_tension = (self.emotion_weight * emotion_tension + 
                              self.audio_weight * audio_tension)
        elif vad_activity > self.config['audio']['vad_activity_threshold']:  # ì–¼êµ´ ì—†ì–´ë„ ë°œí™” ìˆìŒ
            # ì´ì „ ê°ì •ê°’ í™œìš© + í˜„ì¬ ì˜¤ë””ì˜¤
            combined_tension = (self.emotion_weight * prev_emotion_tension + 
                              self.audio_weight * audio_tension)
            silence_count = 0  # ì¹¨ë¬µ ì¹´ìš´íŠ¸ ë¦¬ì…‹
        else:  # ë‘˜ ë‹¤ ì—†ìŒ - Decay ì ìš©
            silence_count += 1
            
            # Decay ì ìš©
            if silence_count >= (3.0 / self.window_duration):  # 3ì´ˆ ì´ìƒ ì¹¨ë¬µ
                decay_rate = self.silence_3sec_decay
            else:
                decay_rate = self.decay_rate
            
            # ì´ì „ ê²°í•© í…ì…˜ì— decay ì ìš©
            prev_combined = (self.emotion_weight * prev_emotion_tension + 
                           self.audio_weight * audio_tension)
            combined_tension = prev_combined * decay_rate
            prev_emotion_tension = prev_emotion_tension * decay_rate
        
        return emotion_tension, audio_tension, combined_tension, prev_emotion_tension, silence_count
    
    def _calculate_multi_emotion_score(self, emotions: np.ndarray) -> float:
        """ë©€í‹°ê°ì • ì ìˆ˜ ê³„ì‚° (ì¤‘ë¦½ ì œì™¸ 7ê°ì • + Arousal*10)"""
        if len(emotions) == 0 or np.isnan(emotions).all():
            return 0.0
        
        # ìœ íš¨í•œ ê°ì • í”„ë ˆì„ë§Œ ì‚¬ìš©
        valid_emotions = emotions[~np.isnan(emotions).any(axis=1)]
        if len(valid_emotions) == 0:
            return 0.0
        
        # í‰ê·  ê°ì • ë²¡í„°
        avg_emotions = np.mean(valid_emotions, axis=0)
        
        # 1. ì¤‘ë¦½ ì œì™¸ 7ê°€ì§€ ê°ì • (ì–‘ìˆ˜ë§Œ)
        emotion_sum = 0.0
        for i in range(8):  # 0~7 ê°ì • ì¸ë±ìŠ¤
            if i != self.neutral_idx:  # ì¤‘ë¦½(5) ì œì™¸
                emotion_val = max(avg_emotions[i], 0.0)  # ì–‘ìˆ˜ë§Œ
                emotion_sum += emotion_val
        
        # 2. Arousal (9ë²ˆì§¸ ì¸ë±ìŠ¤) - ì–‘ìˆ˜ë§Œ, 10ë°°
        arousal_val = max(avg_emotions[9], 0.0) * self.arousal_multiplier
        
        # 3. ì´ ë©€í‹°ê°ì • ì ìˆ˜
        multi_emotion_score = emotion_sum + arousal_val
        
        return multi_emotion_score
    
    def _detect_edit_opportunities(self, tension_results: Dict, data: Dict) -> Dict:
        """í¸ì§‘ í¬ì¸íŠ¸ íƒì§€"""
        print("âœ‚ï¸ í¸ì§‘ í¬ì¸íŠ¸ íƒì§€ ì¤‘...")
        
        combined_tension = np.array(tension_results['combined_tension'])
        timestamps = tension_results['timestamps']
        
        edit_suggestions = {
            'highlights': [],
            'cut_points': [],
            'low_energy_periods': []
        }
        
        # í†µê³„ ê³„ì‚°
        tension_mean = np.mean(combined_tension)
        tension_std = np.std(combined_tension)
        highlight_threshold = tension_mean + self.highlight_sensitivity * tension_std
        
        print(f"   í…ì…˜ í‰ê· : {tension_mean:.2f}")
        print(f"   í…ì…˜ í‘œì¤€í¸ì°¨: {tension_std:.2f}")
        print(f"   í•˜ì´ë¼ì´íŠ¸ ì„ê³„ê°’: {highlight_threshold:.2f}")
        
        # í¸ì§‘ í¬ì¸íŠ¸ íƒì§€
        for i in range(len(combined_tension)):
            current_tension = combined_tension[i]
            current_time = timestamps[i]
            
            # í•˜ì´ë¼ì´íŠ¸ (ë†’ì€ í…ì…˜)
            if current_tension > highlight_threshold:
                edit_suggestions['highlights'].append({
                    'timestamp': float(current_time),
                    'tension': float(current_tension),
                    'type': 'peak'
                })
            
            # ê¸‰ê²©í•œ ë³€í™” (Cut í¬ì¸íŠ¸)
            if i > 0:
                change_rate = abs(current_tension - combined_tension[i-1])
                if change_rate > self.change_threshold:
                    cut_type = 'cut_in' if current_tension > combined_tension[i-1] else 'cut_out'
                    edit_suggestions['cut_points'].append({
                        'timestamp': float(current_time),
                        'change_rate': float(change_rate),
                        'type': cut_type
                    })
            
            # ë‚®ì€ ì—ë„ˆì§€ êµ¬ê°„
            if current_tension < self.low_tension_threshold:
                edit_suggestions['low_energy_periods'].append({
                    'timestamp': float(current_time),
                    'tension': float(current_tension)
                })
        
        print(f"âœ… í¸ì§‘ í¬ì¸íŠ¸ íƒì§€ ì™„ë£Œ:")
        print(f"   í•˜ì´ë¼ì´íŠ¸: {len(edit_suggestions['highlights'])}ê°œ")
        print(f"   ì»· í¬ì¸íŠ¸: {len(edit_suggestions['cut_points'])}ê°œ")
        print(f"   ì €ì—ë„ˆì§€ êµ¬ê°„: {len(edit_suggestions['low_energy_periods'])}ê°œ")
        
        return edit_suggestions
    
    def _generate_json_result(self, data: Dict, tension_results: Dict, 
                            edit_suggestions: Dict, audio_h5_path: str, 
                            video_h5_path: str, youtube_url: str = None) -> Dict:
        """JSON ê²°ê³¼ ìƒì„±"""
        print("ğŸ“ JSON ê²°ê³¼ ìƒì„± ì¤‘...")
        
        # ê¸°ë³¸ í†µê³„
        combined_tension = np.array(tension_results['combined_tension'])
        
        result = {
            'metadata': {
                'video_name': data['video_name'],
                'duration': float(data['timestamps'][-1]),
                'youtube_url': youtube_url,
                'processed_at': datetime.now().isoformat(),
                'audio_source': os.path.basename(audio_h5_path),
                'video_source': os.path.basename(video_h5_path)
            },
            'tension_timeline': {
                'timestamps': [float(t) for t in tension_results['timestamps']],
                'emotion_tension': [float(t) for t in tension_results['emotion_tension']],
                'audio_tension': [float(t) for t in tension_results['audio_tension']],
                'combined_tension': [float(t) for t in tension_results['combined_tension']]
            },
            'edit_suggestions': edit_suggestions,
            'statistics': {
                'avg_tension': float(np.mean(combined_tension)),
                'max_tension': float(np.max(combined_tension)),
                'min_tension': float(np.min(combined_tension)),
                'std_tension': float(np.std(combined_tension)),
                'highlight_count': len(edit_suggestions['highlights']),
                'cut_point_count': len(edit_suggestions['cut_points']),
                'low_energy_count': len(edit_suggestions['low_energy_periods']),
                'voice_activity_ratio': float(np.mean(data['vad_labels']))
            },
            'config_used': {
                'emotion_weight': self.emotion_weight,
                'audio_weight': self.audio_weight,
                'arousal_multiplier': self.arousal_multiplier,
                'window_duration': self.window_duration
            }
        }
        
        print(f"âœ… JSON ê²°ê³¼ ìƒì„± ì™„ë£Œ")
        print(f"   ë©”íƒ€ë°ì´í„°: {len(result['metadata'])}ê°œ í•­ëª©")
        print(f"   íƒ€ì„ë¼ì¸: {len(result['tension_timeline']['timestamps'])}ê°œ í¬ì¸íŠ¸")
        
        return result
    
    def _save_tension_analysis(self, result: Dict, filename_pattern: str):
        """í…ì…˜ ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        try:
            print("ğŸ’¾ ê²°ê³¼ ì €ì¥ ì‹œì‘...")
            
            # íŒŒì¼ëª… ìƒì„±
            safe_name = filename_pattern.replace('/', '_').replace('\\', '_')
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            json_filename = f"tension_{safe_name}_{timestamp}.json"
            json_path = os.path.join(self.tension_output_dir, json_filename)
            
            print(f"   ì €ì¥ ê²½ë¡œ: {json_path}")
            
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… í…ì…˜ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {json_filename}")
            
            # ìš”ì•½ ì •ë³´ ì¶œë ¥
            stats = result['statistics']
            print(f"ğŸ“Š ìµœì¢… í†µê³„:")
            print(f"   í‰ê·  í…ì…˜: {stats['avg_tension']:.2f}")
            print(f"   ìµœëŒ€ í…ì…˜: {stats['max_tension']:.2f}")
            print(f"   í•˜ì´ë¼ì´íŠ¸: {stats['highlight_count']}ê°œ")
            print(f"   ì»· í¬ì¸íŠ¸: {stats['cut_point_count']}ê°œ")
            print(f"   ìŒì„± í™œë™ ë¹„ìœ¨: {stats['voice_activity_ratio']:.1%}")
            
        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "="*50)
    print("ë©€í‹°ê°ì • í…ì…˜ ë¶„ì„ê¸° ì‹¤í–‰")
    print("="*50)
    
    import argparse
    
    # ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description='ë©€í‹°ê°ì • ê¸°ë°˜ í…ì…˜ ê³„ì‚°')
    parser.add_argument('filename_pattern', help='íŒŒì¼ëª… íŒ¨í„´ (ì˜¤ë””ì˜¤+ë¹„ë””ì˜¤ ìë™ ë§¤ì¹­)')
    parser.add_argument('--youtube_url', help='ìœ íŠœë¸Œ URL (ì˜µì…˜)')
    parser.add_argument('--config', default='tension_analyzer/configs/tension_config.yaml', 
                       help='í…ì…˜ ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    
    try:
        args = parser.parse_args()
        print(f"ğŸ“‹ ì¸ì íŒŒì‹± ì™„ë£Œ:")
        print(f"   íŒŒì¼ëª… íŒ¨í„´: {args.filename_pattern}")
        print(f"   ì„¤ì • íŒŒì¼: {args.config}")
        if args.youtube_url:
            print(f"   ìœ íŠœë¸Œ URL: {args.youtube_url}")
        
    except SystemExit:
        print("âŒ ì¸ì íŒŒì‹± ì‹¤íŒ¨ ë˜ëŠ” --help ìš”ì²­")
        return
    
    try:
        # í…ì…˜ ê³„ì‚°ê¸° ì‹¤í–‰
        print("\nğŸš€ í…ì…˜ ê³„ì‚°ê¸° ì´ˆê¸°í™”...")
        calculator = MultiEmotionTensionCalculator(args.config)
        
        # í…ì…˜ ê³„ì‚°
        print("\nâš¡ í…ì…˜ ê³„ì‚° ì‹œì‘...")
        result = calculator.calculate_tension(args.filename_pattern, args.youtube_url)
        
        if result:
            print("\n" + "="*50)
            print("âœ… ë©€í‹°ê°ì • í…ì…˜ ë¶„ì„ ì™„ë£Œ!")
            print("="*50)
            
            stats = result['statistics']
            print(f"ğŸ“Š ìµœì¢… ê²°ê³¼:")
            print(f"   í‰ê·  í…ì…˜: {stats['avg_tension']:.2f}")
            print(f"   ìµœëŒ€ í…ì…˜: {stats['max_tension']:.2f}")
            print(f"   ìµœì†Œ í…ì…˜: {stats['min_tension']:.2f}")
            print(f"   í•˜ì´ë¼ì´íŠ¸: {stats['highlight_count']}ê°œ")
            print(f"   ì»· í¬ì¸íŠ¸: {stats['cut_point_count']}ê°œ")
            
            # ìƒìœ„ í•˜ì´ë¼ì´íŠ¸ ì¶œë ¥
            highlights = sorted(result['edit_suggestions']['highlights'], 
                              key=lambda x: x['tension'], reverse=True)[:3]
            if highlights:
                print(f"\nğŸ¯ ì£¼ìš” í•˜ì´ë¼ì´íŠ¸:")
                for i, hl in enumerate(highlights, 1):
                    timestamp_str = str(timedelta(seconds=int(hl['timestamp'])))
                    print(f"   {i}. {timestamp_str} (í…ì…˜: {hl['tension']:.2f})")
            
            print(f"\nğŸ’¾ ê²°ê³¼ íŒŒì¼: tension_analyzer/outputs/tension_data/")
            
        else:
            print("âŒ í…ì…˜ ë¶„ì„ ì‹¤íŒ¨")
            
    except Exception as e:
        print(f"âŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        print("ğŸ“ ìƒì„¸ ì˜¤ë¥˜:")
        traceback.print_exc()
    
    print("\n" + "="*50)
    print("í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
    print("="*50)


if __name__ == "__main__":
    main()