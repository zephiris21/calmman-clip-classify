#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import cv2
import yaml
import time
import logging
import threading
import queue
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional

import numpy as np
import torch
import torch.nn.functional as F
import h5py
from PIL import Image

from mtcnn_wrapper import FaceDetector
from va_emotion_core import VAEmotionCore


class LongVideoProcessor:
    """
    ê¸´ ì˜ìƒ ë¹„ë””ì˜¤ ì „ì²˜ë¦¬ê¸°
    ì–¼êµ´ íƒì§€ + VA ê°ì • íŠ¹ì§• ì¶”ì¶œí•˜ì—¬ ì›ì‹œ ì‹œí€€ìŠ¤ ì €ì¥
    """
    
    def __init__(self, config_path: str = "video_analyzer/configs/inference_config.yaml"):
        """
        ê¸´ ì˜ìƒ ë¹„ë””ì˜¤ ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        
        Args:
            config_path (str): ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        # PyTorch ì„¤ì • ìµœì í™”
        if torch.cuda.is_available():
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
            try:
                if hasattr(torch.cuda, 'set_per_process_memory_fraction'):
                    torch.cuda.set_per_process_memory_fraction(0.7)
            except Exception as e:
                print(f"CUDA ìµœì í™” ì„¤ì • ì¤‘ ê²½ê³  (ë¬´ì‹œë¨): {e}")
        
        # ì„¤ì • ë¡œë“œ
        self.config = self._load_config(config_path)
        
        # ë¡œê¹… ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self._setup_logging()
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë³€ìˆ˜
        self.stats = {
            'frames_processed': 0,
            'faces_detected': 0,
            'faces_recognized': 0,
            'faces_filtered': 0,
            'emotions_extracted': 0,
            'processing_start_time': None,
            'last_stats_time': time.time(),
            'batch_count': 0,
            'total_face_detection_time': 0,
            'total_emotion_time': 0,
            'total_recognition_time': 0
        }
        
        # ì¢…ë£Œ í”Œë˜ê·¸
        self.stop_flag = False
        self.face_detection_done = False
        
        # í ìƒì„±
        self.frame_queue = queue.Queue(maxsize=self.config['performance']['max_queue_size'])
        self.result_queue = queue.Queue()
        
        # ëª¨ë¸ë“¤ ì´ˆê¸°í™”
        self._init_face_detector()
        self._init_emotion_model()
        self._init_face_recognition()
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self._create_output_dirs()
        
        self.logger.info("âœ… ê¸´ ì˜ìƒ ë¹„ë””ì˜¤ ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        self._print_config_summary()
    
    def _load_config(self, config_path: str) -> Dict:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        default_config = {
            'video': {
                'frame_skip': 15,
                'extract_emotions': True,
                'save_face_images': True,
                'face_images_dir': 'debug_faces'
            },
            'mtcnn': {
                'batch_size': 32,
                'image_size': 224,
                'margin': 20,
                'prob_threshold': 0.9,
                'align_faces': False
            },
            'emotion': {
                'model_path': 'models/affectnet_emotions/enet_b0_8_va_mtl.pt',
                'device': 'cuda' if torch.cuda.is_available() else 'cpu',
                'batch_size': 16
            },
            'face_recognition': {
                'enabled': True,
                'test_mode': False,
                'embedding_path': 'face_recognition/target_embeddings/chimchakman.npy',
                'similarity_threshold': 0.7,
                'batch_size': 32,
                'logging': {
                    'save_filtered_faces': False,
                    'log_filtered_count': True
                }
            },
            'output': {
                'base_dir': 'video_analyzer',
                'preprocessed_dir': 'preprocessed_data',
                'video_sequence_dir': 'video_sequences'
            },
            'performance': {
                'max_queue_size': 200,
                'monitoring_interval': 10.0
            },
            'logging': {
                'level': 'INFO',
                'batch_summary': True
            }
        }
        
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            # ê¸°ë³¸ê°’ê³¼ ë³‘í•©
            def merge_dict(default, loaded):
                for key, value in default.items():
                    if key not in loaded:
                        loaded[key] = value
                    elif isinstance(value, dict) and isinstance(loaded[key], dict):
                        merge_dict(value, loaded[key])
                return loaded
            config = merge_dict(default_config, config)
        else:
            config = default_config
            print(f"âš ï¸ ì„¤ì • íŒŒì¼ ì—†ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©: {config_path}")
        
        return config
    
    def _setup_logging(self):
        """ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •"""
        self.logger = logging.getLogger('LongVideoProcessor')
        self.logger.setLevel(getattr(logging, self.config['logging']['level']))
        
        # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)
        
        # ì½˜ì†” í•¸ë“¤ëŸ¬
        console_handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        console_handler.setFormatter(formatter)
        self.logger.addHandler(console_handler)
    
    def _init_face_detector(self):
        """MTCNN ì–¼êµ´ íƒì§€ê¸° ì´ˆê¸°í™”"""
        mtcnn_config = self.config['mtcnn']
        
        self.face_detector = FaceDetector(
            image_size=mtcnn_config['image_size'],
            margin=mtcnn_config['margin'],
            prob_threshold=mtcnn_config['prob_threshold'],
            align_faces=mtcnn_config['align_faces']
        )
        
        self.logger.info(f"âœ… MTCNN ì´ˆê¸°í™” ì™„ë£Œ (ë°°ì¹˜ í¬ê¸°: {mtcnn_config['batch_size']})")
    
    def _init_emotion_model(self):
        """VA ê°ì • ëª¨ë¸ ì´ˆê¸°í™”"""
        emotion_config = self.config['emotion']
        
        try:
            self.emotion_model = VAEmotionCore(
                model_path=emotion_config['model_path'],
                device=emotion_config['device']
            )
            self.logger.info("âœ… VA ê°ì • ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"âŒ ê°ì • ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def _init_face_recognition(self):
        """FaceNet ì–¼êµ´ ì¸ì‹ ëª¨ë¸ ì´ˆê¸°í™”"""
        face_recog_config = self.config['face_recognition']
        
        # ì–¼êµ´ ì¸ì‹ì´ ë¹„í™œì„±í™”ë˜ì—ˆê±°ë‚˜ í…ŒìŠ¤íŠ¸ ëª¨ë“œì¸ ê²½ìš°
        if not face_recog_config.get('enabled', False) or face_recog_config.get('test_mode', False):
            mode_str = "ë¹„í™œì„±í™”ë¨" if not face_recog_config.get('enabled', False) else "í…ŒìŠ¤íŠ¸ ëª¨ë“œ"
            self.logger.info(f"âš ï¸ ì–¼êµ´ ì¸ì‹ {mode_str}")
            self.facenet_model = None
            self.target_embedding = None
            return
        
        try:
            from facenet_pytorch import InceptionResnetV1
            
            # FaceNetì„ CPUì— ë¡œë“œ (ì•ˆì •ì„±)
            cpu_device = torch.device('cpu')
            
            with torch.no_grad():
                self.facenet_model = InceptionResnetV1(pretrained='vggface2')
                self.facenet_model = self.facenet_model.to(cpu_device).eval()
            
            # íƒ€ê²Ÿ ì„ë² ë”© ë¡œë“œ
            embedding_path = face_recog_config['embedding_path']
            if os.path.exists(embedding_path):
                embedding_data = np.load(embedding_path, allow_pickle=True).item()
                self.target_embedding = torch.tensor(embedding_data['embedding']).to(cpu_device)
                
                self.logger.info(f"âœ… ì–¼êµ´ ì¸ì‹ ì´ˆê¸°í™” ì™„ë£Œ")
                self.logger.info(f"   ì„ë² ë”© íŒŒì¼: {embedding_path}")
                self.logger.info(f"   ìœ ì‚¬ë„ ì„ê³„ê°’: {face_recog_config['similarity_threshold']}")
            else:
                self.logger.error(f"âŒ ì„ë² ë”© íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {embedding_path}")
                self.facenet_model = None
                self.target_embedding = None
            
        except Exception as e:
            self.logger.error(f"âŒ ì–¼êµ´ ì¸ì‹ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.facenet_model = None
            self.target_embedding = None
    
    def _create_output_dirs(self):
        """ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
        base_dir = self.config['output']['base_dir']
        self.preprocessed_dir = os.path.join(base_dir, self.config['output']['preprocessed_dir'])
        self.video_sequence_dir = os.path.join(self.preprocessed_dir, self.config['output']['video_sequence_dir'])
        
        os.makedirs(self.video_sequence_dir, exist_ok=True)
        
        # ë””ë²„ê¹…ìš© ì–¼êµ´ ì´ë¯¸ì§€ í´ë”
        if self.config['video']['save_face_images']:
            self.face_images_dir = os.path.join(self.preprocessed_dir, self.config['video']['face_images_dir'])
            os.makedirs(self.face_images_dir, exist_ok=True)
            # ê¸°ì¡´ íŒŒì¼ë“¤ ì •ë¦¬
            for file in os.listdir(self.face_images_dir):
                if file.endswith(('.jpg', '.png')):
                    os.remove(os.path.join(self.face_images_dir, file))
    
    def _print_config_summary(self):
        """ì„¤ì • ìš”ì•½ ì¶œë ¥"""
        self.logger.info("ğŸ“‹ ì„¤ì • ìš”ì•½:")
        self.logger.info(f"   í”„ë ˆì„ ìŠ¤í‚µ: {self.config['video']['frame_skip']}í”„ë ˆì„ë§ˆë‹¤")
        self.logger.info(f"   MTCNN ë°°ì¹˜: {self.config['mtcnn']['batch_size']}")
        self.logger.info(f"   ê°ì • ë°°ì¹˜: {self.config['emotion']['batch_size']}")
        
        # ì–¼êµ´ ì¸ì‹ ì„¤ì • ì¶œë ¥
        if self.config['face_recognition']['enabled']:
            if self.facenet_model is not None:
                self.logger.info(f"   ì–¼êµ´ ì¸ì‹: í™œì„±í™” (ì„ê³„ê°’: {self.config['face_recognition']['similarity_threshold']})")
            else:
                self.logger.info(f"   ì–¼êµ´ ì¸ì‹: ì„¤ì •ë¨ (ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨)")
        else:
            self.logger.info(f"   ì–¼êµ´ ì¸ì‹: ë¹„í™œì„±í™”")
        
        self.logger.info(f"   ê°ì • ì¶”ì¶œ: {self.config['video']['extract_emotions']}")
        self.logger.info(f"   ì–¼êµ´ ì´ë¯¸ì§€ ì €ì¥: {self.config['video']['save_face_images']}")
        self.logger.info(f"   ë””ë°”ì´ìŠ¤: {self.config['emotion']['device']}")
    
    def process_long_video(self, video_path: str) -> Optional[Dict]:
        """
        ê¸´ ì˜ìƒ ì „ì²˜ë¦¬
        
        Args:
            video_path (str): ì²˜ë¦¬í•  ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            Dict: ì „ì²˜ë¦¬ ê²°ê³¼ ì •ë³´
        """
        if not os.path.exists(video_path):
            self.logger.error(f"âŒ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
            return None
        
        video_name = Path(video_path).stem
        self.logger.info(f"ğŸ¬ ê¸´ ì˜ìƒ ì „ì²˜ë¦¬ ì‹œì‘: {video_name}")
        
        # ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        video_info = self._get_video_info(video_path)
        self.logger.info(f"   ê¸¸ì´: {video_info['duration']:.1f}ì´ˆ, FPS: {video_info['fps']:.1f}")
        
        # ê²°ê³¼ ì €ì¥ìš© ì´ˆê¸°í™”
        self.emotion_sequences = []
        self.face_detected_sequence = []
        self.timestamps_sequence = []
        self.frame_indices_sequence = []
        
        self.stats['processing_start_time'] = time.time()
        self.face_detection_done = False
        self.stop_flag = False
        
        # ìŠ¤ë ˆë“œ ì‹œì‘
        threads = []
        
        # 1. í”„ë ˆì„ ì½ê¸° ìŠ¤ë ˆë“œ
        frame_thread = threading.Thread(target=self._frame_reader_worker, args=(video_path,))
        threads.append(frame_thread)
        
        # 2. ì–¼êµ´ íƒì§€ + ê°ì • ì¶”ì¶œ ìŠ¤ë ˆë“œ
        process_thread = threading.Thread(target=self._face_emotion_worker)
        threads.append(process_thread)
        
        # 3. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ
        monitor_thread = threading.Thread(target=self._performance_monitor)
        monitor_thread.daemon = True
        threads.append(monitor_thread)
        
        # ëª¨ë“  ìŠ¤ë ˆë“œ ì‹œì‘
        for thread in threads:
            thread.start()
        
        # í”„ë ˆì„ ì½ê¸° ì™„ë£Œ ëŒ€ê¸°
        frame_thread.join()
        self.logger.info("âœ… í”„ë ˆì„ ì½ê¸° ì™„ë£Œ")
        
        # ì²˜ë¦¬ ì™„ë£Œ ëŒ€ê¸°
        process_thread.join()
        self.logger.info("âœ… ì–¼êµ´ íƒì§€ ë° ê°ì • ì¶”ì¶œ ì™„ë£Œ")
        
        # ì¢…ë£Œ í”Œë˜ê·¸ ì„¤ì •
        self.stop_flag = True
        
        # ê²°ê³¼ ì €ì¥
        result = self._save_results(video_path, video_info)
        
        # ìµœì¢… í†µê³„ ì¶œë ¥
        self._print_final_stats()
        
        return result
    
    def _get_video_info(self, video_path: str) -> Dict:
        """ë¹„ë””ì˜¤ ì •ë³´ ì¶”ì¶œ"""
        cap = cv2.VideoCapture(video_path)
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = frame_count / fps if fps > 0 else 0
        
        cap.release()
        
        return {
            'fps': fps,
            'frame_count': frame_count,
            'duration': duration
        }
    
    def _frame_reader_worker(self, video_path: str):
        """í”„ë ˆì„ ì½ê¸° ì›Œì»¤"""
        cap = cv2.VideoCapture(video_path)
        frame_skip = self.config['video']['frame_skip']
        frame_count = 0
        
        # í˜¸í™˜ì„± í™•ì¸
        if not cap.isOpened():
            self.logger.error("âŒ ë¹„ë””ì˜¤ íŒŒì¼ ì—´ê¸° ì‹¤íŒ¨")
            self.frame_queue.put(None)
            return
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # í”„ë ˆì„ ìŠ¤í‚µ
                if frame_count % frame_skip != 0:
                    frame_count += 1
                    continue
                
                # BGR to RGB ë³€í™˜
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # íƒ€ì„ìŠ¤íƒ¬í”„ ê³„ì‚°
                timestamp = frame_count / cap.get(cv2.CAP_PROP_FPS)
                
                # íì— ì¶”ê°€
                frame_data = {
                    'frame': frame_rgb,
                    'frame_number': frame_count,
                    'timestamp': timestamp
                }
                
                self.frame_queue.put(frame_data)
                frame_count += 1
                
        except Exception as e:
            self.logger.error(f"âŒ í”„ë ˆì„ ì½ê¸° ì˜¤ë¥˜: {e}")
        finally:
            cap.release()
            # ì¢…ë£Œ ì‹ í˜¸
            self.frame_queue.put(None)
    
    def _face_emotion_worker(self):
        """ì–¼êµ´ íƒì§€ + ê°ì • ì¶”ì¶œ ì›Œì»¤ (ë°°ì¹˜ ìµœì í™”)"""
        batch_size = self.config['mtcnn']['batch_size']
        frame_batch = []
        
        try:
            while True:
                frame_data = self.frame_queue.get()
                
                if frame_data is None:  # ì¢…ë£Œ ì‹ í˜¸
                    # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
                    if frame_batch:
                        self._process_frame_batch_optimized(frame_batch)
                    break
                
                frame_batch.append(frame_data)
                
                # ë°°ì¹˜ê°€ ì°¼ìœ¼ë©´ ì²˜ë¦¬
                if len(frame_batch) >= batch_size:
                    self._process_frame_batch_optimized(frame_batch)
                    frame_batch = []
                
                self.frame_queue.task_done()
                
        except Exception as e:
            self.logger.error(f"âŒ ì–¼êµ´ íƒì§€ ë° ê°ì • ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        finally:
            self.face_detection_done = True
    
    def _process_frame_batch_optimized(self, frame_batch: List[Dict]):
        """í”„ë ˆì„ ë°°ì¹˜ì—ì„œ ì–¼êµ´ íƒì§€ ë° ê°ì • ì¶”ì¶œ (ìµœì í™”)"""
        batch_start_time = time.time()
        
        try:
            # 1. MTCNN ë°°ì¹˜ ì²˜ë¦¬
            pil_images = []
            frame_metadata_list = []
            
            for frame_data in frame_batch:
                pil_image = Image.fromarray(frame_data['frame'])
                pil_images.append(pil_image)
                frame_metadata_list.append({
                    'frame_number': frame_data['frame_number'],
                    'timestamp': frame_data['timestamp']
                })
            
            # ì–¼êµ´ íƒì§€
            detection_start = time.time()
            face_results = self.face_detector.process_image_batch(pil_images, frame_metadata_list)
            self.stats['total_face_detection_time'] += time.time() - detection_start
            
            # 2. ì–¼êµ´ ì¸ì‹ í•„í„°ë§ (ë°°ì¹˜)
            if self.config['face_recognition']['enabled'] and self.facenet_model is not None:
                recognition_start = time.time()
                face_results = self._filter_faces_by_recognition_batch(face_results)
                self.stats['total_recognition_time'] += time.time() - recognition_start
            
            # 3. ê°ì • ì¶”ì¶œ (ë°°ì¹˜)
            if self.config['video']['extract_emotions'] and face_results:
                emotion_start = time.time()
                self._extract_emotions_batch(face_results)
                self.stats['total_emotion_time'] += time.time() - emotion_start
            
            # 4. í”„ë ˆì„ë³„ ê²°ê³¼ ë§¤ì¹­ ë° ì €ì¥
            self._match_and_store_results(frame_batch, face_results)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.stats['batch_count'] += 1
            batch_time = time.time() - batch_start_time
            
            if self.config['logging']['batch_summary']:
                faces_count = len(face_results)
                self.logger.debug(
                    f"ë°°ì¹˜ ì²˜ë¦¬: {len(frame_batch)}í”„ë ˆì„ â†’ {faces_count}ê°œ ì–¼êµ´ ({batch_time:.2f}ì´ˆ)"
                )
            
        except Exception as e:
            self.logger.error(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    def _filter_faces_by_recognition_batch(self, face_results: List[Dict]) -> List[Dict]:
        """ì–¼êµ´ ì¸ì‹ìœ¼ë¡œ ì¹¨ì°©ë§¨ ì–¼êµ´ë§Œ í•„í„°ë§ (ë°°ì¹˜ ì²˜ë¦¬)"""
        if not face_results:
            return face_results
        
        try:
            # ì–¼êµ´ ì´ë¯¸ì§€ë“¤ ì¶”ì¶œ
            face_images = [result['face_image'] for result in face_results]
            
            # ë°°ì¹˜ ì„ë² ë”© ì¶”ì¶œ
            embeddings = self._get_face_embeddings_batch(face_images)
            if embeddings is None:
                return face_results
            
            # ë°°ì¹˜ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = self._calculate_similarities_batch(embeddings)
            if similarities is None:
                return face_results
            
            # ì„ê³„ê°’ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§
            threshold = self.config['face_recognition']['similarity_threshold']
            matches = similarities > threshold
            
            # í•„í„°ë§ëœ ê²°ê³¼ ìƒì„±
            filtered_results = []
            for i, (result, similarity, is_match) in enumerate(zip(face_results, similarities, matches)):
                if is_match:
                    result['similarity'] = float(similarity)
                    filtered_results.append(result)
                    self.stats['faces_recognized'] += 1
                else:
                    self.stats['faces_filtered'] += 1
                    # í•„í„°ë§ëœ ì–¼êµ´ ì €ì¥ (ì˜µì…˜)
                    if self.config['face_recognition']['logging']['save_filtered_faces']:
                        self._save_filtered_face(result, float(similarity))
            
            # í•„í„°ë§ í†µê³„ ë¡œê¹…
            if self.config['face_recognition']['logging']['log_filtered_count']:
                self.logger.debug(
                    f"ì–¼êµ´ ì¸ì‹: {len(face_results)}ê°œ â†’ {len(filtered_results)}ê°œ "
                    f"(í•„í„°ë§: {len(face_results) - len(filtered_results)}ê°œ)"
                )
            
            return filtered_results
            
        except Exception as e:
            self.logger.error(f"âŒ ì–¼êµ´ ì¸ì‹ í•„í„°ë§ ì‹¤íŒ¨: {e}")
            return face_results
    
    def _get_face_embeddings_batch(self, face_images: List[Image.Image]) -> Optional[torch.Tensor]:
        """FaceNet ë°°ì¹˜ ì„ë² ë”© ì¶”ì¶œ"""
        if self.facenet_model is None:
            return None
        
        try:
            # 160x160 ë¦¬ì‚¬ì´ì§• (FaceNet ì…ë ¥ í¬ê¸°)
            resized_images = []
            for face_img in face_images:
                resized_img = face_img.resize((160, 160), Image.BILINEAR)
                img_array = np.array(resized_img)
                img_tensor = torch.from_numpy(img_array).float().permute(2, 0, 1)
                img_tensor = (img_tensor - 127.5) / 128.0  # ì •ê·œí™” [-1, 1]
                resized_images.append(img_tensor)
            
            # ë°°ì¹˜ í…ì„œ ìƒì„±
            batch_tensor = torch.stack(resized_images).to(self.facenet_model.device)
            
            # ì„ë² ë”© ì¶”ì¶œ
            with torch.no_grad():
                embeddings = self.facenet_model(batch_tensor)
                embeddings = F.normalize(embeddings, p=2, dim=1)
            
            return embeddings
            
        except Exception as e:
            self.logger.error(f"âŒ ë°°ì¹˜ ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def _calculate_similarities_batch(self, embeddings: torch.Tensor) -> Optional[torch.Tensor]:
        """ë°°ì¹˜ ì„ë² ë”©ê³¼ íƒ€ê²Ÿ ì„ë² ë”© ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        if embeddings is None or self.target_embedding is None:
            return None
        
        try:
            # ê³„ì‚°ì„ ìœ„í•´ ì„ë² ë”©ê³¼ íƒ€ê²Ÿì´ ê°™ì€ ë””ë°”ì´ìŠ¤ì— ìˆì–´ì•¼ í•¨
            device = embeddings.device
            target_embedding = self.target_embedding.to(device)
            
            # íƒ€ê²Ÿ ì„ë² ë”©ì„ ë°°ì¹˜ í¬ê¸°ë¡œ í™•ì¥
            target_expanded = target_embedding.unsqueeze(0).repeat(embeddings.size(0), 1)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = F.cosine_similarity(embeddings, target_expanded)
            
            return similarities
            
        except Exception as e:
            self.logger.error(f"âŒ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_emotions_batch(self, face_results: List[Dict]):
        """ê°ì • ì¶”ì¶œ (ë°°ì¹˜ ì²˜ë¦¬)"""
        if not face_results:
            return
        
        try:
            emotion_batch_size = self.config['emotion']['batch_size']
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            for i in range(0, len(face_results), emotion_batch_size):
                batch_end = min(i + emotion_batch_size, len(face_results))
                batch_faces = face_results[i:batch_end]
                
                # ë°°ì¹˜ ì–¼êµ´ ì´ë¯¸ì§€ ì¶”ì¶œ
                face_images = [result['face_image'] for result in batch_faces]
                
                # ë°°ì¹˜ ê°ì • ì¶”ì¶œ
                emotion_features = self.emotion_model.extract_emotion_features_batch(face_images)
                
                # ê²°ê³¼ë¥¼ face_resultsì— ì €ì¥
                for j, features in enumerate(emotion_features):
                    batch_faces[j]['emotion_features'] = features
                    self.stats['emotions_extracted'] += 1
                    
        except Exception as e:
            self.logger.error(f"âŒ ë°°ì¹˜ ê°ì • ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    
    def _match_and_store_results(self, frame_batch: List[Dict], face_results: List[Dict]):
        """í”„ë ˆì„ë³„ ê²°ê³¼ ë§¤ì¹­ ë° ì €ì¥"""
        # í”„ë ˆì„ë³„ë¡œ ì²˜ë¦¬
        for frame_data in frame_batch:
            frame_number = frame_data['frame_number']
            timestamp = frame_data['timestamp']
            
            # í•´ë‹¹ í”„ë ˆì„ì˜ ì–¼êµ´ ì°¾ê¸°
            frame_faces = [f for f in face_results if f['frame_number'] == frame_number]
            
            if frame_faces:
                # ì–¼êµ´ì´ ìˆëŠ” ê²½ìš° - ì²« ë²ˆì§¸ ì–¼êµ´ ì‚¬ìš©
                face_data = frame_faces[0]
                
                # ê°ì • íŠ¹ì§• ê°€ì ¸ì˜¤ê¸°
                if 'emotion_features' in face_data:
                    emotion_features = face_data['emotion_features']
                else:
                    emotion_features = np.zeros(10)
                
                # ë””ë²„ê¹…ìš© ì–¼êµ´ ì´ë¯¸ì§€ ì €ì¥
                if self.config['video']['save_face_images']:
                    self._save_debug_face_image(face_data['face_image'], frame_number)
                
                # ê²°ê³¼ ì €ì¥
                self.emotion_sequences.append(emotion_features)
                self.face_detected_sequence.append(True)
                self.stats['faces_detected'] += 1
                
            else:
                # ì–¼êµ´ì´ ì—†ëŠ” ê²½ìš°
                self.emotion_sequences.append(np.full(10, np.nan))
                self.face_detected_sequence.append(False)
            
            # ê³µí†µ ì •ë³´ ì €ì¥
            self.timestamps_sequence.append(timestamp)
            self.frame_indices_sequence.append(frame_number)
            self.stats['frames_processed'] += 1
    
    def _save_filtered_face(self, face_data: Dict, similarity: float):
        """í•„í„°ë§ëœ ì–¼êµ´ ì´ë¯¸ì§€ ì €ì¥ (ë””ë²„ê¹…ìš©)"""
        try:
            timestamp_str = f"{int(face_data.get('timestamp', 0)):05d}"
            filename = f"filtered_{timestamp_str}_{similarity:.3f}.jpg"
            
            filtered_dir = os.path.join(self.preprocessed_dir, "filtered_faces")
            os.makedirs(filtered_dir, exist_ok=True)
            
            save_path = os.path.join(filtered_dir, filename)
            face_data['face_image'].save(save_path)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í•„í„°ë§ëœ ì–¼êµ´ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _save_debug_face_image(self, face_image: Image.Image, frame_number: int):
        """ë””ë²„ê¹…ìš© ì–¼êµ´ ì´ë¯¸ì§€ ì €ì¥"""
        try:
            filename = f"frame_{frame_number:06d}.jpg"
            save_path = os.path.join(self.face_images_dir, filename)
            face_image.save(save_path)
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë””ë²„ê¹… ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _save_results(self, video_path: str, video_info: Dict) -> Dict:
        """ê²°ê³¼ë¥¼ HDF5 íŒŒì¼ë¡œ ì €ì¥"""
        try:
            video_name = Path(video_path).stem
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            hdf5_filename = f"video_seq_{video_name}_{timestamp}.h5"
            hdf5_path = os.path.join(self.video_sequence_dir, hdf5_filename)
            
            with h5py.File(hdf5_path, 'w') as f:
                # ë©”íƒ€ë°ì´í„°
                f.attrs['video_name'] = video_name
                f.attrs['video_path'] = video_path
                f.attrs['duration'] = video_info['duration']
                f.attrs['fps'] = video_info['fps']
                f.attrs['frame_skip'] = self.config['video']['frame_skip']
                f.attrs['total_frames'] = len(self.emotion_sequences)
                f.attrs['face_detection_ratio'] = float(np.mean(self.face_detected_sequence))
                f.attrs['processed_at'] = datetime.now().isoformat()
                
                # ì–¼êµ´ ì¸ì‹ í†µê³„
                if self.config['face_recognition']['enabled']:
                    f.attrs['faces_recognized'] = self.stats['faces_recognized']
                    f.attrs['faces_filtered'] = self.stats['faces_filtered']
                    f.attrs['recognition_ratio'] = float(self.stats['faces_recognized'] / max(1, self.stats['faces_detected']))
                
                # ì‹œí€€ìŠ¤ ë°ì´í„°
                sequences_group = f.create_group('sequences')
                sequences_group.create_dataset('emotions', 
                                             data=np.array(self.emotion_sequences),
                                             compression='gzip')
                sequences_group.create_dataset('face_detected', 
                                             data=np.array(self.face_detected_sequence),
                                             compression='gzip')
                sequences_group.create_dataset('timestamps', 
                                             data=np.array(self.timestamps_sequence),
                                             compression='gzip')
                sequences_group.create_dataset('frame_indices', 
                                             data=np.array(self.frame_indices_sequence),
                                             compression='gzip')
            
            result = {
                'video_name': video_name,
                'video_path': video_path,
                'hdf5_path': hdf5_path,
                'duration': video_info['duration'],
                'total_frames': len(self.emotion_sequences),
                'face_detection_ratio': float(np.mean(self.face_detected_sequence)),
                'stats': self.stats.copy()
            }
            
            self.logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {hdf5_path}")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None
    
    def _performance_monitor(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
        interval = self.config['performance']['monitoring_interval']
        
        while not self.stop_flag:
            time.sleep(interval)
            
            if self.stats['processing_start_time'] is None:
                continue
            
            # í˜„ì¬ í†µê³„
            elapsed = time.time() - self.stats['processing_start_time']
            fps = self.stats['frames_processed'] / elapsed if elapsed > 0 else 0
            
            face_ratio = (self.stats['faces_detected'] / max(1, self.stats['frames_processed'])) * 100
            
            # í‰ê·  ì²˜ë¦¬ ì‹œê°„ë“¤
            avg_detection = self.stats['total_face_detection_time'] / max(1, self.stats['batch_count'])
            avg_emotion = self.stats['total_emotion_time'] / max(1, self.stats['batch_count'])
            
            info_msg = (
                f"ğŸ“Š [{elapsed:.1f}s] "
                f"í”„ë ˆì„: {self.stats['frames_processed']} ({fps:.1f} FPS), "
                f"ì–¼êµ´: {self.stats['faces_detected']} ({face_ratio:.1f}%), "
                f"ê°ì •: {self.stats['emotions_extracted']}, "
                f"íƒì§€: {avg_detection:.3f}s, ê°ì •: {avg_emotion:.3f}s"
            )
            
            # ì–¼êµ´ ì¸ì‹ í†µê³„ ì¶”ê°€
            if self.config['face_recognition']['enabled']:
                recognition_rate = (self.stats['faces_recognized'] / max(1, self.stats['faces_detected'])) * 100
                avg_recognition = self.stats['total_recognition_time'] / max(1, self.stats['batch_count'])
                info_msg += f", ì¸ì‹ë¥ : {recognition_rate:.1f}%, ì¸ì‹: {avg_recognition:.3f}s"
            
            self.logger.info(info_msg)
    
    def _print_final_stats(self):
        """ìµœì¢… í†µê³„ ì¶œë ¥"""
        elapsed = time.time() - self.stats['processing_start_time']
        fps = self.stats['frames_processed'] / elapsed if elapsed > 0 else 0
        face_ratio = (self.stats['faces_detected'] / max(1, self.stats['frames_processed'])) * 100
        
        self.logger.info("ğŸ¯ ì „ì²˜ë¦¬ ì™„ë£Œ!")
        self.logger.info(f"   ì´ ì²˜ë¦¬ ì‹œê°„: {elapsed:.1f}ì´ˆ")
        self.logger.info(f"   ì²˜ë¦¬ëœ í”„ë ˆì„: {self.stats['frames_processed']}ê°œ ({fps:.1f} FPS)")
        self.logger.info(f"   íƒì§€ëœ ì–¼êµ´: {self.stats['faces_detected']}ê°œ ({face_ratio:.1f}%)")
        
        # ì–¼êµ´ ì¸ì‹ í†µê³„
        if self.config['face_recognition']['enabled']:
            recognition_rate = (self.stats['faces_recognized'] / max(1, self.stats['faces_detected'])) * 100
            self.logger.info(f"   ì¸ì‹ëœ ì–¼êµ´: {self.stats['faces_recognized']}ê°œ ({recognition_rate:.1f}%)")
            self.logger.info(f"   í•„í„°ë§ëœ ì–¼êµ´: {self.stats['faces_filtered']}ê°œ")
        
        self.logger.info(f"   ì¶”ì¶œëœ ê°ì •: {self.stats['emotions_extracted']}ê°œ")
        
        # í‰ê·  ì²˜ë¦¬ ì‹œê°„
        if self.stats['batch_count'] > 0:
            avg_detection = self.stats['total_face_detection_time'] / self.stats['batch_count']
            avg_emotion = self.stats['total_emotion_time'] / self.stats['batch_count']
            self.logger.info(f"   í‰ê·  íƒì§€ ì‹œê°„: {avg_detection:.3f}ì´ˆ/ë°°ì¹˜")
            self.logger.info(f"   í‰ê·  ê°ì • ì‹œê°„: {avg_emotion:.3f}ì´ˆ/ë°°ì¹˜")
            
            if self.config['face_recognition']['enabled']:
                avg_recognition = self.stats['total_recognition_time'] / self.stats['batch_count']
                self.logger.info(f"   í‰ê·  ì¸ì‹ ì‹œê°„: {avg_recognition:.3f}ì´ˆ/ë°°ì¹˜")


def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    import argparse
    
    # ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description='ê¸´ ì˜ìƒ ë¹„ë””ì˜¤ ì „ì²˜ë¦¬')
    parser.add_argument('video_path', help='ì²˜ë¦¬í•  ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--config', default='video_analyzer/configs/inference_config.yaml', help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    # ì „ì²˜ë¦¬ê¸° ì‹¤í–‰
    processor = LongVideoProcessor(args.config)
    
    # ë¹„ë””ì˜¤ ì²˜ë¦¬
    result = processor.process_long_video(args.video_path)
    
    if result:
        print("\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"HDF5 íŒŒì¼: {result['hdf5_path']}")
        print(f"ì²˜ë¦¬ ì‹œê°„: {result['duration']:.1f}ì´ˆ")
        print(f"ì–¼êµ´ ì¸ì‹ë¥ : {result['face_detection_ratio']:.1%}")
        if 'stats' in result and 'faces_recognized' in result['stats']:
            recognition_rate = result['stats']['faces_recognized'] / max(1, result['stats']['faces_detected'])
            print(f"ì¹¨ì°©ë§¨ ì¸ì‹ë¥ : {recognition_rate:.1%}")
    else:
        print("âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨")


if __name__ == "__main__":
    main()