import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms
from PIL import Image
import numpy as np
import os
from typing import Union, List, Dict, Optional
import logging

class VAEmotionModel:
    """
    VA-MTL (Valence-Arousal Multi-Task Learning) ê°ì • ì¸ì‹ ëª¨ë¸
    EfficientNet-B0 ê¸°ë°˜, AffectNet ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨ë¨
    """
    
    def __init__(self, model_path: str = "models/affectnet_emotions/enet_b0_8_va_mtl.pt", 
                 device: Optional[str] = None):
        """
        ëª¨ë¸ ì´ˆê¸°í™”
        
        Args:
            model_path (str): ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            device (str, optional): ì—°ì‚° ì¥ì¹˜ ('cpu', 'cuda'). Noneì´ë©´ ìë™ ê°ì§€
        """
        self.model_path = model_path
        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ê°ì • ë ˆì´ë¸” ì •ì˜ (ì‹¤ì œ ëª¨ë¸ ìˆœì„œì— ë§ì¶˜ AffectNet 8ê°œ ê°ì •)
        self.emotion_labels = [
            'Anger',       # 0
            'Contempt',    # 1
            'Disgust',     # 2
            'Fear',        # 3
            'Happiness',   # 4
            'Neutral',     # 5
            'Sadness',     # 6
            'Surprise'     # 7
        ]
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ImageNet ì •ê·œí™”)
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = self._load_model()
        self.feature_extractor = self._create_feature_extractor()
        
        print(f"âœ… VA ê°ì • ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {self.device})")
    
    def _load_model(self) -> nn.Module:
        """ì „ì²´ ëª¨ë¸ ë¡œë“œ (ê°ì • ë¶„ë¥˜ìš©) - timm í˜¸í™˜ì„± ê°œì„ """
        if not os.path.exists(self.model_path):
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_path}")
        
        try:
            # ë°©ë²• 1: ì „ì²´ ëª¨ë¸ ê°ì²´ ë¡œë“œ ì‹œë„
            print("ğŸ“¦ ëª¨ë¸ ë¡œë“œ ì‹œë„ ì¤‘...")
            checkpoint = torch.load(self.model_path, map_location=self.device, weights_only=False)
            
            # ì „ì²´ ëª¨ë¸ì¸ì§€ í™•ì¸
            if hasattr(checkpoint, 'eval') and hasattr(checkpoint, 'forward'):
                print("âœ… ì „ì²´ ëª¨ë¸ ê°ì§€ - ì§ì ‘ ì‚¬ìš©")
                model = checkpoint.to(self.device)
                model.eval()
                
                # ì¶œë ¥ ì°¨ì› í™•ì¸
                with torch.no_grad():
                    dummy_input = torch.randn(1, 3, 224, 224).to(self.device)
                    output = model(dummy_input)
                    print(f"âœ… ëª¨ë¸ ì¶œë ¥ ì°¨ì›: {output.shape}")
                    assert output.shape[1] == 10, f"ì˜ˆìƒ ì¶œë ¥ ì°¨ì›: 10, ì‹¤ì œ: {output.shape[1]}"
                
                return model
            
            elif isinstance(checkpoint, dict):
                # state_dictì¸ ê²½ìš° - timmìœ¼ë¡œ ì¬êµ¬ì„±
                print("âœ… state_dict ê°ì§€ - timmìœ¼ë¡œ ëª¨ë¸ ì¬êµ¬ì„±")
                return self._load_from_state_dict(checkpoint)
            
            else:
                raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ í˜•íƒœ: {type(checkpoint)}")
            
        except Exception as e:
            print(f"âš ï¸ ì›ë³¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ”„ ëŒ€ì•ˆ ë¡œë“œ ë°©ì‹ ì‹œë„...")
            return self._load_fallback()
    
    def _load_from_state_dict(self, state_dict: dict) -> nn.Module:
        """state_dictì—ì„œ ëª¨ë¸ ì¬êµ¬ì„±"""
        try:
            import timm
            print(f"ğŸ“¦ timm ë²„ì „: {timm.__version__}")
            
            # EfficientNet-B0 ê¸°ë°˜ ëª¨ë¸ ìƒì„± (10í´ë˜ìŠ¤)
            model = timm.create_model('efficientnet_b0', pretrained=False, num_classes=10)
            
            # state_dict ë¡œë“œ (í˜¸í™˜ì„± ë¬¸ì œ ìš°íšŒ)
            model.load_state_dict(state_dict, strict=False)
            model.eval()
            model.to(self.device)
            
            print("âœ… timm ê¸°ë°˜ ëª¨ë¸ ì¬êµ¬ì„± ì„±ê³µ")
            return model
            
        except Exception as e:
            raise RuntimeError(f"timm ê¸°ë°˜ ì¬êµ¬ì„± ì‹¤íŒ¨: {e}")
    
    def _load_fallback(self) -> nn.Module:
        """ëŒ€ì•ˆ ë¡œë“œ ë°©ì‹ - ë” ê´€ëŒ€í•œ ì„¤ì •"""
        try:
            # ë” ê´€ëŒ€í•œ ë¡œë“œ ì‹œë„
            import pickle
            
            with open(self.model_path, 'rb') as f:
                checkpoint = pickle.load(f)
            
            if hasattr(checkpoint, 'eval'):
                model = checkpoint.to(self.device)
                model.eval()
                print("âœ… pickle ê¸°ë°˜ ë¡œë“œ ì„±ê³µ")
                return model
            
            raise RuntimeError("ëŒ€ì•ˆ ë¡œë“œ ë°©ì‹ë„ ì‹¤íŒ¨")
            
        except Exception as e:
            raise RuntimeError(f"ëª¨ë“  ë¡œë“œ ë°©ì‹ ì‹¤íŒ¨: {e}")
    
    def _create_feature_extractor(self) -> nn.Module:
        """íŠ¹ì§• ì¶”ì¶œê¸° ìƒì„± (ë¶„ë¥˜ê¸° ì œê±°)"""
        try:
            import timm
            
            # EfficientNet-B0 ë°±ë³¸ë§Œ ë¡œë“œ (num_classes=0ìœ¼ë¡œ ë¶„ë¥˜ê¸° ì œê±°)
            feature_extractor = timm.create_model('efficientnet_b0', pretrained=False, num_classes=0)
            
            # í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ (ë¶„ë¥˜ê¸° ì œì™¸)
            model_state = self.model.state_dict()
            feature_state = {}
            
            for name, param in feature_extractor.state_dict().items():
                if name in model_state:
                    feature_state[name] = model_state[name]
                else:
                    print(f"âš ï¸ ê°€ì¤‘ì¹˜ ëˆ„ë½: {name}")
            
            feature_extractor.load_state_dict(feature_state, strict=False)
            feature_extractor.eval()
            feature_extractor.to(self.device)
            
            # íŠ¹ì§• ì°¨ì› í™•ì¸
            with torch.no_grad():
                dummy_input = torch.randn(1, 3, 224, 224).to(self.device)
                features = feature_extractor(dummy_input)
                assert features.shape[1] == 1280, f"ì˜ˆìƒ íŠ¹ì§• ì°¨ì›: 1280, ì‹¤ì œ: {features.shape[1]}"
            
            return feature_extractor
            
        except Exception as e:
            print(f"âš ï¸ íŠ¹ì§• ì¶”ì¶œê¸° ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _preprocess_image(self, image: Union[str, Image.Image, np.ndarray]) -> torch.Tensor:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        if isinstance(image, str):
            # íŒŒì¼ ê²½ë¡œì¸ ê²½ìš°
            image = Image.open(image).convert('RGB')
        elif isinstance(image, np.ndarray):
            # numpy ë°°ì—´ì¸ ê²½ìš°
            image = Image.fromarray(image)
        elif not isinstance(image, Image.Image):
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(image)}")
        
        # ì „ì²˜ë¦¬ ì ìš©
        tensor = self.transform(image).unsqueeze(0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        return tensor.to(self.device)
    
    def predict_emotions(self, image: Union[str, Image.Image, np.ndarray]) -> Dict:
        """
        ê°ì • ë¶„ë¥˜ ì˜ˆì¸¡
        
        Args:
            image: ì…ë ¥ ì´ë¯¸ì§€ (íŒŒì¼ ê²½ë¡œ, PIL Image, ë˜ëŠ” numpy ë°°ì—´)
            
        Returns:
            Dict: ê°ì • ë¶„ë¥˜ ê²°ê³¼
            {
                'emotions': [8ê°œ ê°ì • í™•ë¥ ],
                'emotion_probs': {ê°ì •ëª…: í™•ë¥ },
                'predicted_emotion': 'ê°€ì¥ ë†’ì€ ê°ì •',
                'valence': float,
                'arousal': float,
                'raw_output': [10ì°¨ì› ì›ë³¸ ì¶œë ¥]
            }
        """
        try:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            input_tensor = self._preprocess_image(image)
            
            # ëª¨ë¸ ì¶”ë¡ 
            with torch.no_grad():
                output = self.model(input_tensor)
                output = output.cpu().numpy().flatten()
            
            # 8ê°œ ê°ì • + 2ê°œ VA ë¶„ë¦¬
            emotions = output[:8]  # ê°ì • ë¡œì§“
            valence = output[8]    # Valence
            arousal = output[9]    # Arousal
            
            # ê°ì • í™•ë¥  ê³„ì‚° (ì†Œí”„íŠ¸ë§¥ìŠ¤)
            emotion_probs = F.softmax(torch.tensor(emotions), dim=0).numpy()
            
            # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ìƒì„±
            emotion_dict = {emotion: float(prob) for emotion, prob in zip(self.emotion_labels, emotion_probs)}
            predicted_emotion = self.emotion_labels[np.argmax(emotion_probs)]
            
            return {
                'emotions': emotion_probs.tolist(),
                'emotion_probs': emotion_dict,
                'predicted_emotion': predicted_emotion,
                'valence': float(valence),
                'arousal': float(arousal),
                'raw_output': output.tolist()
            }
            
        except Exception as e:
            raise RuntimeError(f"ê°ì • ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
    
    def extract_features(self, image: Union[str, Image.Image, np.ndarray]) -> np.ndarray:
        """
        1280ì°¨ì› íŠ¹ì§• ë²¡í„° ì¶”ì¶œ
        
        Args:
            image: ì…ë ¥ ì´ë¯¸ì§€
            
        Returns:
            np.ndarray: 1280ì°¨ì› íŠ¹ì§• ë²¡í„°
        """
        if self.feature_extractor is None:
            raise RuntimeError("íŠ¹ì§• ì¶”ì¶œê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        try:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            input_tensor = self._preprocess_image(image)
            
            # íŠ¹ì§• ì¶”ì¶œ
            with torch.no_grad():
                features = self.feature_extractor(input_tensor)
                features = features.cpu().numpy().flatten()
            
            return features
            
        except Exception as e:
            raise RuntimeError(f"íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    
    def predict_batch(self, images: List[Union[str, Image.Image, np.ndarray]], 
                     mode: str = 'emotions') -> List[Dict]:
        """
        ë°°ì¹˜ ì˜ˆì¸¡
        
        Args:
            images: ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
            mode: 'emotions' (ê°ì • ë¶„ë¥˜) ë˜ëŠ” 'features' (íŠ¹ì§• ì¶”ì¶œ)
            
        Returns:
            List[Dict]: ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        for image in images:
            try:
                if mode == 'emotions':
                    result = self.predict_emotions(image)
                elif mode == 'features':
                    features = self.extract_features(image)
                    result = {'features': features.tolist(), 'feature_dim': len(features)}
                else:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë“œ: {mode}")
                
                results.append(result)
                
            except Exception as e:
                print(f"âš ï¸ ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜ (ì´ë¯¸ì§€ {len(results)}): {e}")
                results.append({'error': str(e)})
        
        return results
    
    def analyze_directory(self, directory_path: str, mode: str = 'emotions') -> Dict:
        """
        ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  ì´ë¯¸ì§€ ë¶„ì„
        
        Args:
            directory_path: ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            mode: 'emotions' ë˜ëŠ” 'features'
            
        Returns:
            Dict: ë¶„ì„ ê²°ê³¼ ìš”ì•½
        """
        if not os.path.exists(directory_path):
            raise FileNotFoundError(f"ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {directory_path}")
        
        # ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}
        image_files = []
        
        for file in os.listdir(directory_path):
            if any(file.lower().endswith(ext) for ext in image_extensions):
                image_files.append(os.path.join(directory_path, file))
        
        if not image_files:
            return {'message': 'ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤', 'results': []}
        
        print(f"ğŸ“ {len(image_files)}ê°œ ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘...")
        
        # ë°°ì¹˜ ì²˜ë¦¬
        results = self.predict_batch(image_files, mode=mode)
        
        # ìš”ì•½ ìƒì„±
        summary = {
            'total_images': len(image_files),
            'successful': len([r for r in results if 'error' not in r]),
            'failed': len([r for r in results if 'error' in r]),
            'results': [
                {
                    'filename': os.path.basename(image_files[i]),
                    'result': results[i]
                }
                for i in range(len(image_files))
            ]
        }
        
        if mode == 'emotions':
            # ê°ì • í†µê³„ ì¶”ê°€
            successful_results = [r for r in results if 'error' not in r]
            if successful_results:
                emotion_counts = {}
                for result in successful_results:
                    emotion = result['predicted_emotion']
                    emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
                
                summary['emotion_statistics'] = emotion_counts
        
        print(f"âœ… ë¶„ì„ ì™„ë£Œ: {summary['successful']}/{summary['total_images']} ì„±ê³µ")
        
        return summary


def test_va_model():
    """í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸ§ª VA ê°ì • ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    try:
        # ëª¨ë¸ ì´ˆê¸°í™”
        va_model = VAEmotionModel()
        
        # data/input ë””ë ‰í† ë¦¬ ë¶„ì„
        input_dir = "data/processed/test_image"
        if os.path.exists(input_dir):
            print(f"\nğŸ“ {input_dir} ë””ë ‰í† ë¦¬ ë¶„ì„...")
            results = va_model.analyze_directory(input_dir, mode='emotions')
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼:")
            print(f"   ì´ ì´ë¯¸ì§€: {results['total_images']}ê°œ")
            print(f"   ì„±ê³µ: {results['successful']}ê°œ")
            print(f"   ì‹¤íŒ¨: {results['failed']}ê°œ")
            
            if 'emotion_statistics' in results:
                print(f"\nğŸ˜Š ê°ì • ë¶„í¬:")
                for emotion, count in results['emotion_statistics'].items():
                    print(f"   {emotion}: {count}ê°œ")
            
            # ê°œë³„ ê²°ê³¼ ì¶œë ¥ (ì²˜ìŒ 3ê°œë§Œ)
            print(f"\nğŸ” ìƒì„¸ ê²°ê³¼ (ì²˜ìŒ 3ê°œ):")
            for i, item in enumerate(results['results'][:3]):
                filename = item['filename']
                result = item['result']
                
                if 'error' not in result:
                    emotion = result['predicted_emotion']
                    confidence = max(result['emotions'])
                    valence = result['valence']
                    arousal = result['arousal']
                    
                    print(f"   {filename}:")
                    print(f"     ê°ì •: {emotion} (ì‹ ë¢°ë„: {confidence:.3f})")
                    print(f"     Valence: {valence:.3f}, Arousal: {arousal:.3f}")
                    
                    # 8ê°œ ê°ì • í™•ë¥  ëª¨ë‘ ì¶œë ¥
                    print(f"     8ê°œ ê°ì • í™•ë¥ :")
                    for emotion_name, prob in result['emotion_probs'].items():
                        print(f"       {emotion_name}: {prob:.3f}")
                    print()
                else:
                    print(f"   {filename}: ì˜¤ë¥˜ - {result['error']}")
        
        else:
            print(f"âš ï¸ {input_dir} ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤")
            print("í…ŒìŠ¤íŠ¸ìš© ì´ë¯¸ì§€ë¥¼ data/input/ í´ë”ì— ë„£ì–´ì£¼ì„¸ìš”")
    
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    test_va_model()